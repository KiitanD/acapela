{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7639da54",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-12T20:19:32.859890Z",
     "iopub.status.busy": "2023-04-12T20:19:32.858941Z",
     "iopub.status.idle": "2023-04-12T20:21:45.605676Z",
     "shell.execute_reply": "2023-04-12T20:21:45.604437Z"
    },
    "papermill": {
     "duration": 132.753515,
     "end_time": "2023-04-12T20:21:45.608423",
     "exception": false,
     "start_time": "2023-04-12T20:19:32.854908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "build-essential is already the newest version (12.8ubuntu1.1).\r\n",
      "The following additional packages will be installed:\r\n",
      "  libjack0 libportaudio2 libportaudiocpp0 uuid-dev\r\n",
      "Suggested packages:\r\n",
      "  libasound2-doc jackd1 portaudio19-doc\r\n",
      "The following packages will be REMOVED:\r\n",
      "  libjack-jackd2-0\r\n",
      "The following NEW packages will be installed:\r\n",
      "  libasound2-dev libjack-dev libjack0 libportaudio2 libportaudiocpp0\r\n",
      "  portaudio19-dev uuid-dev\r\n",
      "0 upgraded, 7 newly installed, 1 to remove and 76 not upgraded.\r\n",
      "Need to get 625 kB of archives.\r\n",
      "After this operation, 3340 kB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 libjack0 amd64 1:0.125.0-3build2 [93.3 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libasound2-dev amd64 1.2.2-2.1ubuntu2.5 [104 kB]\r\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 uuid-dev amd64 2.34-0.1ubuntu9.3 [33.6 kB]\r\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 libjack-dev amd64 1:0.125.0-3build2 [206 kB]\r\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 libportaudio2 amd64 19.6.0-1build1 [65.4 kB]\r\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/universe amd64 libportaudiocpp0 amd64 19.6.0-1build1 [16.1 kB]\r\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/universe amd64 portaudio19-dev amd64 19.6.0-1build1 [106 kB]\r\n",
      "Fetched 625 kB in 1s (1033 kB/s)\r\n",
      "dpkg: libjack-jackd2-0:amd64: dependency problems, but removing anyway as you requested:\r\n",
      " libavdevice58:amd64 depends on libjack-jackd2-0 (>= 1.9.10+20150825) | libjack-0.125; however:\r\n",
      "  Package libjack-jackd2-0:amd64 is to be removed.\r\n",
      "  Package libjack-0.125 is not installed.\r\n",
      "  Package libjack-jackd2-0:amd64 which provides libjack-0.125 is to be removed.\r\n",
      " libavdevice58:amd64 depends on libjack-jackd2-0 (>= 1.9.10+20150825) | libjack-0.125; however:\r\n",
      "  Package libjack-jackd2-0:amd64 is to be removed.\r\n",
      "  Package libjack-0.125 is not installed.\r\n",
      "  Package libjack-jackd2-0:amd64 which provides libjack-0.125 is to be removed.\r\n",
      "\r\n",
      "(Reading database ... 111522 files and directories currently installed.)\r\n",
      "Removing libjack-jackd2-0:amd64 (1.9.12~dfsg-2ubuntu2) ...\r\n",
      "Selecting previously unselected package libjack0:amd64.\r\n",
      "(Reading database ... 111510 files and directories currently installed.)\r\n",
      "Preparing to unpack .../0-libjack0_1%3a0.125.0-3build2_amd64.deb ...\r\n",
      "Unpacking libjack0:amd64 (1:0.125.0-3build2) ...\r\n",
      "Selecting previously unselected package libasound2-dev:amd64.\r\n",
      "Preparing to unpack .../1-libasound2-dev_1.2.2-2.1ubuntu2.5_amd64.deb ...\r\n",
      "Unpacking libasound2-dev:amd64 (1.2.2-2.1ubuntu2.5) ...\r\n",
      "Selecting previously unselected package uuid-dev:amd64.\r\n",
      "Preparing to unpack .../2-uuid-dev_2.34-0.1ubuntu9.3_amd64.deb ...\r\n",
      "Unpacking uuid-dev:amd64 (2.34-0.1ubuntu9.3) ...\r\n",
      "Selecting previously unselected package libjack-dev.\r\n",
      "Preparing to unpack .../3-libjack-dev_1%3a0.125.0-3build2_amd64.deb ...\r\n",
      "Unpacking libjack-dev (1:0.125.0-3build2) ...\r\n",
      "Selecting previously unselected package libportaudio2:amd64.\r\n",
      "Preparing to unpack .../4-libportaudio2_19.6.0-1build1_amd64.deb ...\r\n",
      "Unpacking libportaudio2:amd64 (19.6.0-1build1) ...\r\n",
      "Selecting previously unselected package libportaudiocpp0:amd64.\r\n",
      "Preparing to unpack .../5-libportaudiocpp0_19.6.0-1build1_amd64.deb ...\r\n",
      "Unpacking libportaudiocpp0:amd64 (19.6.0-1build1) ...\r\n",
      "Selecting previously unselected package portaudio19-dev:amd64.\r\n",
      "Preparing to unpack .../6-portaudio19-dev_19.6.0-1build1_amd64.deb ...\r\n",
      "Unpacking portaudio19-dev:amd64 (19.6.0-1build1) ...\r\n",
      "Setting up libjack0:amd64 (1:0.125.0-3build2) ...\r\n",
      "Setting up uuid-dev:amd64 (2.34-0.1ubuntu9.3) ...\r\n",
      "Setting up libjack-dev (1:0.125.0-3build2) ...\r\n",
      "Setting up libasound2-dev:amd64 (1.2.2-2.1ubuntu2.5) ...\r\n",
      "Setting up libportaudio2:amd64 (19.6.0-1build1) ...\r\n",
      "Setting up libportaudiocpp0:amd64 (19.6.0-1build1) ...\r\n",
      "Setting up portaudio19-dev:amd64 (19.6.0-1build1) ...\r\n",
      "Processing triggers for man-db (2.9.1-1) ...\r\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\r\n",
      "Collecting magenta\r\n",
      "  Downloading magenta-2.1.4-py3-none-any.whl (1.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting llvmlite\r\n",
      "  Downloading llvmlite-0.39.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting mido==1.2.6\r\n",
      "  Downloading mido-1.2.6-py2.py3-none-any.whl (69 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.8/69.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pretty-midi==0.2.9\r\n",
      "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting tensorflow-datasets==4.6.0\r\n",
      "  Downloading tensorflow_datasets-4.6.0-py3-none-any.whl (4.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting librosa==0.7.2\r\n",
      "  Downloading librosa-0.7.2.tar.gz (1.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting six==1.16.0\r\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\r\n",
      "Collecting scikit-image==0.19.3\r\n",
      "  Downloading scikit_image-0.19.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (13.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting numpy==1.21.6\r\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting note-seq==0.0.3\r\n",
      "  Downloading note_seq-0.0.3-py3-none-any.whl (210 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.1/210.1 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting scipy==1.7.3\r\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting matplotlib==3.5.2\r\n",
      "  Downloading matplotlib-3.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting sox==1.4.1\r\n",
      "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\r\n",
      "Collecting numba==0.49.1\r\n",
      "  Downloading numba-0.49.1-cp37-cp37m-manylinux2014_x86_64.whl (3.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting mir-eval==0.7\r\n",
      "  Downloading mir_eval-0.7.tar.gz (90 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting Pillow==9.2.0\r\n",
      "  Downloading Pillow-9.2.0-cp37-cp37m-manylinux_2_28_x86_64.whl (3.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting python-rtmidi==1.1.2\r\n",
      "  Downloading python-rtmidi-1.1.2.tar.gz (204 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.5/204.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting tensorflow-probability==0.17.0\r\n",
      "  Downloading tensorflow_probability-0.17.0-py2.py3-none-any.whl (6.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting imageio==2.20.0\r\n",
      "  Downloading imageio-2.20.0-py3-none-any.whl (3.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pygtrie==2.5.0\r\n",
      "  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\r\n",
      "Collecting sk-video==1.1.10\r\n",
      "  Downloading sk_video-1.1.10-py2.py3-none-any.whl (2.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow==2.9.1\r\n",
      "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting dm-sonnet==2.0.0\r\n",
      "  Downloading dm_sonnet-2.0.0-py3-none-any.whl (254 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.5/254.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting absl-py==1.2.0\r\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting wheel==0.37.1\r\n",
      "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\r\n",
      "Collecting tf-slim==1.1.0\r\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tabulate>=0.7.5\r\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\r\n",
      "Collecting wrapt>=1.11.1\r\n",
      "  Downloading wrapt-1.15.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting dm-tree>=0.1.1\r\n",
      "  Downloading dm_tree-0.1.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.8/153.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting audioread>=2.0.0\r\n",
      "  Downloading audioread-3.0.0.tar.gz (377 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting scikit-learn!=0.19.0,>=0.14.0\r\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting joblib>=0.12\r\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting decorator>=3.0.0\r\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\r\n",
      "Collecting resampy>=0.2.2\r\n",
      "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting soundfile>=0.9.0\r\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\r\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting cycler>=0.10\r\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\r\n",
      "Collecting packaging>=20.0\r\n",
      "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting python-dateutil>=2.7\r\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\r\n",
      "  Downloading kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pyparsing>=2.2.1\r\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting future\r\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting pandas>=0.18.1\r\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting intervaltree>=2.1.0\r\n",
      "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting attrs\r\n",
      "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting bokeh>=0.12.0\r\n",
      "  Downloading bokeh-2.4.3-py3-none-any.whl (18.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pydub\r\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\r\n",
      "Collecting IPython\r\n",
      "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m793.8/793.8 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting protobuf>=3.6.1\r\n",
      "  Downloading protobuf-4.22.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting llvmlite\r\n",
      "  Downloading llvmlite-0.32.1-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting setuptools\r\n",
      "  Downloading setuptools-67.6.1-py3-none-any.whl (1.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting networkx>=2.2\r\n",
      "  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tifffile>=2019.7.26\r\n",
      "  Downloading tifffile-2021.11.2-py3-none-any.whl (178 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting PyWavelets>=1.1.1\r\n",
      "  Downloading PyWavelets-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorboard<2.10,>=2.9\r\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\r\n",
      "  Downloading grpcio-1.53.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\r\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting keras<2.10.0,>=2.9.0rc0\r\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\r\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting typing-extensions>=3.6.6\r\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\r\n",
      "Collecting google-pasta>=0.1.1\r\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting h5py>=2.9.0\r\n",
      "  Downloading h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting protobuf>=3.6.1\r\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\r\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting flatbuffers<2,>=1.12\r\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\r\n",
      "Collecting astunparse>=1.6.0\r\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\n",
      "Collecting libclang>=13.0.0\r\n",
      "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\r\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\r\n",
      "Collecting keras-preprocessing>=1.1.1\r\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\r\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\r\n",
      "Collecting promise\r\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting etils[epath]\r\n",
      "  Downloading etils-0.9.0-py3-none-any.whl (140 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tqdm\r\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting dill\r\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting toml\r\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\r\n",
      "Collecting importlib-resources\r\n",
      "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\r\n",
      "Collecting tensorflow-metadata\r\n",
      "  Downloading tensorflow_metadata-1.12.0-py3-none-any.whl (52 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting requests>=2.19.0\r\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting cloudpickle>=1.3\r\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\r\n",
      "Collecting tornado>=5.1\r\n",
      "  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.0/424.0 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting PyYAML>=3.10\r\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting Jinja2>=2.9\r\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting sortedcontainers<3.0,>=2.0\r\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\r\n",
      "Collecting pytz>=2017.3\r\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\r\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\r\n",
      "  Downloading charset_normalizer-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (171 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.0/171.0 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\r\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting idna<4,>=2.5\r\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting resampy>=0.2.2\r\n",
      "  Downloading resampy-0.4.1-py3-none-any.whl (3.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading resampy-0.4.0-py3-none-any.whl (3.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading resampy-0.3.1-py3-none-any.whl (3.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\r\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\r\n",
      "Collecting cffi>=1.0\r\n",
      "  Downloading cffi-1.15.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.9/427.9 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting markdown>=2.6.8\r\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\r\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\r\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\r\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\r\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\r\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\r\n",
      "  Downloading google_auth-2.17.2-py2.py3-none-any.whl (178 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.2/178.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting zipp\r\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\r\n",
      "Collecting backcall\r\n",
      "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\r\n",
      "Collecting matplotlib-inline\r\n",
      "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\r\n",
      "Collecting traitlets>=4.2\r\n",
      "  Downloading traitlets-5.9.0-py3-none-any.whl (117 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pygments\r\n",
      "  Downloading Pygments-2.15.0-py3-none-any.whl (1.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pexpect>4.3\r\n",
      "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting jedi>=0.16\r\n",
      "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pickleshare\r\n",
      "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\r\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\r\n",
      "  Downloading prompt_toolkit-3.0.38-py3-none-any.whl (385 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting googleapis-common-protos<2,>=1.52.0\r\n",
      "  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pycparser\r\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\r\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\r\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\r\n",
      "Collecting rsa<5,>=3.1.4\r\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\r\n",
      "Collecting requests-oauthlib>=0.7.0\r\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\r\n",
      "Collecting parso<0.9.0,>=0.8.0\r\n",
      "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\r\n",
      "  Downloading MarkupSafe-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\r\n",
      "Collecting importlib-metadata>=4.4\r\n",
      "  Downloading importlib_metadata-6.3.0-py3-none-any.whl (22 kB)\r\n",
      "Collecting ptyprocess>=0.5\r\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\r\n",
      "Collecting wcwidth\r\n",
      "  Downloading wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\r\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\r\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\r\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: librosa, mir-eval, pretty-midi, python-rtmidi, audioread, intervaltree, future, promise\r\n",
      "  Building wheel for librosa (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for librosa: filename=librosa-0.7.2-py3-none-any.whl size=1612903 sha256=dae951cde5eae5605623daf7511d64636578042f41ccedcf33a2ee6d1c1950e7\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/b2/93/fb/ae0f9f40f5c4aaf1c8d73f2194581a8f249c7169670acfdf6d\r\n",
      "  Building wheel for mir-eval (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for mir-eval: filename=mir_eval-0.7-py3-none-any.whl size=100720 sha256=0d12160635313a4b42493562268f675a9873e11506be5b807b1b0c2c71ee7494\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/a5/db/bc282c9ac9ec68db9b0a1c1bb3184d91f83f8897d51b6db511\r\n",
      "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591954 sha256=9b1a6469fd073e38fe6c856b926881bde179a727ab74b02d02c5a60b64caba9e\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/57/18/93/2909d84ca856ef11cfb220d650eecb99a4723ce139bb553464\r\n",
      "  Building wheel for python-rtmidi (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for python-rtmidi: filename=python_rtmidi-1.1.2-cp37-cp37m-linux_x86_64.whl size=592254 sha256=55d6b44607291b18218e949973ebae9974a346a9e841666cfae2527c8361d32c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/ac/7c/bf918313c78466861766f5dc370c1ae591fca433531f8c5e09\r\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23706 sha256=71347d27b038596950a39142572459aef463529b921811cfa02222d3f0c8951f\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/dc/38/51/1be02cf6dbd3ef3e2e50a562071c9d574170c4f5096c09d8e1\r\n",
      "  Building wheel for intervaltree (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26118 sha256=0d19fe0dd2dfd6270d4e3314e8ea2318e60182fe73d569c0d45eb6f937e9c244\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/94/38/6fe5b0e582dca953d1ef8f2e8714fa1def8e5bf514c25c11fb\r\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492036 sha256=9dc0d3f8abb3de845f276c6d757bf4f1e2a7ba3e7b173f3551aa0d799a463636\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/52/2a/fc/520209cfa6448febd490720a0b09036cb367628f7c4e9cc172\r\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=61b0e8d340daa452ffe1ea54dfc08c5d5384397aee569edeecbc169852a6a599\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/9d/ad/15/e6d5c43a0f01b88ee5883bd249a18e09d72821e43b1c3e8187\r\n",
      "Successfully built librosa mir-eval pretty-midi python-rtmidi audioread intervaltree future promise\r\n",
      "Installing collected packages: wcwidth, tensorboard-plugin-wit, sortedcontainers, pytz, python-rtmidi, pygtrie, pydub, pyasn1, ptyprocess, pickleshare, mido, llvmlite, libclang, keras, flatbuffers, dm-tree, backcall, zipp, wrapt, wheel, urllib3, typing-extensions, traitlets, tqdm, tornado, toml, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, tabulate, six, setuptools, rsa, PyYAML, pyparsing, pygments, pycparser, pyasn1-modules, protobuf, prompt-toolkit, Pillow, pexpect, parso, packaging, oauthlib, numpy, networkx, MarkupSafe, joblib, intervaltree, idna, grpcio, gast, future, fonttools, etils, dill, decorator, cycler, cloudpickle, charset-normalizer, certifi, cachetools, audioread, attrs, absl-py, werkzeug, tifffile, tf-slim, tensorflow-probability, sox, scipy, requests, PyWavelets, python-dateutil, promise, pretty-midi, opt-einsum, numba, matplotlib-inline, kiwisolver, keras-preprocessing, Jinja2, jedi, importlib-resources, importlib-metadata, imageio, h5py, googleapis-common-protos, google-pasta, google-auth, dm-sonnet, cffi, astunparse, tensorflow-metadata, soundfile, sk-video, scikit-learn, scikit-image, resampy, requests-oauthlib, pandas, mir-eval, matplotlib, markdown, IPython, bokeh, tensorflow-datasets, librosa, google-auth-oauthlib, tensorboard, note-seq, tensorflow, magenta\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "dask-cudf 21.12.2 requires cupy-cuda115, which is not installed.\r\n",
      "cudf 21.12.2 requires cupy-cuda115, which is not installed.\r\n",
      "wfdb 4.1.0 requires SoundFile<0.12.0,>=0.10.0, but you have soundfile 0.12.1 which is incompatible.\r\n",
      "wasabi 1.1.1 requires typing-extensions<4.5.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\r\n",
      "thinc 8.1.9 requires typing-extensions<4.5.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\r\n",
      "tfx-bsl 1.12.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.83.0 which is incompatible.\r\n",
      "tfx-bsl 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\r\n",
      "tfx-bsl 1.12.0 requires tensorflow<3,>=2.11, but you have tensorflow 2.9.1 which is incompatible.\r\n",
      "tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\r\n",
      "tensorflow-transform 1.12.0 requires tensorflow<2.12,>=2.11.0, but you have tensorflow 2.9.1 which is incompatible.\r\n",
      "tensorflow-text 2.11.0 requires tensorflow<2.12,>=2.11.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.9.1 which is incompatible.\r\n",
      "tensorflow-serving-api 2.11.0 requires tensorflow<3,>=2.11.0, but you have tensorflow 2.9.1 which is incompatible.\r\n",
      "tensorflow-io 0.29.0 requires tensorflow-io-gcs-filesystem==0.29.0, but you have tensorflow-io-gcs-filesystem 0.32.0 which is incompatible.\r\n",
      "tensorflow-decision-forests 1.2.0 requires tensorflow~=2.11.0, but you have tensorflow 2.9.1 which is incompatible.\r\n",
      "stumpy 1.11.1 requires numba>=0.54, but you have numba 0.49.1 which is incompatible.\r\n",
      "spacy 3.5.1 requires typing-extensions<4.5.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\r\n",
      "pynndescent 0.5.8 requires numba>=0.51.2, but you have numba 0.49.1 which is incompatible.\r\n",
      "pydocstyle 6.3.0 requires importlib-metadata<5.0.0,>=2.0.0; python_version < \"3.8\", but you have importlib-metadata 6.3.0 which is incompatible.\r\n",
      "pandas-profiling 3.6.2 requires tqdm<4.65,>=4.48.2, but you have tqdm 4.65.0 which is incompatible.\r\n",
      "onnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "ibis-framework 2.1.1 requires importlib-metadata<5,>=4; python_version < \"3.8\", but you have importlib-metadata 6.3.0 which is incompatible.\r\n",
      "google-cloud-core 1.7.3 requires google-auth<2.0dev,>=1.24.0, but you have google-auth 2.17.2 which is incompatible.\r\n",
      "flake8 5.0.4 requires importlib-metadata<4.3,>=1.1.0; python_version < \"3.8\", but you have importlib-metadata 6.3.0 which is incompatible.\r\n",
      "distributed 2021.11.2 requires dask==2021.11.2, but you have dask 2022.2.0 which is incompatible.\r\n",
      "datashader 0.14.4 requires numba>=0.51, but you have numba 0.49.1 which is incompatible.\r\n",
      "dask-cudf 21.12.2 requires dask<=2021.11.2,>=2021.11.1, but you have dask 2022.2.0 which is incompatible.\r\n",
      "cudf 21.12.2 requires numba>=0.53.1, but you have numba 0.49.1 which is incompatible.\r\n",
      "confection 0.0.4 requires typing-extensions<4.5.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\r\n",
      "cmudict 1.0.13 requires importlib-metadata<6.0.0,>=5.1.0, but you have importlib-metadata 6.3.0 which is incompatible.\r\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 2.83.0 which is incompatible.\r\n",
      "boto3 1.26.100 requires botocore<1.30.0,>=1.29.100, but you have botocore 1.27.59 which is incompatible.\r\n",
      "apache-beam 2.44.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\r\n",
      "aiohttp 3.8.3 requires charset-normalizer<3.0,>=2.0, but you have charset-normalizer 3.1.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed IPython-7.34.0 Jinja2-3.1.2 MarkupSafe-2.1.2 Pillow-9.4.0 PyWavelets-1.3.0 PyYAML-6.0 absl-py-1.4.0 astunparse-1.6.3 attrs-22.2.0 audioread-3.0.0 backcall-0.2.0 bokeh-2.4.3 cachetools-5.3.0 certifi-2022.12.7 cffi-1.15.1 charset-normalizer-3.1.0 cloudpickle-2.2.1 cycler-0.11.0 decorator-5.1.1 dill-0.3.6 dm-sonnet-2.0.0 dm-tree-0.1.8 etils-0.9.0 flatbuffers-23.1.21 fonttools-4.38.0 future-0.18.3 gast-0.4.0 google-auth-2.17.2 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 googleapis-common-protos-1.59.0 grpcio-1.53.0 h5py-3.8.0 idna-3.4 imageio-2.25.0 importlib-metadata-6.3.0 importlib-resources-5.12.0 intervaltree-3.1.0 jedi-0.18.2 joblib-1.2.0 keras-2.11.0 keras-preprocessing-1.1.2 kiwisolver-1.4.4 libclang-16.0.0 librosa-0.10.0.post2 llvmlite-0.39.1 magenta-2.1.4 markdown-3.4.3 matplotlib-3.5.3 matplotlib-inline-0.1.6 mido-1.2.6 mir-eval-0.7 networkx-2.6.3 note-seq-0.0.3 numba-0.56.4 numpy-1.21.6 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-23.1 pandas-1.3.5 parso-0.8.3 pexpect-4.8.0 pickleshare-0.7.5 pretty-midi-0.2.9 promise-2.3 prompt-toolkit-3.0.38 protobuf-3.20.3 ptyprocess-0.7.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.21 pydub-0.25.1 pygments-2.15.0 pygtrie-2.5.0 pyparsing-3.0.9 python-dateutil-2.8.2 python-rtmidi-1.1.2 pytz-2023.3 requests-2.28.2 requests-oauthlib-1.3.1 resampy-0.3.1 rsa-4.9 scikit-image-0.19.3 scikit-learn-1.0.2 scipy-1.7.3 setuptools-67.6.1 six-1.16.0 sk-video-1.1.10 sortedcontainers-2.4.0 soundfile-0.12.1 sox-1.4.1 tabulate-0.9.0 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-datasets-4.8.2 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.32.0 tensorflow-metadata-1.12.0 tensorflow-probability-0.19.0 termcolor-2.2.0 tf-slim-1.1.0 threadpoolctl-3.1.0 tifffile-2021.11.2 toml-0.10.2 tornado-6.2 tqdm-4.65.0 traitlets-5.9.0 typing-extensions-4.5.0 urllib3-1.26.15 wcwidth-0.2.6 werkzeug-2.2.3 wheel-0.38.4 wrapt-1.15.0 zipp-3.15.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install -y build-essential libasound2-dev libjack-dev portaudio19-dev\n",
    "!pip install magenta --ignore-installed llvmlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b778b87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T20:21:45.665515Z",
     "iopub.status.busy": "2023-04-12T20:21:45.665193Z",
     "iopub.status.idle": "2023-04-12T20:23:29.023787Z",
     "shell.execute_reply": "2023-04-12T20:23:29.022494Z"
    },
    "papermill": {
     "duration": 103.390173,
     "end_time": "2023-04-12T20:23:29.026491",
     "exception": false,
     "start_time": "2023-04-12T20:21:45.636318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "8015\n"
     ]
    }
   ],
   "source": [
    "import magenta\n",
    "from magenta.scripts.convert_dir_to_note_sequences import convert_midi\n",
    "from magenta.music.sequences_lib import split_note_sequence\n",
    "import tensorflow.compat.v1 as tf\n",
    "import os\n",
    "\n",
    "prepath = \"/kaggle/input/popmidi0-1499/Pop0-1499\"\n",
    "all_ns = []\n",
    "count = 0\n",
    "for file in os.listdir(prepath):\n",
    "    try:\n",
    "        ns = convert_midi(prepath, \"midi\", os.path.join(prepath, file))\n",
    "\n",
    "    except: \n",
    "        print(\"Error on:\", file)\n",
    "        count += 1\n",
    "        continue\n",
    "    \n",
    "    ns_split = split_note_sequence(ns,\n",
    "                            20,\n",
    "                            False)\n",
    "#     print(len(ns_split))\n",
    "    count += 1\n",
    "    all_ns += ns_split\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "\n",
    "print(len(all_ns))\n",
    "!mkdir /kaggle/tmp/\n",
    "if (os.path.exists(\"/kaggle/tmp/pop.tfrecord\") == False):\n",
    "    f = open(\"/kaggle/tmp/pop.tfrecord\", \"w\")\n",
    "with tf.io.TFRecordWriter(\"/kaggle/tmp/pop.tfrecord\") as writer:\n",
    "    for sequence in all_ns:\n",
    "        writer.write(sequence.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "972aa8db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T20:23:29.089474Z",
     "iopub.status.busy": "2023-04-12T20:23:29.087782Z",
     "iopub.status.idle": "2023-04-12T20:23:29.146253Z",
     "shell.execute_reply": "2023-04-12T20:23:29.145281Z"
    },
    "papermill": {
     "duration": 0.093455,
     "end_time": "2023-04-12T20:23:29.148547",
     "exception": false,
     "start_time": "2023-04-12T20:23:29.055092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from note_seq.protobuf import music_pb2\n",
    "from magenta.pipelines import pipeline, note_sequence_pipelines, statistics\n",
    "from magenta.models import polyphony_rnn\n",
    "from magenta.models.polyphony_rnn import polyphony_lib, polyphony_model, polyphony_rnn_pipeline\n",
    "from magenta.pipelines import dag_pipeline\n",
    "from magenta.pipelines import event_sequence_pipeline\n",
    "from magenta.pipelines import pipelines_common\n",
    "\n",
    "\n",
    "class PolyphonicSequenceExtractor(pipeline.Pipeline):\n",
    "  \"\"\"Extracts polyphonic tracks from a quantized NoteSequence.\"\"\"\n",
    "\n",
    "  def __init__(self, min_steps, max_steps, name=None):\n",
    "    super(PolyphonicSequenceExtractor, self).__init__(\n",
    "        input_type=music_pb2.NoteSequence,\n",
    "        output_type=polyphony_lib.PolyphonicSequence,\n",
    "        name=name)\n",
    "    self._min_steps = min_steps\n",
    "    self._max_steps = max_steps\n",
    "\n",
    "  def transform(self, input_object):\n",
    "    quantized_sequence = input_object\n",
    "    poly_seqs, stats = polyphony_lib.extract_polyphonic_sequences(\n",
    "        quantized_sequence,\n",
    "        min_steps_discard=self._min_steps,\n",
    "        max_steps_discard=self._max_steps)\n",
    "    self._set_stats(stats)\n",
    "    return poly_seqs\n",
    "\n",
    "# changed from the library version: remove transposition from the pipeline\n",
    "def get_pipeline_mine(config, min_steps, max_steps, eval_ratio):\n",
    "  \"\"\"Returns the Pipeline instance which creates the RNN dataset.\n",
    "  Args:\n",
    "    config: An EventSequenceRnnConfig.\n",
    "    min_steps: Minimum number of steps for an extracted sequence.\n",
    "    max_steps: Maximum number of steps for an extracted sequence.\n",
    "    eval_ratio: Fraction of input to set aside for evaluation set.\n",
    "  Returns:\n",
    "    A pipeline.Pipeline instance.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  partitioner = pipelines_common.RandomPartition(\n",
    "      music_pb2.NoteSequence,\n",
    "      ['eval_poly_tracks', 'training_poly_tracks'],\n",
    "      [eval_ratio])\n",
    "  dag = {partitioner: dag_pipeline.DagInput(music_pb2.NoteSequence)}\n",
    "\n",
    "  for mode in ['eval', 'training']:\n",
    "    time_change_splitter = note_sequence_pipelines.TimeChangeSplitter(\n",
    "        name='TimeChangeSplitter_' + mode)\n",
    "    quantizer = note_sequence_pipelines.Quantizer(\n",
    "        steps_per_quarter=config.steps_per_quarter, name='Quantizer_' + mode)\n",
    "    poly_extractor = PolyphonicSequenceExtractor(\n",
    "        min_steps=min_steps, max_steps=max_steps, name='PolyExtractor_' + mode)\n",
    "    encoder_pipeline = event_sequence_pipeline.EncoderPipeline(\n",
    "        polyphony_lib.PolyphonicSequence, config.encoder_decoder,\n",
    "        name='EncoderPipeline_' + mode)\n",
    "\n",
    "    dag[time_change_splitter] = partitioner[mode + '_poly_tracks']\n",
    "    dag[quantizer] = time_change_splitter\n",
    "    dag[poly_extractor] = quantizer\n",
    "    dag[encoder_pipeline] = poly_extractor\n",
    "    dag[dag_pipeline.DagOutput(mode + '_poly_tracks')] = encoder_pipeline\n",
    "\n",
    "  return dag_pipeline.DAGPipeline(dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc6ca6ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T20:23:29.205308Z",
     "iopub.status.busy": "2023-04-12T20:23:29.204686Z",
     "iopub.status.idle": "2023-04-12T20:29:34.291689Z",
     "shell.execute_reply": "2023-04-12T20:29:34.290600Z"
    },
    "papermill": {
     "duration": 365.118788,
     "end_time": "2023-04-12T20:29:34.294683",
     "exception": false,
     "start_time": "2023-04-12T20:23:29.175895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_instance = get_pipeline_mine(\n",
    "      min_steps=80,  # 5 measures\n",
    "      max_steps=None,\n",
    "      eval_ratio=0.2,\n",
    "      config=polyphony_model.default_configs['polyphony'])\n",
    "\n",
    "input_dir = '/kaggle/tmp/pop.tfrecord'\n",
    "output_dir = '/kaggle/tmp/split'\n",
    "pipeline.run_pipeline_serial(\n",
    "  pipeline_instance,\n",
    "  pipeline.tf_record_iterator(input_dir, pipeline_instance.input_type),\n",
    "  output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f21c009",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T20:29:34.351820Z",
     "iopub.status.busy": "2023-04-12T20:29:34.351527Z",
     "iopub.status.idle": "2023-04-12T20:29:36.756182Z",
     "shell.execute_reply": "2023-04-12T20:29:36.755136Z"
    },
    "papermill": {
     "duration": 2.435386,
     "end_time": "2023-04-12T20:29:36.758380",
     "exception": false,
     "start_time": "2023-04-12T20:29:34.322994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520 6088\n"
     ]
    }
   ],
   "source": [
    "print(sum(1 for _ in tf.python_io.tf_record_iterator('/kaggle/tmp/split/eval_poly_tracks.tfrecord')),\n",
    "sum(1 for _ in tf.python_io.tf_record_iterator('/kaggle/tmp/split/training_poly_tracks.tfrecord')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfdd4d65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T20:29:36.816307Z",
     "iopub.status.busy": "2023-04-12T20:29:36.815991Z",
     "iopub.status.idle": "2023-04-12T21:12:40.643632Z",
     "shell.execute_reply": "2023-04-12T21:12:40.642262Z"
    },
    "papermill": {
     "duration": 2583.860384,
     "end_time": "2023-04-12T21:12:40.647007",
     "exception": false,
     "start_time": "2023-04-12T20:29:36.786623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 20:29:56.276891 133324266792768 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 20:29:56.277513 133324266792768 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-001/train\r\n",
      "W0412 20:29:56.305573 133324266792768 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 20:29:56.336270 133324266792768 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 20:29:56.340755 133324266792768 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0412 20:29:56.342299 133324266792768 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 20:29:56.343228 133324266792768 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 20:29:56.346760 133324266792768 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0412 20:29:56.366517 133324266792768 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0412 20:29:56.366658 133324266792768 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0412 20:29:56.412354 133324266792768 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0412 20:29:56.415644 133324266792768 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0412 20:29:56.415914 133324266792768 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0412 20:29:56.442056 133324266792768 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0412 20:29:56.508548 133324266792768 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0412 20:29:56.577364 133324266792768 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0412 20:29:56.723520 133324266792768 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0412 20:29:56.726492 133324266792768 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0412 20:29:57.521179 133324266792768 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0412 20:29:57.521429 133324266792768 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0412 20:29:57.630688 133324266792768 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0412 20:29:57.714796 133324266792768 monitored_session.py:243] Graph was finalized.\r\n",
      "I0412 20:30:00.906152 133324266792768 session_manager.py:527] Running local_init_op.\r\n",
      "I0412 20:30:00.913202 133324266792768 session_manager.py:530] Done running local_init_op.\r\n",
      "W0412 20:30:00.940190 133324266792768 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0412 20:30:01.631149 133324266792768 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0412 20:30:01.631632 133324266792768 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 20:30:01.851802 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-0.meta\r\n",
      "I0412 20:30:01.852064 133324266792768 saver.py:93] 400\r\n",
      "I0412 20:30:01.852186 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0412 20:30:01.852303 133324266792768 saver.py:93] 20100\r\n",
      "I0412 20:30:01.852413 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-0.index\r\n",
      "I0412 20:30:01.852545 133324266792768 saver.py:93] 20100\r\n",
      "I0412 20:30:01.852789 133324266792768 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0412 20:30:07.232061 133324266792768 basic_session_run_hooks.py:266] Accuracy = 0.003533096, Global Step = 0, Loss = 5.555868, Perplexity = 258.75153\r\n",
      "I0412 20:30:28.050048 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.2582099, Global Step = 10, Loss = 4.2354865, Perplexity = 69.09529 (20.818 sec)\r\n",
      "I0412 20:30:28.051112 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.480358\r\n",
      "I0412 20:30:47.681121 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.24530433, Global Step = 20, Loss = 4.1555114, Perplexity = 63.784576 (19.631 sec)\r\n",
      "I0412 20:30:47.682289 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.509393\r\n",
      "I0412 20:31:03.516703 133324266792768 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 29...\r\n",
      "I0412 20:31:03.516987 133324266792768 basic_session_run_hooks.py:633] Saving checkpoints for 29 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 20:31:03.628076 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-29.meta\r\n",
      "I0412 20:31:03.628296 133324266792768 saver.py:93] 400\r\n",
      "I0412 20:31:03.628410 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-29.data-00000-of-00001\r\n",
      "I0412 20:31:03.628505 133324266792768 saver.py:93] 20100\r\n",
      "I0412 20:31:03.628608 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-29.index\r\n",
      "I0412 20:31:03.628698 133324266792768 saver.py:93] 20100\r\n",
      "I0412 20:31:03.628878 133324266792768 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 29...\r\n",
      "I0412 20:31:08.258697 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.25418162, Global Step = 30, Loss = 4.1380687, Perplexity = 62.681644 (20.578 sec)\r\n",
      "I0412 20:31:08.259734 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.485969\r\n",
      "I0412 20:31:28.977327 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.25138903, Global Step = 40, Loss = 4.0794916, Perplexity = 59.11541 (20.719 sec)\r\n",
      "I0412 20:31:28.978407 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.482657\r\n",
      "I0412 20:31:49.356916 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.2485273, Global Step = 50, Loss = 4.051389, Perplexity = 57.47725 (20.380 sec)\r\n",
      "I0412 20:31:49.358395 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.490678\r\n",
      "I0412 20:32:04.615113 133324266792768 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 59...\r\n",
      "I0412 20:32:04.615366 133324266792768 basic_session_run_hooks.py:633] Saving checkpoints for 59 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 20:32:04.737060 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-59.index\r\n",
      "I0412 20:32:04.737275 133324266792768 saver.py:93] 0\r\n",
      "I0412 20:32:04.737382 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-59.data-00000-of-00001\r\n",
      "I0412 20:32:04.737472 133324266792768 saver.py:93] 19700\r\n",
      "I0412 20:32:04.737576 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-59.meta\r\n",
      "I0412 20:32:04.737661 133324266792768 saver.py:93] 20100\r\n",
      "I0412 20:32:04.737843 133324266792768 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 59...\r\n",
      "I0412 20:32:08.793327 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.24382983, Global Step = 60, Loss = 4.0240564, Perplexity = 55.927513 (19.436 sec)\r\n",
      "I0412 20:32:08.794419 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.514508\r\n",
      "I0412 20:32:28.800548 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.2692349, Global Step = 70, Loss = 3.9716244, Perplexity = 53.07067 (20.007 sec)\r\n",
      "I0412 20:32:28.801692 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.499819\r\n",
      "I0412 20:32:49.090361 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.29274932, Global Step = 80, Loss = 3.918974, Perplexity = 50.348755 (20.290 sec)\r\n",
      "I0412 20:32:49.091690 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.492857\r\n",
      "I0412 20:33:05.116475 133324266792768 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 89...\r\n",
      "I0412 20:33:05.116731 133324266792768 basic_session_run_hooks.py:633] Saving checkpoints for 89 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 20:33:05.216417 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-89.meta\r\n",
      "I0412 20:33:05.216639 133324266792768 saver.py:93] 400\r\n",
      "I0412 20:33:05.216754 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-89.data-00000-of-00001\r\n",
      "I0412 20:33:05.216850 133324266792768 saver.py:93] 20100\r\n",
      "I0412 20:33:05.216952 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-89.index\r\n",
      "I0412 20:33:05.217088 133324266792768 saver.py:93] 20100\r\n",
      "I0412 20:33:05.217272 133324266792768 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 89...\r\n",
      "I0412 20:33:09.440106 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.25740606, Global Step = 90, Loss = 3.9965696, Perplexity = 54.41118 (20.350 sec)\r\n",
      "I0412 20:33:09.441264 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.491408\r\n",
      "I0412 20:33:29.284418 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.494919\r\n",
      "I0412 20:33:29.285637 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.2722997, Global Step = 100, Loss = 3.9458284, Perplexity = 51.719166 (19.846 sec)\r\n",
      "I0412 20:33:29.286529 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.503898\r\n",
      "I0412 20:33:48.256145 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.2538787, Global Step = 110, Loss = 4.0347724, Perplexity = 56.530052 (18.970 sec)\r\n",
      "I0412 20:33:48.257185 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.527131\r\n",
      "I0412 20:34:05.384133 133324266792768 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 120...\r\n",
      "I0412 20:34:05.384407 133324266792768 basic_session_run_hooks.py:633] Saving checkpoints for 120 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 20:34:05.507863 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-120.meta\r\n",
      "I0412 20:34:05.508122 133324266792768 saver.py:93] 400\r\n",
      "I0412 20:34:05.508245 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-120.data-00000-of-00001\r\n",
      "I0412 20:34:05.508356 133324266792768 saver.py:93] 20100\r\n",
      "I0412 20:34:05.508453 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-120.index\r\n",
      "I0412 20:34:05.508552 133324266792768 saver.py:93] 20100\r\n",
      "I0412 20:34:05.508731 133324266792768 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 120...\r\n",
      "I0412 20:34:07.519176 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.25038007, Global Step = 120, Loss = 4.0049586, Perplexity = 54.869556 (19.263 sec)\r\n",
      "I0412 20:34:07.520218 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.519129\r\n",
      "I0412 20:34:28.189042 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.25245214, Global Step = 130, Loss = 4.0274477, Perplexity = 56.1175 (20.670 sec)\r\n",
      "I0412 20:34:28.190180 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.483794\r\n",
      "I0412 20:34:48.142025 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.24418074, Global Step = 140, Loss = 4.063643, Perplexity = 58.185894 (19.953 sec)\r\n",
      "I0412 20:34:48.143144 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.50118\r\n",
      "I0412 20:35:06.155710 133324266792768 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 150...\r\n",
      "I0412 20:35:06.155986 133324266792768 basic_session_run_hooks.py:633] Saving checkpoints for 150 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 20:35:06.267695 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-150.index\r\n",
      "I0412 20:35:06.267920 133324266792768 saver.py:93] 0\r\n",
      "I0412 20:35:06.268086 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-150.data-00000-of-00001\r\n",
      "I0412 20:35:06.268204 133324266792768 saver.py:93] 19700\r\n",
      "I0412 20:35:06.268280 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-150.meta\r\n",
      "I0412 20:35:06.268381 133324266792768 saver.py:93] 20100\r\n",
      "I0412 20:35:06.268664 133324266792768 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 150...\r\n",
      "I0412 20:35:08.102418 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.27584785, Global Step = 150, Loss = 3.927352, Perplexity = 50.772354 (19.960 sec)\r\n",
      "I0412 20:35:08.103535 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.500991\r\n",
      "I0412 20:35:27.162351 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.28239667, Global Step = 160, Loss = 3.9670863, Perplexity = 52.830376 (19.060 sec)\r\n",
      "I0412 20:35:27.163610 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.524656\r\n",
      "I0412 20:35:46.215453 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.25866523, Global Step = 170, Loss = 3.9169881, Perplexity = 50.248875 (19.053 sec)\r\n",
      "I0412 20:35:46.216572 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.524853\r\n",
      "I0412 20:36:06.654839 133324266792768 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 181...\r\n",
      "I0412 20:36:06.655093 133324266792768 basic_session_run_hooks.py:633] Saving checkpoints for 181 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 20:36:06.776062 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-181.meta\r\n",
      "I0412 20:36:06.776278 133324266792768 saver.py:93] 400\r\n",
      "I0412 20:36:06.776388 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-181.index\r\n",
      "I0412 20:36:06.776483 133324266792768 saver.py:93] 400\r\n",
      "I0412 20:36:06.776583 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-181.data-00000-of-00001\r\n",
      "I0412 20:36:06.776668 133324266792768 saver.py:93] 20100\r\n",
      "I0412 20:36:06.776846 133324266792768 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 181...\r\n",
      "I0412 20:36:06.777227 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.25247538, Global Step = 180, Loss = 3.9907622, Perplexity = 54.096107 (20.562 sec)\r\n",
      "I0412 20:36:06.778086 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.486346\r\n",
      "I0412 20:36:25.764453 133324266792768 basic_session_run_hooks.py:264] Accuracy = 0.26351824, Global Step = 190, Loss = 3.9454432, Perplexity = 51.69924 (18.987 sec)\r\n",
      "I0412 20:36:25.765513 133324266792768 basic_session_run_hooks.py:717] global_step/sec: 0.526664\r\n",
      "I0412 20:36:43.569216 133324266792768 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 200...\r\n",
      "I0412 20:36:43.569477 133324266792768 basic_session_run_hooks.py:633] Saving checkpoints for 200 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 20:36:43.687443 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-200.data-00000-of-00001\r\n",
      "I0412 20:36:43.687669 133324266792768 saver.py:93] 19700\r\n",
      "I0412 20:36:43.687787 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-200.meta\r\n",
      "I0412 20:36:43.687892 133324266792768 saver.py:93] 20100\r\n",
      "I0412 20:36:43.688014 133324266792768 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-200.index\r\n",
      "I0412 20:36:43.688143 133324266792768 saver.py:93] 20100\r\n",
      "I0412 20:36:43.688319 133324266792768 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 200...\r\n",
      "I0412 20:36:43.738522 133324266792768 events_rnn_train.py:113] Training complete.\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 20:37:02.524367 127835441821504 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 20:37:02.525238 127835441821504 polyphony_rnn_train.py:94] Train dir: /kaggle/tmp/polyphony_rnn/logdir/run-001/train\r\n",
      "I0412 20:37:02.525942 127835441821504 polyphony_rnn_train.py:99] Eval dir: /kaggle/tmp/polyphony_rnn/logdir/run-001/eval\r\n",
      "W0412 20:37:02.526759 127835441821504 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/polyphony_rnn_train\", line 8, in <module>\r\n",
      "    sys.exit(console_entry_point())\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 114, in console_entry_point\r\n",
      "    tf.app.run(main)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 36, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 308, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 254, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 104, in main\r\n",
      "    events_rnn_train.run_eval(build_graph_fn, train_dir, eval_dir, num_batches)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_train.py\", line 138, in run_eval\r\n",
      "    build_graph_fn()\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py\", line 113, in build\r\n",
      "    label_shape=label_shape, shuffle=mode == 'train')\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py\", line 66, in get_padded_batch\r\n",
      "    file_queue = tf.train.string_input_producer(file_list)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 357, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py\", line 253, in string_input_producer\r\n",
      "    raise ValueError(not_null_err)\r\n",
      "ValueError: string_input_producer requires a non-null input tensor\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 20:37:20.025897 126613238220608 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.005, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 20:37:20.026382 126613238220608 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-005/train\r\n",
      "W0412 20:37:20.027337 126613238220608 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 20:37:20.037641 126613238220608 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 20:37:20.039983 126613238220608 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0412 20:37:20.042004 126613238220608 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 20:37:20.043249 126613238220608 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 20:37:20.048122 126613238220608 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0412 20:37:20.058161 126613238220608 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0412 20:37:20.058335 126613238220608 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0412 20:37:20.105137 126613238220608 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0412 20:37:20.108098 126613238220608 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0412 20:37:20.108346 126613238220608 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0412 20:37:20.132521 126613238220608 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0412 20:37:20.155227 126613238220608 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0412 20:37:20.218386 126613238220608 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0412 20:37:20.394847 126613238220608 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0412 20:37:20.398059 126613238220608 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0412 20:37:21.202080 126613238220608 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0412 20:37:21.202360 126613238220608 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0412 20:37:21.311189 126613238220608 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0412 20:37:21.381438 126613238220608 monitored_session.py:243] Graph was finalized.\r\n",
      "I0412 20:37:22.671789 126613238220608 session_manager.py:527] Running local_init_op.\r\n",
      "I0412 20:37:22.678758 126613238220608 session_manager.py:530] Done running local_init_op.\r\n",
      "W0412 20:37:22.706010 126613238220608 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0412 20:37:23.402524 126613238220608 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0412 20:37:23.406661 126613238220608 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 20:37:23.608886 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-0.meta\r\n",
      "I0412 20:37:23.609254 126613238220608 saver.py:93] 400\r\n",
      "I0412 20:37:23.609566 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0412 20:37:23.609794 126613238220608 saver.py:93] 20100\r\n",
      "I0412 20:37:23.609998 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-0.index\r\n",
      "I0412 20:37:23.610211 126613238220608 saver.py:93] 20100\r\n",
      "I0412 20:37:23.610616 126613238220608 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0412 20:37:26.965298 126613238220608 basic_session_run_hooks.py:266] Accuracy = 0.004978485, Global Step = 0, Loss = 5.55658, Perplexity = 258.93576\r\n",
      "I0412 20:37:46.521055 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.2638653, Global Step = 10, Loss = 4.065998, Perplexity = 58.32309 (19.556 sec)\r\n",
      "I0412 20:37:46.522227 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.511354\r\n",
      "I0412 20:38:05.023463 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.2556308, Global Step = 20, Loss = 3.9237928, Perplexity = 50.59197 (18.502 sec)\r\n",
      "I0412 20:38:05.024750 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.540468\r\n",
      "I0412 20:38:25.037438 126613238220608 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 31...\r\n",
      "I0412 20:38:25.037698 126613238220608 basic_session_run_hooks.py:633] Saving checkpoints for 31 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 20:38:25.146941 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-31.meta\r\n",
      "I0412 20:38:25.147194 126613238220608 saver.py:93] 400\r\n",
      "I0412 20:38:25.147307 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-31.index\r\n",
      "I0412 20:38:25.147411 126613238220608 saver.py:93] 400\r\n",
      "I0412 20:38:25.147514 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-31.data-00000-of-00001\r\n",
      "I0412 20:38:25.147608 126613238220608 saver.py:93] 20100\r\n",
      "I0412 20:38:25.147788 126613238220608 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 31...\r\n",
      "I0412 20:38:25.148171 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.23479629, Global Step = 30, Loss = 4.028455, Perplexity = 56.17404 (20.125 sec)\r\n",
      "I0412 20:38:25.149060 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.496911\r\n",
      "I0412 20:38:46.061968 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.2453116, Global Step = 40, Loss = 3.9648926, Perplexity = 52.71461 (20.914 sec)\r\n",
      "I0412 20:38:46.063151 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.478147\r\n",
      "I0412 20:39:05.052587 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.25787765, Global Step = 50, Loss = 3.9502745, Perplexity = 51.949627 (18.991 sec)\r\n",
      "I0412 20:39:05.053668 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.526579\r\n",
      "I0412 20:39:24.797659 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.25865698, Global Step = 60, Loss = 3.9653435, Perplexity = 52.73838 (19.745 sec)\r\n",
      "I0412 20:39:24.798766 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.506454\r\n",
      "I0412 20:39:26.607869 126613238220608 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 62...\r\n",
      "I0412 20:39:26.608152 126613238220608 basic_session_run_hooks.py:633] Saving checkpoints for 62 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 20:39:26.717442 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-62.data-00000-of-00001\r\n",
      "I0412 20:39:26.717656 126613238220608 saver.py:93] 19700\r\n",
      "I0412 20:39:26.717775 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-62.meta\r\n",
      "I0412 20:39:26.717886 126613238220608 saver.py:93] 20100\r\n",
      "I0412 20:39:26.718003 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-62.index\r\n",
      "I0412 20:39:26.718129 126613238220608 saver.py:93] 20100\r\n",
      "I0412 20:39:26.718319 126613238220608 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 62...\r\n",
      "I0412 20:39:44.934715 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.27853793, Global Step = 70, Loss = 3.8760269, Perplexity = 48.2322 (20.137 sec)\r\n",
      "I0412 20:39:44.935776 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.496598\r\n",
      "I0412 20:40:04.096053 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.24654362, Global Step = 80, Loss = 3.9425602, Perplexity = 51.55041 (19.161 sec)\r\n",
      "I0412 20:40:04.097098 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.521886\r\n",
      "I0412 20:40:24.697271 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.23561372, Global Step = 90, Loss = 3.9790213, Perplexity = 53.464684 (20.601 sec)\r\n",
      "I0412 20:40:24.698231 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.48541\r\n",
      "I0412 20:40:26.698991 126613238220608 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 92...\r\n",
      "I0412 20:40:26.699271 126613238220608 basic_session_run_hooks.py:633] Saving checkpoints for 92 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 20:40:26.798836 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-92.index\r\n",
      "I0412 20:40:26.799087 126613238220608 saver.py:93] 0\r\n",
      "I0412 20:40:26.799203 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-92.data-00000-of-00001\r\n",
      "I0412 20:40:26.799322 126613238220608 saver.py:93] 19700\r\n",
      "I0412 20:40:26.799413 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-92.meta\r\n",
      "I0412 20:40:26.799518 126613238220608 saver.py:93] 20100\r\n",
      "I0412 20:40:26.799712 126613238220608 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 92...\r\n",
      "I0412 20:40:43.917503 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.507736\r\n",
      "I0412 20:40:43.918674 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.26403618, Global Step = 100, Loss = 3.9236646, Perplexity = 50.58548 (19.221 sec)\r\n",
      "I0412 20:40:43.919615 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.520253\r\n",
      "I0412 20:41:03.138671 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.25890297, Global Step = 110, Loss = 3.749361, Perplexity = 42.493923 (19.220 sec)\r\n",
      "I0412 20:41:03.139780 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.520287\r\n",
      "I0412 20:41:22.372478 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.26256436, Global Step = 120, Loss = 3.5914557, Perplexity = 36.286858 (19.234 sec)\r\n",
      "I0412 20:41:22.373534 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.51992\r\n",
      "I0412 20:41:28.552729 126613238220608 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 124...\r\n",
      "I0412 20:41:28.553001 126613238220608 basic_session_run_hooks.py:633] Saving checkpoints for 124 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 20:41:28.651596 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-124.index\r\n",
      "I0412 20:41:28.651808 126613238220608 saver.py:93] 0\r\n",
      "I0412 20:41:28.651923 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-124.data-00000-of-00001\r\n",
      "I0412 20:41:28.652093 126613238220608 saver.py:93] 19700\r\n",
      "I0412 20:41:28.652204 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-124.meta\r\n",
      "I0412 20:41:28.652322 126613238220608 saver.py:93] 20100\r\n",
      "I0412 20:41:28.652496 126613238220608 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 124...\r\n",
      "I0412 20:41:42.513366 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.23576649, Global Step = 130, Loss = 3.6713688, Perplexity = 39.30567 (20.141 sec)\r\n",
      "I0412 20:41:42.514503 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.4965\r\n",
      "I0412 20:42:03.438662 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.26954618, Global Step = 140, Loss = 3.4792132, Perplexity = 32.434193 (20.925 sec)\r\n",
      "I0412 20:42:03.439746 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.477892\r\n",
      "I0412 20:42:23.578280 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.28414193, Global Step = 150, Loss = 3.3746786, Perplexity = 29.214893 (20.140 sec)\r\n",
      "I0412 20:42:23.579387 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.496534\r\n",
      "I0412 20:42:29.671540 126613238220608 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 154...\r\n",
      "I0412 20:42:29.671811 126613238220608 basic_session_run_hooks.py:633] Saving checkpoints for 154 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 20:42:29.783495 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-154.index\r\n",
      "I0412 20:42:29.783722 126613238220608 saver.py:93] 0\r\n",
      "I0412 20:42:29.783839 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-154.meta\r\n",
      "I0412 20:42:29.783930 126613238220608 saver.py:93] 400\r\n",
      "I0412 20:42:29.784067 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-154.data-00000-of-00001\r\n",
      "I0412 20:42:29.784163 126613238220608 saver.py:93] 20100\r\n",
      "I0412 20:42:29.784350 126613238220608 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 154...\r\n",
      "I0412 20:42:42.822289 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.27215174, Global Step = 160, Loss = 3.355054, Perplexity = 28.64715 (19.244 sec)\r\n",
      "I0412 20:42:42.823433 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.519641\r\n",
      "I0412 20:43:01.985830 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.274498, Global Step = 170, Loss = 3.2782, Perplexity = 26.527977 (19.164 sec)\r\n",
      "I0412 20:43:01.986902 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.521826\r\n",
      "I0412 20:43:21.654013 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.25489613, Global Step = 180, Loss = 3.3309069, Perplexity = 27.96369 (19.668 sec)\r\n",
      "I0412 20:43:21.655276 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.50843\r\n",
      "I0412 20:43:30.088349 126613238220608 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 185...\r\n",
      "I0412 20:43:30.088618 126613238220608 basic_session_run_hooks.py:633] Saving checkpoints for 185 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 20:43:30.191290 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-185.meta\r\n",
      "I0412 20:43:30.191508 126613238220608 saver.py:93] 400\r\n",
      "I0412 20:43:30.191627 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-185.data-00000-of-00001\r\n",
      "I0412 20:43:30.191721 126613238220608 saver.py:93] 20100\r\n",
      "I0412 20:43:30.191825 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-185.index\r\n",
      "I0412 20:43:30.191915 126613238220608 saver.py:93] 20100\r\n",
      "I0412 20:43:30.192117 126613238220608 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 185...\r\n",
      "I0412 20:43:41.733124 126613238220608 basic_session_run_hooks.py:264] Accuracy = 0.27956855, Global Step = 190, Loss = 3.1627162, Perplexity = 23.634705 (20.079 sec)\r\n",
      "I0412 20:43:41.734171 126613238220608 basic_session_run_hooks.py:717] global_step/sec: 0.498035\r\n",
      "I0412 20:43:59.331412 126613238220608 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 200...\r\n",
      "I0412 20:43:59.331736 126613238220608 basic_session_run_hooks.py:633] Saving checkpoints for 200 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 20:43:59.455578 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-200.data-00000-of-00001\r\n",
      "I0412 20:43:59.455792 126613238220608 saver.py:93] 19700\r\n",
      "I0412 20:43:59.455910 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-200.meta\r\n",
      "I0412 20:43:59.456035 126613238220608 saver.py:93] 20100\r\n",
      "I0412 20:43:59.456146 126613238220608 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-200.index\r\n",
      "I0412 20:43:59.456241 126613238220608 saver.py:93] 20100\r\n",
      "I0412 20:43:59.456423 126613238220608 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 200...\r\n",
      "I0412 20:43:59.501697 126613238220608 events_rnn_train.py:113] Training complete.\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 20:44:17.072989 137023492261696 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 20:44:17.073515 137023492261696 polyphony_rnn_train.py:94] Train dir: /kaggle/tmp/polyphony_rnn/logdir/run-005/train\r\n",
      "I0412 20:44:17.074036 137023492261696 polyphony_rnn_train.py:99] Eval dir: /kaggle/tmp/polyphony_rnn/logdir/run-005/eval\r\n",
      "W0412 20:44:17.074719 137023492261696 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/polyphony_rnn_train\", line 8, in <module>\r\n",
      "    sys.exit(console_entry_point())\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 114, in console_entry_point\r\n",
      "    tf.app.run(main)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 36, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 308, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 254, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 104, in main\r\n",
      "    events_rnn_train.run_eval(build_graph_fn, train_dir, eval_dir, num_batches)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_train.py\", line 138, in run_eval\r\n",
      "    build_graph_fn()\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py\", line 113, in build\r\n",
      "    label_shape=label_shape, shuffle=mode == 'train')\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py\", line 66, in get_padded_batch\r\n",
      "    file_queue = tf.train.string_input_producer(file_list)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 357, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py\", line 253, in string_input_producer\r\n",
      "    raise ValueError(not_null_err)\r\n",
      "ValueError: string_input_producer requires a non-null input tensor\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 20:44:34.954987 123634453296960 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.01, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 20:44:34.955490 123634453296960 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-01/train\r\n",
      "W0412 20:44:34.956498 123634453296960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 20:44:34.964355 123634453296960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 20:44:34.966222 123634453296960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0412 20:44:34.967774 123634453296960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 20:44:34.968749 123634453296960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 20:44:34.972407 123634453296960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0412 20:44:34.979695 123634453296960 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0412 20:44:34.979839 123634453296960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0412 20:44:35.026330 123634453296960 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0412 20:44:35.030604 123634453296960 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0412 20:44:35.031015 123634453296960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0412 20:44:35.065594 123634453296960 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0412 20:44:35.088554 123634453296960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0412 20:44:35.155453 123634453296960 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0412 20:44:35.305376 123634453296960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0412 20:44:35.308467 123634453296960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0412 20:44:36.113610 123634453296960 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0412 20:44:36.113880 123634453296960 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0412 20:44:36.222480 123634453296960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0412 20:44:36.290987 123634453296960 monitored_session.py:243] Graph was finalized.\r\n",
      "I0412 20:44:37.491260 123634453296960 session_manager.py:527] Running local_init_op.\r\n",
      "I0412 20:44:37.498246 123634453296960 session_manager.py:530] Done running local_init_op.\r\n",
      "W0412 20:44:37.524021 123634453296960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0412 20:44:38.200500 123634453296960 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0412 20:44:38.202873 123634453296960 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 20:44:38.395623 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-0.meta\r\n",
      "I0412 20:44:38.396024 123634453296960 saver.py:93] 400\r\n",
      "I0412 20:44:38.396341 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0412 20:44:38.396532 123634453296960 saver.py:93] 20100\r\n",
      "I0412 20:44:38.396645 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-0.index\r\n",
      "I0412 20:44:38.396828 123634453296960 saver.py:93] 20100\r\n",
      "I0412 20:44:38.397188 123634453296960 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0412 20:44:41.856756 123634453296960 basic_session_run_hooks.py:266] Accuracy = 0.004385375, Global Step = 0, Loss = 5.556004, Perplexity = 258.78665\r\n",
      "I0412 20:45:02.156247 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.24973604, Global Step = 10, Loss = 4.239028, Perplexity = 69.340416 (20.299 sec)\r\n",
      "I0412 20:45:02.157420 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.49262\r\n",
      "I0412 20:45:20.813246 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.252648, Global Step = 20, Loss = 4.059756, Perplexity = 57.960155 (18.657 sec)\r\n",
      "I0412 20:45:20.814333 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.535995\r\n",
      "I0412 20:45:39.469267 123634453296960 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 30...\r\n",
      "I0412 20:45:39.469525 123634453296960 basic_session_run_hooks.py:633] Saving checkpoints for 30 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 20:45:39.578258 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-30.index\r\n",
      "I0412 20:45:39.578480 123634453296960 saver.py:93] 0\r\n",
      "I0412 20:45:39.578593 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-30.data-00000-of-00001\r\n",
      "I0412 20:45:39.578688 123634453296960 saver.py:93] 19700\r\n",
      "I0412 20:45:39.578789 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-30.meta\r\n",
      "I0412 20:45:39.578880 123634453296960 saver.py:93] 20100\r\n",
      "I0412 20:45:39.579082 123634453296960 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 30...\r\n",
      "I0412 20:45:41.664013 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.24870954, Global Step = 30, Loss = 4.0345573, Perplexity = 56.517895 (20.851 sec)\r\n",
      "I0412 20:45:41.665094 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.479599\r\n",
      "I0412 20:46:02.895199 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.23818284, Global Step = 40, Loss = 4.0101194, Perplexity = 55.153458 (21.231 sec)\r\n",
      "I0412 20:46:02.896409 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.471002\r\n",
      "I0412 20:46:22.926227 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.25802064, Global Step = 50, Loss = 3.975055, Perplexity = 53.253044 (20.031 sec)\r\n",
      "I0412 20:46:22.927363 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.499227\r\n",
      "I0412 20:46:39.730483 123634453296960 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 59...\r\n",
      "I0412 20:46:39.730744 123634453296960 basic_session_run_hooks.py:633] Saving checkpoints for 59 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 20:46:39.855427 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-59.index\r\n",
      "I0412 20:46:39.855663 123634453296960 saver.py:93] 0\r\n",
      "I0412 20:46:39.855777 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-59.data-00000-of-00001\r\n",
      "I0412 20:46:39.855872 123634453296960 saver.py:93] 19700\r\n",
      "I0412 20:46:39.855986 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-59.meta\r\n",
      "I0412 20:46:39.856087 123634453296960 saver.py:93] 20100\r\n",
      "I0412 20:46:39.856272 123634453296960 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 59...\r\n",
      "I0412 20:46:43.866093 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.2569778, Global Step = 60, Loss = 3.971012, Perplexity = 53.038185 (20.940 sec)\r\n",
      "I0412 20:46:43.867217 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.477558\r\n",
      "I0412 20:47:03.271037 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.25537094, Global Step = 70, Loss = 3.9932392, Perplexity = 54.230267 (19.405 sec)\r\n",
      "I0412 20:47:03.272182 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.515332\r\n",
      "I0412 20:47:22.498222 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.2632472, Global Step = 80, Loss = 3.918386, Perplexity = 50.319164 (19.227 sec)\r\n",
      "I0412 20:47:22.499323 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.520098\r\n",
      "I0412 20:47:41.115751 123634453296960 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 90...\r\n",
      "I0412 20:47:41.116052 123634453296960 basic_session_run_hooks.py:633] Saving checkpoints for 90 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 20:47:41.226749 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-90.meta\r\n",
      "I0412 20:47:41.227016 123634453296960 saver.py:93] 400\r\n",
      "I0412 20:47:41.227159 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-90.index\r\n",
      "I0412 20:47:41.227283 123634453296960 saver.py:93] 400\r\n",
      "I0412 20:47:41.227385 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-90.data-00000-of-00001\r\n",
      "I0412 20:47:41.227491 123634453296960 saver.py:93] 20100\r\n",
      "I0412 20:47:41.227715 123634453296960 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 90...\r\n",
      "I0412 20:47:43.445589 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.25405732, Global Step = 90, Loss = 4.002293, Perplexity = 54.72349 (20.947 sec)\r\n",
      "I0412 20:47:43.446670 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.477387\r\n",
      "I0412 20:48:03.768895 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.495263\r\n",
      "I0412 20:48:03.770112 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.2554233, Global Step = 100, Loss = 3.9512246, Perplexity = 51.999004 (20.325 sec)\r\n",
      "I0412 20:48:03.771114 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.492018\r\n",
      "I0412 20:48:23.618352 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.2602935, Global Step = 110, Loss = 3.93194, Perplexity = 51.005836 (19.848 sec)\r\n",
      "I0412 20:48:23.619473 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.50382\r\n",
      "I0412 20:48:43.053803 123634453296960 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 121...\r\n",
      "I0412 20:48:43.054147 123634453296960 basic_session_run_hooks.py:633] Saving checkpoints for 121 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 20:48:43.179292 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-121.meta\r\n",
      "I0412 20:48:43.179512 123634453296960 saver.py:93] 400\r\n",
      "I0412 20:48:43.179630 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-121.data-00000-of-00001\r\n",
      "I0412 20:48:43.179729 123634453296960 saver.py:93] 20100\r\n",
      "I0412 20:48:43.179827 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-121.index\r\n",
      "I0412 20:48:43.179916 123634453296960 saver.py:93] 20100\r\n",
      "I0412 20:48:43.180123 123634453296960 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 121...\r\n",
      "I0412 20:48:43.180501 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.2588434, Global Step = 120, Loss = 3.8991826, Perplexity = 49.36208 (19.562 sec)\r\n",
      "I0412 20:48:43.181364 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.511198\r\n",
      "I0412 20:49:03.602939 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.24913718, Global Step = 130, Loss = 4.012881, Perplexity = 55.305965 (20.422 sec)\r\n",
      "I0412 20:49:03.604050 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.489652\r\n",
      "I0412 20:49:25.148936 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.24966595, Global Step = 140, Loss = 4.0182858, Perplexity = 55.6057 (21.546 sec)\r\n",
      "I0412 20:49:25.150207 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.46412\r\n",
      "I0412 20:49:44.805950 123634453296960 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 151...\r\n",
      "I0412 20:49:44.806310 123634453296960 basic_session_run_hooks.py:633] Saving checkpoints for 151 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 20:49:44.967081 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-151.data-00000-of-00001\r\n",
      "I0412 20:49:44.967405 123634453296960 saver.py:93] 19700\r\n",
      "I0412 20:49:44.967662 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-151.index\r\n",
      "I0412 20:49:44.967843 123634453296960 saver.py:93] 19700\r\n",
      "I0412 20:49:44.967950 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-151.meta\r\n",
      "I0412 20:49:44.968152 123634453296960 saver.py:93] 20100\r\n",
      "I0412 20:49:44.968486 123634453296960 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 151...\r\n",
      "I0412 20:49:44.969032 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.2715565, Global Step = 150, Loss = 3.980473, Perplexity = 53.542355 (19.820 sec)\r\n",
      "I0412 20:49:44.970338 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.504537\r\n",
      "I0412 20:50:04.437473 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.28356013, Global Step = 160, Loss = 3.8613179, Perplexity = 47.527946 (19.469 sec)\r\n",
      "I0412 20:50:04.438500 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.513659\r\n",
      "I0412 20:50:24.698786 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.25720164, Global Step = 170, Loss = 3.9605536, Perplexity = 52.486374 (20.261 sec)\r\n",
      "I0412 20:50:24.699846 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.493551\r\n",
      "I0412 20:50:44.396655 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.25290307, Global Step = 180, Loss = 4.027431, Perplexity = 56.11656 (19.698 sec)\r\n",
      "I0412 20:50:44.397884 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.507665\r\n",
      "I0412 20:50:46.562222 123634453296960 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 182...\r\n",
      "I0412 20:50:46.562481 123634453296960 basic_session_run_hooks.py:633] Saving checkpoints for 182 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 20:50:46.665487 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-182.meta\r\n",
      "I0412 20:50:46.665721 123634453296960 saver.py:93] 400\r\n",
      "I0412 20:50:46.665836 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-182.data-00000-of-00001\r\n",
      "I0412 20:50:46.665931 123634453296960 saver.py:93] 20100\r\n",
      "I0412 20:50:46.666068 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-182.index\r\n",
      "I0412 20:50:46.666162 123634453296960 saver.py:93] 20100\r\n",
      "I0412 20:50:46.666348 123634453296960 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 182...\r\n",
      "I0412 20:51:05.786154 123634453296960 basic_session_run_hooks.py:264] Accuracy = 0.2503115, Global Step = 190, Loss = 3.9973, Perplexity = 54.450928 (21.390 sec)\r\n",
      "I0412 20:51:05.787421 123634453296960 basic_session_run_hooks.py:717] global_step/sec: 0.467518\r\n",
      "I0412 20:51:23.284784 123634453296960 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 200...\r\n",
      "I0412 20:51:23.285171 123634453296960 basic_session_run_hooks.py:633] Saving checkpoints for 200 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 20:51:23.417294 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-200.data-00000-of-00001\r\n",
      "I0412 20:51:23.417520 123634453296960 saver.py:93] 19700\r\n",
      "I0412 20:51:23.417636 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-200.meta\r\n",
      "I0412 20:51:23.417726 123634453296960 saver.py:93] 20100\r\n",
      "I0412 20:51:23.417829 123634453296960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-200.index\r\n",
      "I0412 20:51:23.417911 123634453296960 saver.py:93] 20100\r\n",
      "I0412 20:51:23.418132 123634453296960 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 200...\r\n",
      "I0412 20:51:23.468002 123634453296960 events_rnn_train.py:113] Training complete.\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 20:51:41.707317 135995635017536 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 20:51:41.707794 135995635017536 polyphony_rnn_train.py:94] Train dir: /kaggle/tmp/polyphony_rnn/logdir/run-01/train\r\n",
      "I0412 20:51:41.708385 135995635017536 polyphony_rnn_train.py:99] Eval dir: /kaggle/tmp/polyphony_rnn/logdir/run-01/eval\r\n",
      "W0412 20:51:41.709121 135995635017536 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/polyphony_rnn_train\", line 8, in <module>\r\n",
      "    sys.exit(console_entry_point())\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 114, in console_entry_point\r\n",
      "    tf.app.run(main)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 36, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 308, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 254, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 104, in main\r\n",
      "    events_rnn_train.run_eval(build_graph_fn, train_dir, eval_dir, num_batches)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_train.py\", line 138, in run_eval\r\n",
      "    build_graph_fn()\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py\", line 113, in build\r\n",
      "    label_shape=label_shape, shuffle=mode == 'train')\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py\", line 66, in get_padded_batch\r\n",
      "    file_queue = tf.train.string_input_producer(file_list)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 357, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py\", line 253, in string_input_producer\r\n",
      "    raise ValueError(not_null_err)\r\n",
      "ValueError: string_input_producer requires a non-null input tensor\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 20:51:59.630944 140393678399296 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.02, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 20:51:59.631466 140393678399296 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-02/train\r\n",
      "W0412 20:51:59.632663 140393678399296 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 20:51:59.641967 140393678399296 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 20:51:59.644032 140393678399296 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0412 20:51:59.646637 140393678399296 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 20:51:59.647704 140393678399296 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 20:51:59.651558 140393678399296 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0412 20:51:59.659198 140393678399296 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0412 20:51:59.659343 140393678399296 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0412 20:51:59.703782 140393678399296 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0412 20:51:59.707818 140393678399296 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0412 20:51:59.708183 140393678399296 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0412 20:51:59.746377 140393678399296 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0412 20:51:59.771091 140393678399296 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0412 20:51:59.837148 140393678399296 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0412 20:51:59.985653 140393678399296 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0412 20:51:59.988686 140393678399296 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0412 20:52:00.806954 140393678399296 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0412 20:52:00.807233 140393678399296 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0412 20:52:00.918246 140393678399296 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0412 20:52:00.989330 140393678399296 monitored_session.py:243] Graph was finalized.\r\n",
      "I0412 20:52:02.198460 140393678399296 session_manager.py:527] Running local_init_op.\r\n",
      "I0412 20:52:02.208603 140393678399296 session_manager.py:530] Done running local_init_op.\r\n",
      "W0412 20:52:02.235181 140393678399296 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0412 20:52:02.946304 140393678399296 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0412 20:52:02.949712 140393678399296 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0412 20:52:03.150712 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-0.meta\r\n",
      "I0412 20:52:03.151167 140393678399296 saver.py:93] 400\r\n",
      "I0412 20:52:03.151388 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0412 20:52:03.151554 140393678399296 saver.py:93] 20100\r\n",
      "I0412 20:52:03.151710 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-0.index\r\n",
      "I0412 20:52:03.151856 140393678399296 saver.py:93] 20100\r\n",
      "I0412 20:52:03.152141 140393678399296 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0412 20:52:06.440553 140393678399296 basic_session_run_hooks.py:266] Accuracy = 0.0023508752, Global Step = 0, Loss = 5.557634, Perplexity = 259.20877\r\n",
      "I0412 20:52:25.469638 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.27973983, Global Step = 10, Loss = 4.0744247, Perplexity = 58.816635 (19.029 sec)\r\n",
      "I0412 20:52:25.470777 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.525514\r\n",
      "I0412 20:52:44.008806 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.25675642, Global Step = 20, Loss = 4.003309, Perplexity = 54.779102 (18.539 sec)\r\n",
      "I0412 20:52:44.009936 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.539399\r\n",
      "I0412 20:53:04.420244 140393678399296 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 31...\r\n",
      "I0412 20:53:04.420524 140393678399296 basic_session_run_hooks.py:633] Saving checkpoints for 31 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0412 20:53:04.540237 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-31.meta\r\n",
      "I0412 20:53:04.540476 140393678399296 saver.py:93] 400\r\n",
      "I0412 20:53:04.540599 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-31.index\r\n",
      "I0412 20:53:04.540702 140393678399296 saver.py:93] 400\r\n",
      "I0412 20:53:04.540808 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-31.data-00000-of-00001\r\n",
      "I0412 20:53:04.540904 140393678399296 saver.py:93] 20100\r\n",
      "I0412 20:53:04.541120 140393678399296 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 31...\r\n",
      "I0412 20:53:04.541562 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.24949479, Global Step = 30, Loss = 3.9965434, Perplexity = 54.40975 (20.533 sec)\r\n",
      "I0412 20:53:04.542652 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.487027\r\n",
      "I0412 20:53:24.398043 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.24300125, Global Step = 40, Loss = 4.0324483, Perplexity = 56.398823 (19.857 sec)\r\n",
      "I0412 20:53:24.399115 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.503614\r\n",
      "I0412 20:53:44.904197 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.24100795, Global Step = 50, Loss = 4.0601444, Perplexity = 57.98268 (20.506 sec)\r\n",
      "I0412 20:53:44.905351 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.487657\r\n",
      "I0412 20:54:05.061679 140393678399296 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 61...\r\n",
      "I0412 20:54:05.061932 140393678399296 basic_session_run_hooks.py:633] Saving checkpoints for 61 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0412 20:54:05.192512 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-61.meta\r\n",
      "I0412 20:54:05.192742 140393678399296 saver.py:93] 400\r\n",
      "I0412 20:54:05.192854 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-61.data-00000-of-00001\r\n",
      "I0412 20:54:05.192945 140393678399296 saver.py:93] 20100\r\n",
      "I0412 20:54:05.193078 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-61.index\r\n",
      "I0412 20:54:05.193210 140393678399296 saver.py:93] 20100\r\n",
      "I0412 20:54:05.193419 140393678399296 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 61...\r\n",
      "I0412 20:54:05.193800 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.24769121, Global Step = 60, Loss = 4.0306363, Perplexity = 56.296722 (20.290 sec)\r\n",
      "I0412 20:54:05.194722 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.492868\r\n",
      "I0412 20:54:23.448419 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.27198637, Global Step = 70, Loss = 3.956177, Perplexity = 52.25716 (18.255 sec)\r\n",
      "I0412 20:54:23.449506 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.547802\r\n",
      "I0412 20:54:43.354718 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.2750421, Global Step = 80, Loss = 3.9811506, Perplexity = 53.578648 (19.906 sec)\r\n",
      "I0412 20:54:43.355844 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.502353\r\n",
      "I0412 20:55:03.640624 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.24937102, Global Step = 90, Loss = 4.082617, Perplexity = 59.300446 (20.286 sec)\r\n",
      "I0412 20:55:03.641716 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.492954\r\n",
      "I0412 20:55:05.665282 140393678399296 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 92...\r\n",
      "I0412 20:55:05.665546 140393678399296 basic_session_run_hooks.py:633] Saving checkpoints for 92 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0412 20:55:05.771672 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-92.index\r\n",
      "I0412 20:55:05.771890 140393678399296 saver.py:93] 0\r\n",
      "I0412 20:55:05.772024 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-92.data-00000-of-00001\r\n",
      "I0412 20:55:05.772149 140393678399296 saver.py:93] 19700\r\n",
      "I0412 20:55:05.772237 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-92.meta\r\n",
      "I0412 20:55:05.772327 140393678399296 saver.py:93] 20100\r\n",
      "I0412 20:55:05.772510 140393678399296 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 92...\r\n",
      "I0412 20:55:22.903744 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.508999\r\n",
      "I0412 20:55:22.904898 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.2550917, Global Step = 100, Loss = 3.9698741, Perplexity = 52.977863 (19.264 sec)\r\n",
      "I0412 20:55:22.909223 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.519009\r\n",
      "I0412 20:55:42.586153 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.25038484, Global Step = 110, Loss = 3.9383814, Perplexity = 51.335445 (19.681 sec)\r\n",
      "I0412 20:55:42.587180 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.508183\r\n",
      "I0412 20:56:01.554761 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.25942883, Global Step = 120, Loss = 4.0055256, Perplexity = 54.900673 (18.969 sec)\r\n",
      "I0412 20:56:01.555853 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.527185\r\n",
      "I0412 20:56:06.101980 140393678399296 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 123...\r\n",
      "I0412 20:56:06.102262 140393678399296 basic_session_run_hooks.py:633] Saving checkpoints for 123 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0412 20:56:06.211693 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-123.index\r\n",
      "I0412 20:56:06.211925 140393678399296 saver.py:93] 0\r\n",
      "I0412 20:56:06.212074 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-123.meta\r\n",
      "I0412 20:56:06.212193 140393678399296 saver.py:93] 400\r\n",
      "I0412 20:56:06.212283 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-123.data-00000-of-00001\r\n",
      "I0412 20:56:06.212373 140393678399296 saver.py:93] 20100\r\n",
      "I0412 20:56:06.212569 140393678399296 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 123...\r\n",
      "I0412 20:56:21.867042 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.24183254, Global Step = 130, Loss = 4.0332913, Perplexity = 56.44639 (20.312 sec)\r\n",
      "I0412 20:56:21.868144 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.492312\r\n",
      "I0412 20:56:42.194166 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.26281753, Global Step = 140, Loss = 3.9596384, Perplexity = 52.438354 (20.327 sec)\r\n",
      "I0412 20:56:42.195247 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.491954\r\n",
      "I0412 20:57:01.728196 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.25630403, Global Step = 150, Loss = 4.0024323, Perplexity = 54.731113 (19.534 sec)\r\n",
      "I0412 20:57:01.729359 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.511925\r\n",
      "I0412 20:57:07.574245 140393678399296 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 154...\r\n",
      "I0412 20:57:07.574553 140393678399296 basic_session_run_hooks.py:633] Saving checkpoints for 154 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0412 20:57:07.674794 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-154.index\r\n",
      "I0412 20:57:07.675028 140393678399296 saver.py:93] 0\r\n",
      "I0412 20:57:07.675143 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-154.meta\r\n",
      "I0412 20:57:07.675249 140393678399296 saver.py:93] 400\r\n",
      "I0412 20:57:07.675343 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-154.data-00000-of-00001\r\n",
      "I0412 20:57:07.675431 140393678399296 saver.py:93] 20100\r\n",
      "I0412 20:57:07.675613 140393678399296 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 154...\r\n",
      "I0412 20:57:21.205877 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.24489203, Global Step = 160, Loss = 3.984761, Perplexity = 53.772438 (19.478 sec)\r\n",
      "I0412 20:57:21.207009 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.51341\r\n",
      "I0412 20:57:39.247276 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.25071323, Global Step = 170, Loss = 3.996355, Perplexity = 54.399506 (18.041 sec)\r\n",
      "I0412 20:57:39.248412 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.55428\r\n",
      "I0412 20:57:59.671048 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.24647748, Global Step = 180, Loss = 4.096739, Perplexity = 60.14383 (20.424 sec)\r\n",
      "I0412 20:57:59.672117 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.489628\r\n",
      "I0412 20:58:09.451588 140393678399296 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 186...\r\n",
      "I0412 20:58:09.451849 140393678399296 basic_session_run_hooks.py:633] Saving checkpoints for 186 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0412 20:58:09.555345 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-186.meta\r\n",
      "I0412 20:58:09.555582 140393678399296 saver.py:93] 400\r\n",
      "I0412 20:58:09.555707 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-186.data-00000-of-00001\r\n",
      "I0412 20:58:09.555819 140393678399296 saver.py:93] 20100\r\n",
      "I0412 20:58:09.555918 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-186.index\r\n",
      "I0412 20:58:09.556042 140393678399296 saver.py:93] 20100\r\n",
      "I0412 20:58:09.556231 140393678399296 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 186...\r\n",
      "I0412 20:58:20.021005 140393678399296 basic_session_run_hooks.py:264] Accuracy = 0.23168677, Global Step = 190, Loss = 4.008799, Perplexity = 55.08068 (20.350 sec)\r\n",
      "I0412 20:58:20.022029 140393678399296 basic_session_run_hooks.py:717] global_step/sec: 0.491402\r\n",
      "I0412 20:58:37.340707 140393678399296 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 200...\r\n",
      "I0412 20:58:37.341002 140393678399296 basic_session_run_hooks.py:633] Saving checkpoints for 200 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0412 20:58:37.489723 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-200.data-00000-of-00001\r\n",
      "I0412 20:58:37.489952 140393678399296 saver.py:93] 19700\r\n",
      "I0412 20:58:37.490093 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-200.meta\r\n",
      "I0412 20:58:37.490214 140393678399296 saver.py:93] 20100\r\n",
      "I0412 20:58:37.490310 140393678399296 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-200.index\r\n",
      "I0412 20:58:37.490413 140393678399296 saver.py:93] 20100\r\n",
      "I0412 20:58:37.490609 140393678399296 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 200...\r\n",
      "I0412 20:58:37.537864 140393678399296 events_rnn_train.py:113] Training complete.\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 20:58:55.977729 139170549118784 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.05, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 20:58:55.978170 139170549118784 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-05/train\r\n",
      "W0412 20:58:55.979237 139170549118784 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 20:58:55.987060 139170549118784 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 20:58:55.988887 139170549118784 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0412 20:58:55.990453 139170549118784 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 20:58:55.991422 139170549118784 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 20:58:55.996649 139170549118784 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0412 20:58:56.003852 139170549118784 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0412 20:58:56.004009 139170549118784 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0412 20:58:56.045135 139170549118784 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0412 20:58:56.049147 139170549118784 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0412 20:58:56.049508 139170549118784 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0412 20:58:56.073241 139170549118784 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0412 20:58:56.095256 139170549118784 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0412 20:58:56.159205 139170549118784 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0412 20:58:56.305117 139170549118784 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0412 20:58:56.308583 139170549118784 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0412 20:58:57.088728 139170549118784 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0412 20:58:57.088996 139170549118784 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0412 20:58:57.196982 139170549118784 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0412 20:58:57.297225 139170549118784 monitored_session.py:243] Graph was finalized.\r\n",
      "I0412 20:58:58.569114 139170549118784 session_manager.py:527] Running local_init_op.\r\n",
      "I0412 20:58:58.576090 139170549118784 session_manager.py:530] Done running local_init_op.\r\n",
      "W0412 20:58:58.602239 139170549118784 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0412 20:58:59.288058 139170549118784 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0412 20:58:59.289330 139170549118784 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt.\r\n",
      "I0412 20:58:59.474298 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-0.meta\r\n",
      "I0412 20:58:59.474586 139170549118784 saver.py:93] 400\r\n",
      "I0412 20:58:59.474802 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0412 20:58:59.475007 139170549118784 saver.py:93] 20100\r\n",
      "I0412 20:58:59.475189 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-0.index\r\n",
      "I0412 20:58:59.475371 139170549118784 saver.py:93] 20100\r\n",
      "I0412 20:58:59.475609 139170549118784 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0412 20:59:02.830081 139170549118784 basic_session_run_hooks.py:266] Accuracy = 0.002980626, Global Step = 0, Loss = 5.5584917, Perplexity = 259.4312\r\n",
      "I0412 20:59:22.454905 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.24530523, Global Step = 10, Loss = 4.806288, Perplexity = 122.276855 (19.625 sec)\r\n",
      "I0412 20:59:22.456021 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.509556\r\n",
      "I0412 20:59:41.189670 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.2613571, Global Step = 20, Loss = 4.1531963, Perplexity = 63.637077 (18.735 sec)\r\n",
      "I0412 20:59:41.190772 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.533767\r\n",
      "I0412 21:00:01.323889 139170549118784 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 31...\r\n",
      "I0412 21:00:01.324155 139170549118784 basic_session_run_hooks.py:633] Saving checkpoints for 31 into /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt.\r\n",
      "I0412 21:00:01.462033 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-31.meta\r\n",
      "I0412 21:00:01.462279 139170549118784 saver.py:93] 400\r\n",
      "I0412 21:00:01.462406 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-31.index\r\n",
      "I0412 21:00:01.462536 139170549118784 saver.py:93] 400\r\n",
      "I0412 21:00:01.462641 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-31.data-00000-of-00001\r\n",
      "I0412 21:00:01.462770 139170549118784 saver.py:93] 20100\r\n",
      "I0412 21:00:01.462996 139170549118784 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 31...\r\n",
      "I0412 21:00:01.463274 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.25687844, Global Step = 30, Loss = 3.9793067, Perplexity = 53.479946 (20.274 sec)\r\n",
      "I0412 21:00:01.464306 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.493254\r\n",
      "I0412 21:00:21.228210 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.25610188, Global Step = 40, Loss = 4.0808983, Perplexity = 59.198624 (19.765 sec)\r\n",
      "I0412 21:00:21.229293 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.505945\r\n",
      "I0412 21:00:41.298191 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.25292653, Global Step = 50, Loss = 3.9815435, Perplexity = 53.5997 (20.070 sec)\r\n",
      "I0412 21:00:41.299207 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.498259\r\n",
      "I0412 21:01:00.893460 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.27037868, Global Step = 60, Loss = 3.9587705, Perplexity = 52.39287 (19.595 sec)\r\n",
      "I0412 21:01:00.894656 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.510322\r\n",
      "I0412 21:01:02.707938 139170549118784 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 62...\r\n",
      "I0412 21:01:02.708254 139170549118784 basic_session_run_hooks.py:633] Saving checkpoints for 62 into /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt.\r\n",
      "I0412 21:01:02.815302 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-62.data-00000-of-00001\r\n",
      "I0412 21:01:02.815523 139170549118784 saver.py:93] 19700\r\n",
      "I0412 21:01:02.815649 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-62.meta\r\n",
      "I0412 21:01:02.815752 139170549118784 saver.py:93] 20100\r\n",
      "I0412 21:01:02.815856 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-62.index\r\n",
      "I0412 21:01:02.815948 139170549118784 saver.py:93] 20100\r\n",
      "I0412 21:01:02.816197 139170549118784 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 62...\r\n",
      "I0412 21:01:20.681385 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.26293737, Global Step = 70, Loss = 4.0370793, Perplexity = 56.660614 (19.788 sec)\r\n",
      "I0412 21:01:20.683087 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.505346\r\n",
      "I0412 21:01:40.092854 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.24638848, Global Step = 80, Loss = 4.033405, Perplexity = 56.452793 (19.411 sec)\r\n",
      "I0412 21:01:40.094047 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.515173\r\n",
      "I0412 21:02:00.619154 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.252522, Global Step = 90, Loss = 4.0607963, Perplexity = 58.020493 (20.526 sec)\r\n",
      "I0412 21:02:00.620318 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.48718\r\n",
      "I0412 21:02:04.695652 139170549118784 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 93...\r\n",
      "I0412 21:02:04.695899 139170549118784 basic_session_run_hooks.py:633] Saving checkpoints for 93 into /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt.\r\n",
      "I0412 21:02:04.793274 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-93.data-00000-of-00001\r\n",
      "I0412 21:02:04.793491 139170549118784 saver.py:93] 19700\r\n",
      "I0412 21:02:04.793621 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-93.meta\r\n",
      "I0412 21:02:04.793732 139170549118784 saver.py:93] 20100\r\n",
      "I0412 21:02:04.793829 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-93.index\r\n",
      "I0412 21:02:04.793919 139170549118784 saver.py:93] 20100\r\n",
      "I0412 21:02:04.794142 139170549118784 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 93...\r\n",
      "I0412 21:02:19.663149 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.508043\r\n",
      "I0412 21:02:19.664281 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.25529188, Global Step = 100, Loss = 4.022793, Perplexity = 55.856888 (19.045 sec)\r\n",
      "I0412 21:02:19.665183 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.525075\r\n",
      "I0412 21:02:39.065957 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.25492895, Global Step = 110, Loss = 3.9833808, Perplexity = 53.69827 (19.402 sec)\r\n",
      "I0412 21:02:39.067087 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.515415\r\n",
      "I0412 21:02:58.495062 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.25000614, Global Step = 120, Loss = 3.975414, Perplexity = 53.272167 (19.429 sec)\r\n",
      "I0412 21:02:58.496224 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.51469\r\n",
      "I0412 21:03:04.710127 139170549118784 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 124...\r\n",
      "I0412 21:03:04.710389 139170549118784 basic_session_run_hooks.py:633] Saving checkpoints for 124 into /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt.\r\n",
      "I0412 21:03:04.807954 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-124.index\r\n",
      "I0412 21:03:04.808208 139170549118784 saver.py:93] 0\r\n",
      "I0412 21:03:04.808325 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-124.data-00000-of-00001\r\n",
      "I0412 21:03:04.808436 139170549118784 saver.py:93] 19700\r\n",
      "I0412 21:03:04.808553 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-124.meta\r\n",
      "I0412 21:03:04.808649 139170549118784 saver.py:93] 20100\r\n",
      "I0412 21:03:04.808841 139170549118784 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 124...\r\n",
      "I0412 21:03:18.644294 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.25388095, Global Step = 130, Loss = 3.9368422, Perplexity = 51.25649 (20.149 sec)\r\n",
      "I0412 21:03:18.645482 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.496296\r\n",
      "I0412 21:03:39.519951 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.24722447, Global Step = 140, Loss = 3.9483802, Perplexity = 51.85131 (20.876 sec)\r\n",
      "I0412 21:03:39.521126 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.479027\r\n",
      "I0412 21:03:59.932740 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.25348216, Global Step = 150, Loss = 3.7923086, Perplexity = 44.358685 (20.413 sec)\r\n",
      "I0412 21:03:59.933861 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.489891\r\n",
      "I0412 21:04:05.974834 139170549118784 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 154...\r\n",
      "I0412 21:04:05.975194 139170549118784 basic_session_run_hooks.py:633] Saving checkpoints for 154 into /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt.\r\n",
      "I0412 21:04:06.075057 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-154.index\r\n",
      "I0412 21:04:06.075269 139170549118784 saver.py:93] 0\r\n",
      "I0412 21:04:06.075386 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-154.meta\r\n",
      "I0412 21:04:06.075492 139170549118784 saver.py:93] 400\r\n",
      "I0412 21:04:06.075604 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-154.data-00000-of-00001\r\n",
      "I0412 21:04:06.075703 139170549118784 saver.py:93] 20100\r\n",
      "I0412 21:04:06.075892 139170549118784 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 154...\r\n",
      "I0412 21:04:19.619011 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.25866973, Global Step = 160, Loss = 3.7966247, Perplexity = 44.550552 (19.686 sec)\r\n",
      "I0412 21:04:19.620087 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.50797\r\n",
      "I0412 21:04:38.771457 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.26394245, Global Step = 170, Loss = 3.674639, Perplexity = 39.434418 (19.152 sec)\r\n",
      "I0412 21:04:38.772534 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.522126\r\n",
      "I0412 21:04:58.714453 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.25398934, Global Step = 180, Loss = 3.7623227, Perplexity = 43.048298 (19.943 sec)\r\n",
      "I0412 21:04:58.715661 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.501426\r\n",
      "I0412 21:05:07.309704 139170549118784 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 185...\r\n",
      "I0412 21:05:07.309983 139170549118784 basic_session_run_hooks.py:633] Saving checkpoints for 185 into /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt.\r\n",
      "I0412 21:05:07.418792 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-185.meta\r\n",
      "I0412 21:05:07.419039 139170549118784 saver.py:93] 400\r\n",
      "I0412 21:05:07.419167 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-185.data-00000-of-00001\r\n",
      "I0412 21:05:07.419291 139170549118784 saver.py:93] 20100\r\n",
      "I0412 21:05:07.419383 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-185.index\r\n",
      "I0412 21:05:07.419480 139170549118784 saver.py:93] 20100\r\n",
      "I0412 21:05:07.419675 139170549118784 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 185...\r\n",
      "I0412 21:05:19.567120 139170549118784 basic_session_run_hooks.py:264] Accuracy = 0.25157824, Global Step = 190, Loss = 3.6479409, Perplexity = 38.395523 (20.853 sec)\r\n",
      "I0412 21:05:19.568235 139170549118784 basic_session_run_hooks.py:717] global_step/sec: 0.479557\r\n",
      "I0412 21:05:37.371330 139170549118784 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 200...\r\n",
      "I0412 21:05:37.371581 139170549118784 basic_session_run_hooks.py:633] Saving checkpoints for 200 into /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt.\r\n",
      "I0412 21:05:37.471537 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-200.data-00000-of-00001\r\n",
      "I0412 21:05:37.471780 139170549118784 saver.py:93] 19700\r\n",
      "I0412 21:05:37.471905 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-200.meta\r\n",
      "I0412 21:05:37.472026 139170549118784 saver.py:93] 20100\r\n",
      "I0412 21:05:37.472147 139170549118784 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-200.index\r\n",
      "I0412 21:05:37.472257 139170549118784 saver.py:93] 20100\r\n",
      "I0412 21:05:37.472443 139170549118784 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 200...\r\n",
      "I0412 21:05:37.520016 139170549118784 events_rnn_train.py:113] Training complete.\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 21:05:55.047310 127376437724992 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.1, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 21:05:55.047793 127376437724992 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-1/train\r\n",
      "W0412 21:05:55.048893 127376437724992 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 21:05:55.056926 127376437724992 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 21:05:55.058990 127376437724992 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0412 21:05:55.060547 127376437724992 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 21:05:55.061503 127376437724992 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 21:05:55.065176 127376437724992 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0412 21:05:55.072571 127376437724992 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0412 21:05:55.072715 127376437724992 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0412 21:05:55.117627 127376437724992 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0412 21:05:55.121712 127376437724992 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0412 21:05:55.122077 127376437724992 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0412 21:05:55.160004 127376437724992 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0412 21:05:55.184454 127376437724992 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0412 21:05:55.265748 127376437724992 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0412 21:05:55.477226 127376437724992 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0412 21:05:55.480749 127376437724992 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0412 21:05:56.279066 127376437724992 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0412 21:05:56.279321 127376437724992 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0412 21:05:56.390880 127376437724992 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0412 21:05:56.462164 127376437724992 monitored_session.py:243] Graph was finalized.\r\n",
      "I0412 21:05:57.665018 127376437724992 session_manager.py:527] Running local_init_op.\r\n",
      "I0412 21:05:57.672026 127376437724992 session_manager.py:530] Done running local_init_op.\r\n",
      "W0412 21:05:57.699281 127376437724992 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0412 21:05:58.375901 127376437724992 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0412 21:05:58.377190 127376437724992 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt.\r\n",
      "I0412 21:05:58.579588 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-0.meta\r\n",
      "I0412 21:05:58.580051 127376437724992 saver.py:93] 400\r\n",
      "I0412 21:05:58.580345 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0412 21:05:58.580541 127376437724992 saver.py:93] 20100\r\n",
      "I0412 21:05:58.580651 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-0.index\r\n",
      "I0412 21:05:58.580829 127376437724992 saver.py:93] 20100\r\n",
      "I0412 21:05:58.581184 127376437724992 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0412 21:06:02.175469 127376437724992 basic_session_run_hooks.py:266] Accuracy = 0.00466704, Global Step = 0, Loss = 5.5559382, Perplexity = 258.76965\r\n",
      "I0412 21:06:21.741668 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.22564505, Global Step = 10, Loss = 4.269054, Perplexity = 71.454 (19.566 sec)\r\n",
      "I0412 21:06:21.742689 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.511087\r\n",
      "I0412 21:06:41.121525 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.2435599, Global Step = 20, Loss = 4.1298857, Perplexity = 62.170815 (19.380 sec)\r\n",
      "I0412 21:06:41.122623 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.515998\r\n",
      "I0412 21:06:58.806627 127376437724992 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 30...\r\n",
      "I0412 21:06:58.806898 127376437724992 basic_session_run_hooks.py:633] Saving checkpoints for 30 into /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt.\r\n",
      "I0412 21:06:58.917095 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-30.index\r\n",
      "I0412 21:06:58.917321 127376437724992 saver.py:93] 0\r\n",
      "I0412 21:06:58.917436 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-30.data-00000-of-00001\r\n",
      "I0412 21:06:58.917540 127376437724992 saver.py:93] 19700\r\n",
      "I0412 21:06:58.917643 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-30.meta\r\n",
      "I0412 21:06:58.917733 127376437724992 saver.py:93] 20100\r\n",
      "I0412 21:06:58.917921 127376437724992 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 30...\r\n",
      "I0412 21:07:00.744622 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.2460039, Global Step = 30, Loss = 4.0334306, Perplexity = 56.45425 (19.623 sec)\r\n",
      "I0412 21:07:00.745715 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.509604\r\n",
      "I0412 21:07:21.668971 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.23421922, Global Step = 40, Loss = 4.119335, Perplexity = 61.51833 (20.924 sec)\r\n",
      "I0412 21:07:21.670224 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.477909\r\n",
      "I0412 21:07:41.779335 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.2758997, Global Step = 50, Loss = 4.016456, Perplexity = 55.50406 (20.110 sec)\r\n",
      "I0412 21:07:41.780447 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.497259\r\n",
      "I0412 21:07:59.370769 127376437724992 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 60...\r\n",
      "I0412 21:07:59.371046 127376437724992 basic_session_run_hooks.py:633] Saving checkpoints for 60 into /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt.\r\n",
      "I0412 21:07:59.508038 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-60.data-00000-of-00001\r\n",
      "I0412 21:07:59.508262 127376437724992 saver.py:93] 19700\r\n",
      "I0412 21:07:59.508379 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-60.index\r\n",
      "I0412 21:07:59.508487 127376437724992 saver.py:93] 19700\r\n",
      "I0412 21:07:59.508595 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-60.meta\r\n",
      "I0412 21:07:59.508692 127376437724992 saver.py:93] 20100\r\n",
      "I0412 21:07:59.508881 127376437724992 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 60...\r\n",
      "I0412 21:08:01.627319 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.25992313, Global Step = 60, Loss = 4.0015845, Perplexity = 54.68473 (19.848 sec)\r\n",
      "I0412 21:08:01.628331 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.503832\r\n",
      "I0412 21:08:21.079260 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.28741974, Global Step = 70, Loss = 4.030915, Perplexity = 56.3124 (19.452 sec)\r\n",
      "I0412 21:08:21.080374 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.514085\r\n",
      "I0412 21:08:41.331829 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.27046797, Global Step = 80, Loss = 3.949395, Perplexity = 51.903954 (20.253 sec)\r\n",
      "I0412 21:08:41.332930 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.493765\r\n",
      "I0412 21:08:59.722311 127376437724992 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 90...\r\n",
      "I0412 21:08:59.722572 127376437724992 basic_session_run_hooks.py:633] Saving checkpoints for 90 into /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt.\r\n",
      "I0412 21:08:59.827370 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-90.meta\r\n",
      "I0412 21:08:59.827590 127376437724992 saver.py:93] 400\r\n",
      "I0412 21:08:59.827703 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-90.index\r\n",
      "I0412 21:08:59.827799 127376437724992 saver.py:93] 400\r\n",
      "I0412 21:08:59.827908 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-90.data-00000-of-00001\r\n",
      "I0412 21:08:59.828022 127376437724992 saver.py:93] 20100\r\n",
      "I0412 21:08:59.828222 127376437724992 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 90...\r\n",
      "I0412 21:09:01.850474 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.24987724, Global Step = 90, Loss = 4.0302896, Perplexity = 56.27721 (20.519 sec)\r\n",
      "I0412 21:09:01.851557 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.487362\r\n",
      "I0412 21:09:22.217122 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.499894\r\n",
      "I0412 21:09:22.218483 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.25521335, Global Step = 100, Loss = 3.9859016, Perplexity = 53.8338 (20.368 sec)\r\n",
      "I0412 21:09:22.219578 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.490966\r\n",
      "I0412 21:09:41.008880 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.25400743, Global Step = 110, Loss = 3.9125717, Perplexity = 50.02744 (18.790 sec)\r\n",
      "I0412 21:09:41.009944 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.532188\r\n",
      "I0412 21:10:00.587151 127376437724992 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 121...\r\n",
      "I0412 21:10:00.587413 127376437724992 basic_session_run_hooks.py:633] Saving checkpoints for 121 into /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt.\r\n",
      "I0412 21:10:00.719336 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-121.meta\r\n",
      "I0412 21:10:00.719556 127376437724992 saver.py:93] 400\r\n",
      "I0412 21:10:00.719684 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-121.data-00000-of-00001\r\n",
      "I0412 21:10:00.719785 127376437724992 saver.py:93] 20100\r\n",
      "I0412 21:10:00.719886 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-121.index\r\n",
      "I0412 21:10:00.719997 127376437724992 saver.py:93] 20100\r\n",
      "I0412 21:10:00.720215 127376437724992 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 121...\r\n",
      "I0412 21:10:00.720594 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.24933901, Global Step = 120, Loss = 3.9818301, Perplexity = 53.615067 (19.712 sec)\r\n",
      "I0412 21:10:00.721605 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.507314\r\n",
      "I0412 21:10:21.287739 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.26172203, Global Step = 130, Loss = 3.9715507, Perplexity = 53.066757 (20.567 sec)\r\n",
      "I0412 21:10:21.288808 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.486211\r\n",
      "I0412 21:10:41.723480 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.2553442, Global Step = 140, Loss = 3.9558864, Perplexity = 52.241978 (20.436 sec)\r\n",
      "I0412 21:10:41.724762 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.489334\r\n",
      "I0412 21:11:01.217998 127376437724992 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 151...\r\n",
      "I0412 21:11:01.218267 127376437724992 basic_session_run_hooks.py:633] Saving checkpoints for 151 into /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt.\r\n",
      "I0412 21:11:01.319199 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-151.data-00000-of-00001\r\n",
      "I0412 21:11:01.319422 127376437724992 saver.py:93] 19700\r\n",
      "I0412 21:11:01.319535 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-151.index\r\n",
      "I0412 21:11:01.319630 127376437724992 saver.py:93] 19700\r\n",
      "I0412 21:11:01.319735 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-151.meta\r\n",
      "I0412 21:11:01.319826 127376437724992 saver.py:93] 20100\r\n",
      "I0412 21:11:01.320029 127376437724992 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 151...\r\n",
      "I0412 21:11:01.320474 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.2510069, Global Step = 150, Loss = 3.9767687, Perplexity = 53.344387 (19.597 sec)\r\n",
      "I0412 21:11:01.321395 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.510291\r\n",
      "I0412 21:11:20.531852 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.24774916, Global Step = 160, Loss = 4.0296826, Perplexity = 56.24306 (19.211 sec)\r\n",
      "I0412 21:11:20.532947 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.520521\r\n",
      "I0412 21:11:40.098751 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.26018527, Global Step = 170, Loss = 3.8958457, Perplexity = 49.19764 (19.567 sec)\r\n",
      "I0412 21:11:40.099828 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.511068\r\n",
      "I0412 21:12:00.926354 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.2542962, Global Step = 180, Loss = 4.009361, Perplexity = 55.11163 (20.828 sec)\r\n",
      "I0412 21:12:00.927529 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.480129\r\n",
      "I0412 21:12:03.090008 127376437724992 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 182...\r\n",
      "I0412 21:12:03.090292 127376437724992 basic_session_run_hooks.py:633] Saving checkpoints for 182 into /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt.\r\n",
      "I0412 21:12:03.191618 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-182.meta\r\n",
      "I0412 21:12:03.191835 127376437724992 saver.py:93] 400\r\n",
      "I0412 21:12:03.191945 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-182.data-00000-of-00001\r\n",
      "I0412 21:12:03.192100 127376437724992 saver.py:93] 20100\r\n",
      "I0412 21:12:03.192179 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-182.index\r\n",
      "I0412 21:12:03.192287 127376437724992 saver.py:93] 20100\r\n",
      "I0412 21:12:03.192483 127376437724992 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 182...\r\n",
      "I0412 21:12:20.772759 127376437724992 basic_session_run_hooks.py:264] Accuracy = 0.25886416, Global Step = 190, Loss = 3.9530218, Perplexity = 52.09254 (19.846 sec)\r\n",
      "I0412 21:12:20.773868 127376437724992 basic_session_run_hooks.py:717] global_step/sec: 0.503871\r\n",
      "I0412 21:12:38.167330 127376437724992 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 200...\r\n",
      "I0412 21:12:38.167593 127376437724992 basic_session_run_hooks.py:633] Saving checkpoints for 200 into /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt.\r\n",
      "I0412 21:12:38.307575 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-200.data-00000-of-00001\r\n",
      "I0412 21:12:38.307798 127376437724992 saver.py:93] 19700\r\n",
      "I0412 21:12:38.307944 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-200.meta\r\n",
      "I0412 21:12:38.308104 127376437724992 saver.py:93] 20100\r\n",
      "I0412 21:12:38.308186 127376437724992 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-200.index\r\n",
      "I0412 21:12:38.308307 127376437724992 saver.py:93] 20100\r\n",
      "I0412 21:12:38.308491 127376437724992 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 200...\r\n",
      "I0412 21:12:38.353933 127376437724992 events_rnn_train.py:113] Training complete.\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Different Learning Rates: 0.001, 0.005, 0.01, 0.02, 0.05, 0.1\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-001 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.001\" \\\n",
    "--num_checkpoints=100 \n",
    "# --summary_frequency=10\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-001 \\\n",
    "--sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "--num_eval_examples=1000 \\\n",
    "--eval\n",
    "\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-005 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.005\" \\\n",
    "--num_checkpoints=100 \n",
    "# --summary_frequency=10\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-005 \\\n",
    "--sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "--num_eval_examples=1000 \\\n",
    "--eval\n",
    "\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-01 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.01\" \\\n",
    "--num_checkpoints=20\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-01 \\\n",
    "--sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "--num_eval_examples=1000 \\\n",
    "--eval\n",
    "\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-02 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.02\" \\\n",
    "--num_checkpoints=20\n",
    "\n",
    "# !polyphony_rnn_train \\\n",
    "# --run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-02 \\\n",
    "# --sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "# --num_eval_examples=1000 \\\n",
    "# --eval\n",
    "\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-05 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.05\" \\\n",
    "--num_checkpoints=20 \n",
    "\n",
    "# !polyphony_rnn_train \\\n",
    "# --run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-05 \\\n",
    "# --sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "# --num_eval_examples=1000 \\\n",
    "# --eval\n",
    "\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-1 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.1\" \\\n",
    "--num_checkpoints=20\n",
    "\n",
    "# !polyphony_rnn_train \\\n",
    "# --run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-1 \\\n",
    "# --sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "# --num_eval_examples = 1000 \\\n",
    "# --eval\n",
    "\n",
    "\n",
    "!cp -R /kaggle/tmp/polyphony_rnn/logdir /kaggle/working/logdir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3200.309364,
   "end_time": "2023-04-12T21:12:44.102118",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-12T20:19:23.792754",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
