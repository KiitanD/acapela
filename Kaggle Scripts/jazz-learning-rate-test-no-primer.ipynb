{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784085fe",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-12T14:43:02.347223Z",
     "iopub.status.busy": "2023-04-12T14:43:02.346147Z",
     "iopub.status.idle": "2023-04-12T14:45:15.208528Z",
     "shell.execute_reply": "2023-04-12T14:45:15.207295Z"
    },
    "papermill": {
     "duration": 132.869476,
     "end_time": "2023-04-12T14:45:15.211242",
     "exception": false,
     "start_time": "2023-04-12T14:43:02.341766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "build-essential is already the newest version (12.8ubuntu1.1).\r\n",
      "The following additional packages will be installed:\r\n",
      "  libjack0 libportaudio2 libportaudiocpp0 uuid-dev\r\n",
      "Suggested packages:\r\n",
      "  libasound2-doc jackd1 portaudio19-doc\r\n",
      "The following packages will be REMOVED:\r\n",
      "  libjack-jackd2-0\r\n",
      "The following NEW packages will be installed:\r\n",
      "  libasound2-dev libjack-dev libjack0 libportaudio2 libportaudiocpp0\r\n",
      "  portaudio19-dev uuid-dev\r\n",
      "0 upgraded, 7 newly installed, 1 to remove and 76 not upgraded.\r\n",
      "Need to get 625 kB of archives.\r\n",
      "After this operation, 3340 kB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 libjack0 amd64 1:0.125.0-3build2 [93.3 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libasound2-dev amd64 1.2.2-2.1ubuntu2.5 [104 kB]\r\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 uuid-dev amd64 2.34-0.1ubuntu9.3 [33.6 kB]\r\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 libjack-dev amd64 1:0.125.0-3build2 [206 kB]\r\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 libportaudio2 amd64 19.6.0-1build1 [65.4 kB]\r\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/universe amd64 libportaudiocpp0 amd64 19.6.0-1build1 [16.1 kB]\r\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/universe amd64 portaudio19-dev amd64 19.6.0-1build1 [106 kB]\r\n",
      "Fetched 625 kB in 1s (495 kB/s)\r\n",
      "dpkg: libjack-jackd2-0:amd64: dependency problems, but removing anyway as you requested:\r\n",
      " libavdevice58:amd64 depends on libjack-jackd2-0 (>= 1.9.10+20150825) | libjack-0.125; however:\r\n",
      "  Package libjack-jackd2-0:amd64 is to be removed.\r\n",
      "  Package libjack-0.125 is not installed.\r\n",
      "  Package libjack-jackd2-0:amd64 which provides libjack-0.125 is to be removed.\r\n",
      " libavdevice58:amd64 depends on libjack-jackd2-0 (>= 1.9.10+20150825) | libjack-0.125; however:\r\n",
      "  Package libjack-jackd2-0:amd64 is to be removed.\r\n",
      "  Package libjack-0.125 is not installed.\r\n",
      "  Package libjack-jackd2-0:amd64 which provides libjack-0.125 is to be removed.\r\n",
      "\r\n",
      "(Reading database ... 111522 files and directories currently installed.)\r\n",
      "Removing libjack-jackd2-0:amd64 (1.9.12~dfsg-2ubuntu2) ...\r\n",
      "Selecting previously unselected package libjack0:amd64.\r\n",
      "(Reading database ... 111510 files and directories currently installed.)\r\n",
      "Preparing to unpack .../0-libjack0_1%3a0.125.0-3build2_amd64.deb ...\r\n",
      "Unpacking libjack0:amd64 (1:0.125.0-3build2) ...\r\n",
      "Selecting previously unselected package libasound2-dev:amd64.\r\n",
      "Preparing to unpack .../1-libasound2-dev_1.2.2-2.1ubuntu2.5_amd64.deb ...\r\n",
      "Unpacking libasound2-dev:amd64 (1.2.2-2.1ubuntu2.5) ...\r\n",
      "Selecting previously unselected package uuid-dev:amd64.\r\n",
      "Preparing to unpack .../2-uuid-dev_2.34-0.1ubuntu9.3_amd64.deb ...\r\n",
      "Unpacking uuid-dev:amd64 (2.34-0.1ubuntu9.3) ...\r\n",
      "Selecting previously unselected package libjack-dev.\r\n",
      "Preparing to unpack .../3-libjack-dev_1%3a0.125.0-3build2_amd64.deb ...\r\n",
      "Unpacking libjack-dev (1:0.125.0-3build2) ...\r\n",
      "Selecting previously unselected package libportaudio2:amd64.\r\n",
      "Preparing to unpack .../4-libportaudio2_19.6.0-1build1_amd64.deb ...\r\n",
      "Unpacking libportaudio2:amd64 (19.6.0-1build1) ...\r\n",
      "Selecting previously unselected package libportaudiocpp0:amd64.\r\n",
      "Preparing to unpack .../5-libportaudiocpp0_19.6.0-1build1_amd64.deb ...\r\n",
      "Unpacking libportaudiocpp0:amd64 (19.6.0-1build1) ...\r\n",
      "Selecting previously unselected package portaudio19-dev:amd64.\r\n",
      "Preparing to unpack .../6-portaudio19-dev_19.6.0-1build1_amd64.deb ...\r\n",
      "Unpacking portaudio19-dev:amd64 (19.6.0-1build1) ...\r\n",
      "Setting up libjack0:amd64 (1:0.125.0-3build2) ...\r\n",
      "Setting up uuid-dev:amd64 (2.34-0.1ubuntu9.3) ...\r\n",
      "Setting up libjack-dev (1:0.125.0-3build2) ...\r\n",
      "Setting up libasound2-dev:amd64 (1.2.2-2.1ubuntu2.5) ...\r\n",
      "Setting up libportaudio2:amd64 (19.6.0-1build1) ...\r\n",
      "Setting up libportaudiocpp0:amd64 (19.6.0-1build1) ...\r\n",
      "Setting up portaudio19-dev:amd64 (19.6.0-1build1) ...\r\n",
      "Processing triggers for man-db (2.9.1-1) ...\r\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\r\n",
      "Collecting magenta\r\n",
      "  Downloading magenta-2.1.4-py3-none-any.whl (1.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting llvmlite\r\n",
      "  Downloading llvmlite-0.39.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pretty-midi==0.2.9\r\n",
      "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting numba==0.49.1\r\n",
      "  Downloading numba-0.49.1-cp37-cp37m-manylinux2014_x86_64.whl (3.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting numpy==1.21.6\r\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting matplotlib==3.5.2\r\n",
      "  Downloading matplotlib-3.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow==2.9.1\r\n",
      "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting scipy==1.7.3\r\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting librosa==0.7.2\r\n",
      "  Downloading librosa-0.7.2.tar.gz (1.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting mir-eval==0.7\r\n",
      "  Downloading mir_eval-0.7.tar.gz (90 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting six==1.16.0\r\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\r\n",
      "Collecting python-rtmidi==1.1.2\r\n",
      "  Downloading python-rtmidi-1.1.2.tar.gz (204 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.5/204.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hCollecting tensorflow-probability==0.17.0\r\n",
      "  Downloading tensorflow_probability-0.17.0-py2.py3-none-any.whl (6.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting wheel==0.37.1\r\n",
      "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\r\n",
      "Collecting Pillow==9.2.0\r\n",
      "  Downloading Pillow-9.2.0-cp37-cp37m-manylinux_2_28_x86_64.whl (3.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting scikit-image==0.19.3\r\n",
      "  Downloading scikit_image-0.19.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (13.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting absl-py==1.2.0\r\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tf-slim==1.1.0\r\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow-datasets==4.6.0\r\n",
      "  Downloading tensorflow_datasets-4.6.0-py3-none-any.whl (4.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pygtrie==2.5.0\r\n",
      "  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\r\n",
      "Collecting dm-sonnet==2.0.0\r\n",
      "  Downloading dm_sonnet-2.0.0-py3-none-any.whl (254 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.5/254.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting note-seq==0.0.3\r\n",
      "  Downloading note_seq-0.0.3-py3-none-any.whl (210 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.1/210.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting sox==1.4.1\r\n",
      "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\r\n",
      "Collecting sk-video==1.1.10\r\n",
      "  Downloading sk_video-1.1.10-py2.py3-none-any.whl (2.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting mido==1.2.6\r\n",
      "  Downloading mido-1.2.6-py2.py3-none-any.whl (69 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.8/69.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting imageio==2.20.0\r\n",
      "  Downloading imageio-2.20.0-py3-none-any.whl (3.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting wrapt>=1.11.1\r\n",
      "  Downloading wrapt-1.15.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tabulate>=0.7.5\r\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\r\n",
      "Collecting dm-tree>=0.1.1\r\n",
      "  Downloading dm_tree-0.1.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.8/153.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting audioread>=2.0.0\r\n",
      "  Downloading audioread-3.0.0.tar.gz (377 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting scikit-learn!=0.19.0,>=0.14.0\r\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting joblib>=0.12\r\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting decorator>=3.0.0\r\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\r\n",
      "Collecting resampy>=0.2.2\r\n",
      "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting soundfile>=0.9.0\r\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pyparsing>=2.2.1\r\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting python-dateutil>=2.7\r\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\r\n",
      "  Downloading kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting packaging>=20.0\r\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\r\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting cycler>=0.10\r\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\r\n",
      "Collecting future\r\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting intervaltree>=2.1.0\r\n",
      "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting IPython\r\n",
      "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m793.8/793.8 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting attrs\r\n",
      "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting bokeh>=0.12.0\r\n",
      "  Downloading bokeh-2.4.3-py3-none-any.whl (18.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pandas>=0.18.1\r\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting protobuf>=3.6.1\r\n",
      "  Downloading protobuf-4.22.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pydub\r\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\r\n",
      "Collecting setuptools\r\n",
      "  Downloading setuptools-67.6.1-py3-none-any.whl (1.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting llvmlite\r\n",
      "  Downloading llvmlite-0.32.1-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting networkx>=2.2\r\n",
      "  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting PyWavelets>=1.1.1\r\n",
      "  Downloading PyWavelets-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tifffile>=2019.7.26\r\n",
      "  Downloading tifffile-2021.11.2-py3-none-any.whl (178 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\r\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\r\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\n",
      "Collecting libclang>=13.0.0\r\n",
      "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorboard<2.10,>=2.9\r\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting h5py>=2.9.0\r\n",
      "  Downloading h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting protobuf>=3.6.1\r\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.1\r\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\r\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting flatbuffers<2,>=1.12\r\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\r\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\r\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\r\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\r\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\r\n",
      "Collecting typing-extensions>=3.6.6\r\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\r\n",
      "Collecting termcolor>=1.1.0\r\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\r\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\r\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\r\n",
      "  Downloading grpcio-1.53.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tqdm\r\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting importlib-resources\r\n",
      "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\r\n",
      "Collecting tensorflow-metadata\r\n",
      "  Downloading tensorflow_metadata-1.12.0-py3-none-any.whl (52 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting requests>=2.19.0\r\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting dill\r\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting toml\r\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\r\n",
      "Collecting promise\r\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting etils[epath]\r\n",
      "  Downloading etils-0.9.0-py3-none-any.whl (140 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting cloudpickle>=1.3\r\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\r\n",
      "Collecting PyYAML>=3.10\r\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tornado>=5.1\r\n",
      "  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.0/424.0 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting Jinja2>=2.9\r\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting sortedcontainers<3.0,>=2.0\r\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\r\n",
      "Collecting pytz>=2017.3\r\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\r\n",
      "  Downloading charset_normalizer-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (171 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.0/171.0 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting idna<4,>=2.5\r\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\r\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\r\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting resampy>=0.2.2\r\n",
      "  Downloading resampy-0.4.1-py3-none-any.whl (3.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading resampy-0.4.0-py3-none-any.whl (3.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading resampy-0.3.1-py3-none-any.whl (3.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\r\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\r\n",
      "Collecting cffi>=1.0\r\n",
      "  Downloading cffi-1.15.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.9/427.9 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\r\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\r\n",
      "Collecting google-auth<3,>=1.6.3\r\n",
      "  Downloading google_auth-2.17.2-py2.py3-none-any.whl (178 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.2/178.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\r\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\r\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting markdown>=2.6.8\r\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\r\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting zipp\r\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\r\n",
      "Collecting backcall\r\n",
      "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\r\n",
      "Collecting matplotlib-inline\r\n",
      "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\r\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\r\n",
      "  Downloading prompt_toolkit-3.0.38-py3-none-any.whl (385 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting jedi>=0.16\r\n",
      "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pickleshare\r\n",
      "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\r\n",
      "Collecting traitlets>=4.2\r\n",
      "  Downloading traitlets-5.9.0-py3-none-any.whl (117 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pygments\r\n",
      "  Downloading Pygments-2.15.0-py3-none-any.whl (1.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pexpect>4.3\r\n",
      "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting googleapis-common-protos<2,>=1.52.0\r\n",
      "  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pycparser\r\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\r\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\r\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\r\n",
      "Collecting cachetools<6.0,>=2.0.0\r\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\r\n",
      "Collecting requests-oauthlib>=0.7.0\r\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\r\n",
      "Collecting parso<0.9.0,>=0.8.0\r\n",
      "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\r\n",
      "  Downloading MarkupSafe-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\r\n",
      "Collecting importlib-metadata>=4.4\r\n",
      "  Downloading importlib_metadata-6.3.0-py3-none-any.whl (22 kB)\r\n",
      "Collecting ptyprocess>=0.5\r\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\r\n",
      "Collecting wcwidth\r\n",
      "  Downloading wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\r\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\r\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\r\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: librosa, mir-eval, pretty-midi, python-rtmidi, audioread, intervaltree, future, promise\r\n",
      "  Building wheel for librosa (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for librosa: filename=librosa-0.7.2-py3-none-any.whl size=1612903 sha256=7921888a1e824e2cb8d72296a9320b576a35acea67a6f2fd6c505648958c1819\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/b2/93/fb/ae0f9f40f5c4aaf1c8d73f2194581a8f249c7169670acfdf6d\r\n",
      "  Building wheel for mir-eval (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for mir-eval: filename=mir_eval-0.7-py3-none-any.whl size=100720 sha256=6d78df91bac2b3c4c5babcc8af95eac5ff7314b667dc58d20fe85c8aa22c2de0\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/a5/db/bc282c9ac9ec68db9b0a1c1bb3184d91f83f8897d51b6db511\r\n",
      "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591954 sha256=00379dfb79cd963ecfb615238f99d4772da656b5bfa7cbb347df8fa7ef77fa15\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/57/18/93/2909d84ca856ef11cfb220d650eecb99a4723ce139bb553464\r\n",
      "  Building wheel for python-rtmidi (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for python-rtmidi: filename=python_rtmidi-1.1.2-cp37-cp37m-linux_x86_64.whl size=592267 sha256=34f0cf6bee5f19e1c6c952a3eafaef9328dc74dd6848816de29e1381ba3ea68f\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/ac/7c/bf918313c78466861766f5dc370c1ae591fca433531f8c5e09\r\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23706 sha256=8d5509ffb2a5cb422fd719e66094733807a4b4da26a65cd97dc55fb662cd105a\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/dc/38/51/1be02cf6dbd3ef3e2e50a562071c9d574170c4f5096c09d8e1\r\n",
      "  Building wheel for intervaltree (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26118 sha256=b938349f5b7615e1c115b3beed2137eeb7e17e918f9330ed265eabede09e25ba\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/94/38/6fe5b0e582dca953d1ef8f2e8714fa1def8e5bf514c25c11fb\r\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492036 sha256=09d8fe202122aa7e6fab6fc904f0e644e275866bf3007861dbaacdcd22fc9e2c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/52/2a/fc/520209cfa6448febd490720a0b09036cb367628f7c4e9cc172\r\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=6bc509abb5a2febfb0f41443d17f167951adfca6f36ffe2609fd84cf725406c0\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/9d/ad/15/e6d5c43a0f01b88ee5883bd249a18e09d72821e43b1c3e8187\r\n",
      "Successfully built librosa mir-eval pretty-midi python-rtmidi audioread intervaltree future promise\r\n",
      "Installing collected packages: wcwidth, tensorboard-plugin-wit, sortedcontainers, pytz, python-rtmidi, pygtrie, pydub, pyasn1, ptyprocess, pickleshare, mido, llvmlite, libclang, keras, flatbuffers, dm-tree, backcall, zipp, wrapt, wheel, urllib3, typing-extensions, traitlets, tqdm, tornado, toml, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, tabulate, six, setuptools, rsa, PyYAML, pyparsing, pygments, pycparser, pyasn1-modules, protobuf, prompt-toolkit, Pillow, pexpect, parso, packaging, oauthlib, numpy, networkx, MarkupSafe, joblib, intervaltree, idna, grpcio, gast, future, fonttools, etils, dill, decorator, cycler, cloudpickle, charset-normalizer, certifi, cachetools, audioread, attrs, absl-py, werkzeug, tifffile, tf-slim, tensorflow-probability, sox, scipy, requests, PyWavelets, python-dateutil, promise, pretty-midi, opt-einsum, numba, matplotlib-inline, kiwisolver, keras-preprocessing, Jinja2, jedi, importlib-resources, importlib-metadata, imageio, h5py, googleapis-common-protos, google-pasta, google-auth, dm-sonnet, cffi, astunparse, tensorflow-metadata, soundfile, sk-video, scikit-learn, scikit-image, resampy, requests-oauthlib, pandas, mir-eval, matplotlib, markdown, IPython, bokeh, tensorflow-datasets, librosa, google-auth-oauthlib, tensorboard, note-seq, tensorflow, magenta\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "dask-cudf 21.12.2 requires cupy-cuda115, which is not installed.\r\n",
      "cudf 21.12.2 requires cupy-cuda115, which is not installed.\r\n",
      "wfdb 4.1.0 requires SoundFile<0.12.0,>=0.10.0, but you have soundfile 0.12.1 which is incompatible.\r\n",
      "wasabi 1.1.1 requires typing-extensions<4.5.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\r\n",
      "thinc 8.1.9 requires typing-extensions<4.5.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\r\n",
      "tfx-bsl 1.12.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.83.0 which is incompatible.\r\n",
      "tfx-bsl 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\r\n",
      "tfx-bsl 1.12.0 requires tensorflow<3,>=2.11, but you have tensorflow 2.9.1 which is incompatible.\r\n",
      "tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\r\n",
      "tensorflow-transform 1.12.0 requires tensorflow<2.12,>=2.11.0, but you have tensorflow 2.9.1 which is incompatible.\r\n",
      "tensorflow-text 2.11.0 requires tensorflow<2.12,>=2.11.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.9.1 which is incompatible.\r\n",
      "tensorflow-serving-api 2.11.0 requires tensorflow<3,>=2.11.0, but you have tensorflow 2.9.1 which is incompatible.\r\n",
      "tensorflow-io 0.29.0 requires tensorflow-io-gcs-filesystem==0.29.0, but you have tensorflow-io-gcs-filesystem 0.32.0 which is incompatible.\r\n",
      "tensorflow-decision-forests 1.2.0 requires tensorflow~=2.11.0, but you have tensorflow 2.9.1 which is incompatible.\r\n",
      "stumpy 1.11.1 requires numba>=0.54, but you have numba 0.49.1 which is incompatible.\r\n",
      "spacy 3.5.1 requires typing-extensions<4.5.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\r\n",
      "pynndescent 0.5.8 requires numba>=0.51.2, but you have numba 0.49.1 which is incompatible.\r\n",
      "pydocstyle 6.3.0 requires importlib-metadata<5.0.0,>=2.0.0; python_version < \"3.8\", but you have importlib-metadata 6.3.0 which is incompatible.\r\n",
      "pandas-profiling 3.6.2 requires tqdm<4.65,>=4.48.2, but you have tqdm 4.65.0 which is incompatible.\r\n",
      "onnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "ibis-framework 2.1.1 requires importlib-metadata<5,>=4; python_version < \"3.8\", but you have importlib-metadata 6.3.0 which is incompatible.\r\n",
      "google-cloud-core 1.7.3 requires google-auth<2.0dev,>=1.24.0, but you have google-auth 2.17.2 which is incompatible.\r\n",
      "flake8 5.0.4 requires importlib-metadata<4.3,>=1.1.0; python_version < \"3.8\", but you have importlib-metadata 6.3.0 which is incompatible.\r\n",
      "distributed 2021.11.2 requires dask==2021.11.2, but you have dask 2022.2.0 which is incompatible.\r\n",
      "datashader 0.14.4 requires numba>=0.51, but you have numba 0.49.1 which is incompatible.\r\n",
      "dask-cudf 21.12.2 requires dask<=2021.11.2,>=2021.11.1, but you have dask 2022.2.0 which is incompatible.\r\n",
      "cudf 21.12.2 requires numba>=0.53.1, but you have numba 0.49.1 which is incompatible.\r\n",
      "confection 0.0.4 requires typing-extensions<4.5.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\r\n",
      "cmudict 1.0.13 requires importlib-metadata<6.0.0,>=5.1.0, but you have importlib-metadata 6.3.0 which is incompatible.\r\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 2.83.0 which is incompatible.\r\n",
      "boto3 1.26.100 requires botocore<1.30.0,>=1.29.100, but you have botocore 1.27.59 which is incompatible.\r\n",
      "apache-beam 2.44.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\r\n",
      "aiohttp 3.8.3 requires charset-normalizer<3.0,>=2.0, but you have charset-normalizer 3.1.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed IPython-7.34.0 Jinja2-3.1.2 MarkupSafe-2.1.2 Pillow-9.4.0 PyWavelets-1.3.0 PyYAML-6.0 absl-py-1.4.0 astunparse-1.6.3 attrs-22.2.0 audioread-3.0.0 backcall-0.2.0 bokeh-2.4.3 cachetools-5.3.0 certifi-2022.12.7 cffi-1.15.1 charset-normalizer-3.1.0 cloudpickle-2.2.1 cycler-0.11.0 decorator-5.1.1 dill-0.3.6 dm-sonnet-2.0.0 dm-tree-0.1.8 etils-0.9.0 flatbuffers-23.1.21 fonttools-4.38.0 future-0.18.3 gast-0.4.0 google-auth-2.17.2 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 googleapis-common-protos-1.59.0 grpcio-1.53.0 h5py-3.8.0 idna-3.4 imageio-2.25.0 importlib-metadata-6.3.0 importlib-resources-5.12.0 intervaltree-3.1.0 jedi-0.18.2 joblib-1.2.0 keras-2.11.0 keras-preprocessing-1.1.2 kiwisolver-1.4.4 libclang-16.0.0 librosa-0.10.0.post2 llvmlite-0.39.1 magenta-2.1.4 markdown-3.4.3 matplotlib-3.5.3 matplotlib-inline-0.1.6 mido-1.2.6 mir-eval-0.7 networkx-2.6.3 note-seq-0.0.3 numba-0.56.4 numpy-1.21.6 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-23.0 pandas-1.3.5 parso-0.8.3 pexpect-4.8.0 pickleshare-0.7.5 pretty-midi-0.2.9 promise-2.3 prompt-toolkit-3.0.38 protobuf-3.20.3 ptyprocess-0.7.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.21 pydub-0.25.1 pygments-2.15.0 pygtrie-2.5.0 pyparsing-3.0.9 python-dateutil-2.8.2 python-rtmidi-1.1.2 pytz-2023.3 requests-2.28.2 requests-oauthlib-1.3.1 resampy-0.3.1 rsa-4.9 scikit-image-0.19.3 scikit-learn-1.0.2 scipy-1.7.3 setuptools-67.6.1 six-1.16.0 sk-video-1.1.10 sortedcontainers-2.4.0 soundfile-0.12.1 sox-1.4.1 tabulate-0.9.0 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-datasets-4.8.2 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.32.0 tensorflow-metadata-1.12.0 tensorflow-probability-0.19.0 termcolor-2.2.0 tf-slim-1.1.0 threadpoolctl-3.1.0 tifffile-2021.11.2 toml-0.10.2 tornado-6.2 tqdm-4.65.0 traitlets-5.9.0 typing-extensions-4.5.0 urllib3-1.26.15 wcwidth-0.2.6 werkzeug-2.2.3 wheel-0.38.4 wrapt-1.15.0 zipp-3.15.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install -y build-essential libasound2-dev libjack-dev portaudio19-dev\n",
    "!pip install magenta --ignore-installed llvmlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2630180c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:45:15.268682Z",
     "iopub.status.busy": "2023-04-12T14:45:15.267653Z",
     "iopub.status.idle": "2023-04-12T14:47:02.121945Z",
     "shell.execute_reply": "2023-04-12T14:47:02.120598Z"
    },
    "papermill": {
     "duration": 106.884911,
     "end_time": "2023-04-12T14:47:02.124582",
     "exception": false,
     "start_time": "2023-04-12T14:45:15.239671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "8127\n"
     ]
    }
   ],
   "source": [
    "import magenta\n",
    "from magenta.scripts.convert_dir_to_note_sequences import convert_midi\n",
    "from magenta.music.sequences_lib import split_note_sequence\n",
    "import tensorflow.compat.v1 as tf\n",
    "import os\n",
    "\n",
    "prepath = \"/kaggle/input/jazzmidi0-1499/midi\"\n",
    "all_ns = []\n",
    "count = 0\n",
    "for file in os.listdir(prepath):\n",
    "    try:\n",
    "        ns = convert_midi(prepath, \"midi\", os.path.join(prepath, file))\n",
    "\n",
    "    except: \n",
    "        print(\"Error on:\", file)\n",
    "        count += 1\n",
    "        continue\n",
    "    \n",
    "    ns_split = split_note_sequence(ns,\n",
    "                            20,\n",
    "                            False)\n",
    "#     print(len(ns_split))\n",
    "    count += 1\n",
    "    all_ns += ns_split\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "\n",
    "print(len(all_ns))\n",
    "!mkdir /kaggle/tmp/\n",
    "if (os.path.exists(\"/kaggle/tmp/jazz.tfrecord\") == False):\n",
    "    f = open(\"/kaggle/tmp/jazz.tfrecord\", \"w\")\n",
    "with tf.io.TFRecordWriter(\"/kaggle/tmp/jazz.tfrecord\") as writer:\n",
    "    for sequence in all_ns:\n",
    "        writer.write(sequence.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104fd489",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:47:02.182186Z",
     "iopub.status.busy": "2023-04-12T14:47:02.180617Z",
     "iopub.status.idle": "2023-04-12T14:47:02.234350Z",
     "shell.execute_reply": "2023-04-12T14:47:02.233429Z"
    },
    "papermill": {
     "duration": 0.084306,
     "end_time": "2023-04-12T14:47:02.236499",
     "exception": false,
     "start_time": "2023-04-12T14:47:02.152193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from note_seq.protobuf import music_pb2\n",
    "from magenta.pipelines import pipeline, note_sequence_pipelines, statistics\n",
    "from magenta.models import polyphony_rnn\n",
    "from magenta.models.polyphony_rnn import polyphony_lib, polyphony_model, polyphony_rnn_pipeline\n",
    "from magenta.pipelines import dag_pipeline\n",
    "from magenta.pipelines import event_sequence_pipeline\n",
    "from magenta.pipelines import pipelines_common\n",
    "\n",
    "\n",
    "class PolyphonicSequenceExtractor(pipeline.Pipeline):\n",
    "  \"\"\"Extracts polyphonic tracks from a quantized NoteSequence.\"\"\"\n",
    "\n",
    "  def __init__(self, min_steps, max_steps, name=None):\n",
    "    super(PolyphonicSequenceExtractor, self).__init__(\n",
    "        input_type=music_pb2.NoteSequence,\n",
    "        output_type=polyphony_lib.PolyphonicSequence,\n",
    "        name=name)\n",
    "    self._min_steps = min_steps\n",
    "    self._max_steps = max_steps\n",
    "\n",
    "  def transform(self, input_object):\n",
    "    quantized_sequence = input_object\n",
    "    poly_seqs, stats = polyphony_lib.extract_polyphonic_sequences(\n",
    "        quantized_sequence,\n",
    "        min_steps_discard=self._min_steps,\n",
    "        max_steps_discard=self._max_steps)\n",
    "    self._set_stats(stats)\n",
    "    return poly_seqs\n",
    "\n",
    "# changed from the library version: remove transposition from the pipeline\n",
    "def get_pipeline_mine(config, min_steps, max_steps, eval_ratio):\n",
    "  \"\"\"Returns the Pipeline instance which creates the RNN dataset.\n",
    "  Args:\n",
    "    config: An EventSequenceRnnConfig.\n",
    "    min_steps: Minimum number of steps for an extracted sequence.\n",
    "    max_steps: Maximum number of steps for an extracted sequence.\n",
    "    eval_ratio: Fraction of input to set aside for evaluation set.\n",
    "  Returns:\n",
    "    A pipeline.Pipeline instance.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  partitioner = pipelines_common.RandomPartition(\n",
    "      music_pb2.NoteSequence,\n",
    "      ['eval_poly_tracks', 'training_poly_tracks'],\n",
    "      [eval_ratio])\n",
    "  dag = {partitioner: dag_pipeline.DagInput(music_pb2.NoteSequence)}\n",
    "\n",
    "  for mode in ['eval', 'training']:\n",
    "    time_change_splitter = note_sequence_pipelines.TimeChangeSplitter(\n",
    "        name='TimeChangeSplitter_' + mode)\n",
    "    quantizer = note_sequence_pipelines.Quantizer(\n",
    "        steps_per_quarter=config.steps_per_quarter, name='Quantizer_' + mode)\n",
    "    poly_extractor = PolyphonicSequenceExtractor(\n",
    "        min_steps=min_steps, max_steps=max_steps, name='PolyExtractor_' + mode)\n",
    "    encoder_pipeline = event_sequence_pipeline.EncoderPipeline(\n",
    "        polyphony_lib.PolyphonicSequence, config.encoder_decoder,\n",
    "        name='EncoderPipeline_' + mode)\n",
    "\n",
    "    dag[time_change_splitter] = partitioner[mode + '_poly_tracks']\n",
    "    dag[quantizer] = time_change_splitter\n",
    "    dag[poly_extractor] = quantizer\n",
    "    dag[encoder_pipeline] = poly_extractor\n",
    "    dag[dag_pipeline.DagOutput(mode + '_poly_tracks')] = encoder_pipeline\n",
    "\n",
    "  return dag_pipeline.DAGPipeline(dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e933d723",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:47:02.292468Z",
     "iopub.status.busy": "2023-04-12T14:47:02.291644Z",
     "iopub.status.idle": "2023-04-12T14:53:16.045652Z",
     "shell.execute_reply": "2023-04-12T14:53:16.044336Z"
    },
    "papermill": {
     "duration": 373.785209,
     "end_time": "2023-04-12T14:53:16.048797",
     "exception": false,
     "start_time": "2023-04-12T14:47:02.263588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_instance = get_pipeline_mine(\n",
    "      min_steps=80,  # 5 measures\n",
    "      max_steps=None,\n",
    "      eval_ratio=0.2,\n",
    "      config=polyphony_model.default_configs['polyphony'])\n",
    "\n",
    "input_dir = '/kaggle/tmp/jazz.tfrecord'\n",
    "output_dir = '/kaggle/tmp/split'\n",
    "pipeline.run_pipeline_serial(\n",
    "  pipeline_instance,\n",
    "  pipeline.tf_record_iterator(input_dir, pipeline_instance.input_type),\n",
    "  output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8769d0bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:53:16.114451Z",
     "iopub.status.busy": "2023-04-12T14:53:16.114103Z",
     "iopub.status.idle": "2023-04-12T14:53:18.606809Z",
     "shell.execute_reply": "2023-04-12T14:53:18.604821Z"
    },
    "papermill": {
     "duration": 2.52688,
     "end_time": "2023-04-12T14:53:18.608968",
     "exception": false,
     "start_time": "2023-04-12T14:53:16.082088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1571 6156\n"
     ]
    }
   ],
   "source": [
    "print(sum(1 for _ in tf.python_io.tf_record_iterator('/kaggle/tmp/split/eval_poly_tracks.tfrecord')),\n",
    "sum(1 for _ in tf.python_io.tf_record_iterator('/kaggle/tmp/split/training_poly_tracks.tfrecord')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1e5a5c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T14:53:18.666612Z",
     "iopub.status.busy": "2023-04-12T14:53:18.665808Z",
     "iopub.status.idle": "2023-04-12T15:27:28.065340Z",
     "shell.execute_reply": "2023-04-12T15:27:28.064000Z"
    },
    "papermill": {
     "duration": 2049.431315,
     "end_time": "2023-04-12T15:27:28.068168",
     "exception": false,
     "start_time": "2023-04-12T14:53:18.636853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 14:53:38.919970 130318506518336 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 14:53:38.920830 130318506518336 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-001/train\r\n",
      "W0412 14:53:38.951617 130318506518336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 14:53:38.980407 130318506518336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 14:53:38.984692 130318506518336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0412 14:53:38.986288 130318506518336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 14:53:38.987260 130318506518336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 14:53:38.991029 130318506518336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0412 14:53:39.011463 130318506518336 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0412 14:53:39.011632 130318506518336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0412 14:53:39.056329 130318506518336 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0412 14:53:39.059576 130318506518336 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0412 14:53:39.059864 130318506518336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0412 14:53:39.085338 130318506518336 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0412 14:53:39.157979 130318506518336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0412 14:53:39.230361 130318506518336 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0412 14:53:39.381919 130318506518336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0412 14:53:39.385081 130318506518336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0412 14:53:40.214797 130318506518336 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0412 14:53:40.215092 130318506518336 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0412 14:53:40.325609 130318506518336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0412 14:53:40.414463 130318506518336 monitored_session.py:243] Graph was finalized.\r\n",
      "I0412 14:53:43.633378 130318506518336 session_manager.py:527] Running local_init_op.\r\n",
      "I0412 14:53:43.641925 130318506518336 session_manager.py:530] Done running local_init_op.\r\n",
      "W0412 14:53:43.668433 130318506518336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0412 14:53:44.366671 130318506518336 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0412 14:53:44.367181 130318506518336 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 14:53:44.601080 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-0.meta\r\n",
      "I0412 14:53:44.601501 130318506518336 saver.py:93] 400\r\n",
      "I0412 14:53:44.601626 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-0.index\r\n",
      "I0412 14:53:44.601764 130318506518336 saver.py:93] 400\r\n",
      "I0412 14:53:44.601866 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0412 14:53:44.601987 130318506518336 saver.py:93] 20100\r\n",
      "I0412 14:53:44.602233 130318506518336 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0412 14:53:50.894716 130318506518336 basic_session_run_hooks.py:266] Accuracy = 0.0024634965, Global Step = 0, Loss = 5.5584264, Perplexity = 259.41428\r\n",
      "I0412 14:54:15.264199 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.27442607, Global Step = 10, Loss = 4.247778, Perplexity = 69.94981 (24.369 sec)\r\n",
      "I0412 14:54:15.265311 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.410349\r\n",
      "I0412 14:54:37.520655 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.27592945, Global Step = 20, Loss = 4.162826, Perplexity = 64.252846 (22.256 sec)\r\n",
      "I0412 14:54:37.521744 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.449309\r\n",
      "I0412 14:54:45.899948 130318506518336 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 25...\r\n",
      "I0412 14:54:45.900224 130318506518336 basic_session_run_hooks.py:633] Saving checkpoints for 25 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 14:54:46.012157 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-25.data-00000-of-00001\r\n",
      "I0412 14:54:46.012393 130318506518336 saver.py:93] 19700\r\n",
      "I0412 14:54:46.012504 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-25.index\r\n",
      "I0412 14:54:46.012592 130318506518336 saver.py:93] 19700\r\n",
      "I0412 14:54:46.012692 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-25.meta\r\n",
      "I0412 14:54:46.012775 130318506518336 saver.py:93] 20100\r\n",
      "I0412 14:54:46.012984 130318506518336 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 25...\r\n",
      "I0412 14:54:59.511447 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.2576187, Global Step = 30, Loss = 4.1780944, Perplexity = 65.24141 (21.991 sec)\r\n",
      "I0412 14:54:59.512522 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.454736\r\n",
      "I0412 14:55:21.876419 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.26213792, Global Step = 40, Loss = 4.1267586, Perplexity = 61.976704 (22.365 sec)\r\n",
      "I0412 14:55:21.877584 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.447126\r\n",
      "I0412 14:55:43.344868 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.2666508, Global Step = 50, Loss = 4.0355034, Perplexity = 56.57139 (21.468 sec)\r\n",
      "I0412 14:55:43.345976 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.465801\r\n",
      "I0412 14:55:47.281866 130318506518336 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 53...\r\n",
      "I0412 14:55:47.282157 130318506518336 basic_session_run_hooks.py:633] Saving checkpoints for 53 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 14:55:47.382979 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-53.data-00000-of-00001\r\n",
      "I0412 14:55:47.383216 130318506518336 saver.py:93] 19700\r\n",
      "I0412 14:55:47.383327 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-53.index\r\n",
      "I0412 14:55:47.383415 130318506518336 saver.py:93] 19700\r\n",
      "I0412 14:55:47.383511 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-53.meta\r\n",
      "I0412 14:55:47.383594 130318506518336 saver.py:93] 20100\r\n",
      "I0412 14:55:47.383802 130318506518336 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 53...\r\n",
      "I0412 14:56:04.488672 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.2803984, Global Step = 60, Loss = 3.976165, Perplexity = 53.31219 (21.144 sec)\r\n",
      "I0412 14:56:04.489756 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.472952\r\n",
      "I0412 14:56:28.059949 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.26083243, Global Step = 70, Loss = 3.987655, Perplexity = 53.928276 (23.571 sec)\r\n",
      "I0412 14:56:28.061073 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.424244\r\n",
      "I0412 14:56:48.271622 130318506518336 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 80...\r\n",
      "I0412 14:56:48.271948 130318506518336 basic_session_run_hooks.py:633] Saving checkpoints for 80 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 14:56:48.392591 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-80.index\r\n",
      "I0412 14:56:48.392819 130318506518336 saver.py:93] 0\r\n",
      "I0412 14:56:48.392973 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-80.data-00000-of-00001\r\n",
      "I0412 14:56:48.393100 130318506518336 saver.py:93] 19700\r\n",
      "I0412 14:56:48.393200 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-80.meta\r\n",
      "I0412 14:56:48.393299 130318506518336 saver.py:93] 20100\r\n",
      "I0412 14:56:48.393497 130318506518336 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 80...\r\n",
      "I0412 14:56:50.864223 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.22823119, Global Step = 80, Loss = 4.195433, Perplexity = 66.38248 (22.804 sec)\r\n",
      "I0412 14:56:50.865238 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.438517\r\n",
      "I0412 14:57:14.716102 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.25757077, Global Step = 90, Loss = 4.036565, Perplexity = 56.631466 (23.852 sec)\r\n",
      "I0412 14:57:14.717205 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.419253\r\n",
      "I0412 14:57:38.339846 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.439665\r\n",
      "I0412 14:57:38.345257 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.25872403, Global Step = 100, Loss = 4.0578585, Perplexity = 57.85029 (23.629 sec)\r\n",
      "I0412 14:57:38.351250 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.423118\r\n",
      "I0412 14:57:48.532167 130318506518336 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 105...\r\n",
      "I0412 14:57:48.532429 130318506518336 basic_session_run_hooks.py:633] Saving checkpoints for 105 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 14:57:48.638757 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-105.index\r\n",
      "I0412 14:57:48.639048 130318506518336 saver.py:93] 0\r\n",
      "I0412 14:57:48.639177 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-105.meta\r\n",
      "I0412 14:57:48.639294 130318506518336 saver.py:93] 400\r\n",
      "I0412 14:57:48.639404 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-105.data-00000-of-00001\r\n",
      "I0412 14:57:48.639496 130318506518336 saver.py:93] 20100\r\n",
      "I0412 14:57:48.639704 130318506518336 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 105...\r\n",
      "I0412 14:58:01.348731 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.26796353, Global Step = 110, Loss = 4.0307503, Perplexity = 56.303135 (23.003 sec)\r\n",
      "I0412 14:58:01.349821 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.434809\r\n",
      "I0412 14:58:23.730967 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.25045362, Global Step = 120, Loss = 4.0639977, Perplexity = 58.206543 (22.382 sec)\r\n",
      "I0412 14:58:23.732092 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.446782\r\n",
      "I0412 14:58:46.623123 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.2610711, Global Step = 130, Loss = 4.089324, Perplexity = 59.699516 (22.892 sec)\r\n",
      "I0412 14:58:46.624182 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.436832\r\n",
      "I0412 14:58:49.214681 130318506518336 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 132...\r\n",
      "I0412 14:58:49.214952 130318506518336 basic_session_run_hooks.py:633] Saving checkpoints for 132 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 14:58:49.317374 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-132.data-00000-of-00001\r\n",
      "I0412 14:58:49.317604 130318506518336 saver.py:93] 19700\r\n",
      "I0412 14:58:49.317728 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-132.meta\r\n",
      "I0412 14:58:49.317830 130318506518336 saver.py:93] 20100\r\n",
      "I0412 14:58:49.317956 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-132.index\r\n",
      "I0412 14:58:49.318088 130318506518336 saver.py:93] 20100\r\n",
      "I0412 14:58:49.318289 130318506518336 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 132...\r\n",
      "I0412 14:59:07.199448 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.26599047, Global Step = 140, Loss = 3.9817557, Perplexity = 53.611076 (20.576 sec)\r\n",
      "I0412 14:59:07.200492 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.485996\r\n",
      "I0412 14:59:29.041878 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.25552145, Global Step = 150, Loss = 3.9747589, Perplexity = 53.237278 (21.842 sec)\r\n",
      "I0412 14:59:29.042935 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.457825\r\n",
      "I0412 14:59:51.277844 130318506518336 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 161...\r\n",
      "I0412 14:59:51.278115 130318506518336 basic_session_run_hooks.py:633] Saving checkpoints for 161 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 14:59:51.403527 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-161.meta\r\n",
      "I0412 14:59:51.403766 130318506518336 saver.py:93] 400\r\n",
      "I0412 14:59:51.403878 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-161.index\r\n",
      "I0412 14:59:51.403999 130318506518336 saver.py:93] 400\r\n",
      "I0412 14:59:51.404095 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-161.data-00000-of-00001\r\n",
      "I0412 14:59:51.404178 130318506518336 saver.py:93] 20100\r\n",
      "I0412 14:59:51.404367 130318506518336 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 161...\r\n",
      "I0412 14:59:51.404625 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.2704332, Global Step = 160, Loss = 3.9517207, Perplexity = 52.02481 (22.363 sec)\r\n",
      "I0412 14:59:51.405828 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.447168\r\n",
      "I0412 15:00:13.332038 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.25071156, Global Step = 170, Loss = 4.000221, Perplexity = 54.610203 (21.927 sec)\r\n",
      "I0412 15:00:13.333142 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.456052\r\n",
      "I0412 15:00:37.179135 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.2742164, Global Step = 180, Loss = 3.9787204, Perplexity = 53.4486 (23.847 sec)\r\n",
      "I0412 15:00:37.180170 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.41934\r\n",
      "I0412 15:00:54.152217 130318506518336 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 188...\r\n",
      "I0412 15:00:54.152472 130318506518336 basic_session_run_hooks.py:633] Saving checkpoints for 188 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 15:00:54.255854 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-188.data-00000-of-00001\r\n",
      "I0412 15:00:54.256122 130318506518336 saver.py:93] 19700\r\n",
      "I0412 15:00:54.256240 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-188.meta\r\n",
      "I0412 15:00:54.256330 130318506518336 saver.py:93] 20100\r\n",
      "I0412 15:00:54.256423 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-188.index\r\n",
      "I0412 15:00:54.256508 130318506518336 saver.py:93] 20100\r\n",
      "I0412 15:00:54.256705 130318506518336 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 188...\r\n",
      "I0412 15:01:00.912885 130318506518336 basic_session_run_hooks.py:264] Accuracy = 0.253615, Global Step = 190, Loss = 4.08681, Perplexity = 59.549633 (23.734 sec)\r\n",
      "I0412 15:01:00.913919 130318506518336 basic_session_run_hooks.py:717] global_step/sec: 0.421341\r\n",
      "I0412 15:01:20.480615 130318506518336 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 200...\r\n",
      "I0412 15:01:20.480898 130318506518336 basic_session_run_hooks.py:633] Saving checkpoints for 200 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 15:01:20.605140 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-200.index\r\n",
      "I0412 15:01:20.605361 130318506518336 saver.py:93] 0\r\n",
      "I0412 15:01:20.605467 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-200.meta\r\n",
      "I0412 15:01:20.605554 130318506518336 saver.py:93] 400\r\n",
      "I0412 15:01:20.605667 130318506518336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-200.data-00000-of-00001\r\n",
      "I0412 15:01:20.605752 130318506518336 saver.py:93] 20100\r\n",
      "I0412 15:01:20.605968 130318506518336 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 200...\r\n",
      "I0412 15:01:20.656526 130318506518336 events_rnn_train.py:113] Training complete.\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 15:01:39.835248 137174155597632 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 15:01:39.835932 137174155597632 polyphony_rnn_train.py:94] Train dir: /kaggle/tmp/polyphony_rnn/logdir/run-001/train\r\n",
      "I0412 15:01:39.836443 137174155597632 polyphony_rnn_train.py:99] Eval dir: /kaggle/tmp/polyphony_rnn/logdir/run-001/eval\r\n",
      "W0412 15:01:39.837150 137174155597632 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/polyphony_rnn_train\", line 8, in <module>\r\n",
      "    sys.exit(console_entry_point())\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 114, in console_entry_point\r\n",
      "    tf.app.run(main)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 36, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 308, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 254, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 104, in main\r\n",
      "    events_rnn_train.run_eval(build_graph_fn, train_dir, eval_dir, num_batches)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_train.py\", line 138, in run_eval\r\n",
      "    build_graph_fn()\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py\", line 113, in build\r\n",
      "    label_shape=label_shape, shuffle=mode == 'train')\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py\", line 66, in get_padded_batch\r\n",
      "    file_queue = tf.train.string_input_producer(file_list)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 357, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py\", line 253, in string_input_producer\r\n",
      "    raise ValueError(not_null_err)\r\n",
      "ValueError: string_input_producer requires a non-null input tensor\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 15:01:58.059937 131715127822144 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.005, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 15:01:58.060462 131715127822144 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-005/train\r\n",
      "W0412 15:01:58.061526 131715127822144 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 15:01:58.069618 131715127822144 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 15:01:58.071772 131715127822144 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0412 15:01:58.073312 131715127822144 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 15:01:58.074261 131715127822144 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 15:01:58.077792 131715127822144 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0412 15:01:58.084919 131715127822144 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0412 15:01:58.085057 131715127822144 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0412 15:01:58.125017 131715127822144 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0412 15:01:58.129209 131715127822144 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0412 15:01:58.129593 131715127822144 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0412 15:01:58.167065 131715127822144 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0412 15:01:58.188971 131715127822144 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0412 15:01:58.253132 131715127822144 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0412 15:01:58.399867 131715127822144 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0412 15:01:58.402921 131715127822144 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0412 15:01:59.199625 131715127822144 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0412 15:01:59.199913 131715127822144 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0412 15:01:59.311436 131715127822144 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0412 15:01:59.382620 131715127822144 monitored_session.py:243] Graph was finalized.\r\n",
      "I0412 15:02:00.569810 131715127822144 session_manager.py:527] Running local_init_op.\r\n",
      "I0412 15:02:00.576858 131715127822144 session_manager.py:530] Done running local_init_op.\r\n",
      "W0412 15:02:00.602516 131715127822144 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0412 15:02:01.300970 131715127822144 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0412 15:02:01.302171 131715127822144 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 15:02:01.487240 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-0.meta\r\n",
      "I0412 15:02:01.487578 131715127822144 saver.py:93] 400\r\n",
      "I0412 15:02:01.487730 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-0.index\r\n",
      "I0412 15:02:01.487858 131715127822144 saver.py:93] 400\r\n",
      "I0412 15:02:01.487974 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0412 15:02:01.488092 131715127822144 saver.py:93] 20100\r\n",
      "I0412 15:02:01.488291 131715127822144 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0412 15:02:05.609940 131715127822144 basic_session_run_hooks.py:266] Accuracy = 0.0029584116, Global Step = 0, Loss = 5.5579257, Perplexity = 259.28442\r\n",
      "I0412 15:02:29.299640 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.2487409, Global Step = 10, Loss = 4.223662, Perplexity = 68.28307 (23.690 sec)\r\n",
      "I0412 15:02:29.300678 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.422124\r\n",
      "I0412 15:02:49.681542 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.26993215, Global Step = 20, Loss = 4.030909, Perplexity = 56.312077 (20.382 sec)\r\n",
      "I0412 15:02:49.682621 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.49063\r\n",
      "I0412 15:03:03.672727 131715127822144 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 27...\r\n",
      "I0412 15:03:03.673010 131715127822144 basic_session_run_hooks.py:633] Saving checkpoints for 27 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 15:03:03.796399 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-27.meta\r\n",
      "I0412 15:03:03.796620 131715127822144 saver.py:93] 400\r\n",
      "I0412 15:03:03.796744 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-27.data-00000-of-00001\r\n",
      "I0412 15:03:03.796840 131715127822144 saver.py:93] 20100\r\n",
      "I0412 15:03:03.796967 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-27.index\r\n",
      "I0412 15:03:03.797065 131715127822144 saver.py:93] 20100\r\n",
      "I0412 15:03:03.797258 131715127822144 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 27...\r\n",
      "I0412 15:03:12.213471 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.23980223, Global Step = 30, Loss = 4.1534443, Perplexity = 63.65286 (22.532 sec)\r\n",
      "I0412 15:03:12.214790 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.44381\r\n",
      "I0412 15:03:34.350827 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.26817057, Global Step = 40, Loss = 4.015394, Perplexity = 55.44515 (22.137 sec)\r\n",
      "I0412 15:03:34.352111 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.451726\r\n",
      "I0412 15:03:53.367002 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.2575489, Global Step = 50, Loss = 4.021051, Perplexity = 55.759674 (19.016 sec)\r\n",
      "I0412 15:03:53.368055 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.525874\r\n",
      "I0412 15:04:05.089869 131715127822144 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 56...\r\n",
      "I0412 15:04:05.090144 131715127822144 basic_session_run_hooks.py:633] Saving checkpoints for 56 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 15:04:05.218239 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-56.data-00000-of-00001\r\n",
      "I0412 15:04:05.218478 131715127822144 saver.py:93] 19700\r\n",
      "I0412 15:04:05.218599 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-56.index\r\n",
      "I0412 15:04:05.218706 131715127822144 saver.py:93] 19700\r\n",
      "I0412 15:04:05.218806 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-56.meta\r\n",
      "I0412 15:04:05.218911 131715127822144 saver.py:93] 20100\r\n",
      "I0412 15:04:05.219120 131715127822144 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 56...\r\n",
      "I0412 15:04:16.604213 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.253758, Global Step = 60, Loss = 4.009625, Perplexity = 55.12619 (23.237 sec)\r\n",
      "I0412 15:04:16.605355 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.430343\r\n",
      "I0412 15:04:39.145596 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.25392202, Global Step = 70, Loss = 3.968743, Perplexity = 52.917976 (22.541 sec)\r\n",
      "I0412 15:04:39.146674 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.44363\r\n",
      "I0412 15:05:00.891383 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.23131657, Global Step = 80, Loss = 4.103103, Perplexity = 60.527824 (21.746 sec)\r\n",
      "I0412 15:05:00.892535 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.459857\r\n",
      "I0412 15:05:05.617085 131715127822144 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 83...\r\n",
      "I0412 15:05:05.617341 131715127822144 basic_session_run_hooks.py:633] Saving checkpoints for 83 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 15:05:05.726178 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-83.meta\r\n",
      "I0412 15:05:05.726404 131715127822144 saver.py:93] 400\r\n",
      "I0412 15:05:05.726517 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-83.data-00000-of-00001\r\n",
      "I0412 15:05:05.726611 131715127822144 saver.py:93] 20100\r\n",
      "I0412 15:05:05.726724 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-83.index\r\n",
      "I0412 15:05:05.726817 131715127822144 saver.py:93] 20100\r\n",
      "I0412 15:05:05.727013 131715127822144 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 83...\r\n",
      "I0412 15:05:24.290971 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.26853004, Global Step = 90, Loss = 3.8762019, Perplexity = 48.240643 (23.400 sec)\r\n",
      "I0412 15:05:24.292214 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.427356\r\n",
      "I0412 15:05:47.368253 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.45094\r\n",
      "I0412 15:05:47.369459 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.24604642, Global Step = 100, Loss = 3.8462484, Perplexity = 46.817093 (23.079 sec)\r\n",
      "I0412 15:05:47.370321 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.433311\r\n",
      "I0412 15:06:07.101860 131715127822144 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 110...\r\n",
      "I0412 15:06:07.102150 131715127822144 basic_session_run_hooks.py:633] Saving checkpoints for 110 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 15:06:07.324736 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-110.data-00000-of-00001\r\n",
      "I0412 15:06:07.325083 131715127822144 saver.py:93] 19700\r\n",
      "I0412 15:06:07.325375 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-110.index\r\n",
      "I0412 15:06:07.325573 131715127822144 saver.py:93] 19700\r\n",
      "I0412 15:06:07.325685 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-110.meta\r\n",
      "I0412 15:06:07.325879 131715127822144 saver.py:93] 20100\r\n",
      "I0412 15:06:07.326231 131715127822144 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 110...\r\n",
      "I0412 15:06:09.887573 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.2701544, Global Step = 110, Loss = 3.7722688, Perplexity = 43.478596 (22.518 sec)\r\n",
      "I0412 15:06:09.888639 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.444083\r\n",
      "I0412 15:06:31.224233 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.27969494, Global Step = 120, Loss = 3.6215749, Perplexity = 37.396416 (21.337 sec)\r\n",
      "I0412 15:06:31.225311 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.468677\r\n",
      "I0412 15:06:52.572941 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.24706168, Global Step = 130, Loss = 3.7201278, Perplexity = 41.26967 (21.349 sec)\r\n",
      "I0412 15:06:52.574001 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.468413\r\n",
      "I0412 15:07:08.815093 131715127822144 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 139...\r\n",
      "I0412 15:07:08.815358 131715127822144 basic_session_run_hooks.py:633] Saving checkpoints for 139 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 15:07:08.934130 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-139.meta\r\n",
      "I0412 15:07:08.934365 131715127822144 saver.py:93] 400\r\n",
      "I0412 15:07:08.934475 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-139.data-00000-of-00001\r\n",
      "I0412 15:07:08.934568 131715127822144 saver.py:93] 20100\r\n",
      "I0412 15:07:08.934675 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-139.index\r\n",
      "I0412 15:07:08.934777 131715127822144 saver.py:93] 20100\r\n",
      "I0412 15:07:08.934981 131715127822144 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 139...\r\n",
      "I0412 15:07:12.969862 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.25456008, Global Step = 140, Loss = 3.5802653, Perplexity = 35.883057 (20.397 sec)\r\n",
      "I0412 15:07:12.970970 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.49027\r\n",
      "I0412 15:07:32.179682 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.2653158, Global Step = 150, Loss = 3.5413766, Perplexity = 34.514397 (19.210 sec)\r\n",
      "I0412 15:07:32.180768 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.520567\r\n",
      "I0412 15:07:52.574587 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.28445047, Global Step = 160, Loss = 3.4441292, Perplexity = 31.316002 (20.395 sec)\r\n",
      "I0412 15:07:52.575665 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.490319\r\n",
      "I0412 15:08:09.819592 131715127822144 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 169...\r\n",
      "I0412 15:08:09.819848 131715127822144 basic_session_run_hooks.py:633] Saving checkpoints for 169 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 15:08:09.938629 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-169.meta\r\n",
      "I0412 15:08:09.938854 131715127822144 saver.py:93] 400\r\n",
      "I0412 15:08:09.939019 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-169.index\r\n",
      "I0412 15:08:09.939150 131715127822144 saver.py:93] 400\r\n",
      "I0412 15:08:09.939228 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-169.data-00000-of-00001\r\n",
      "I0412 15:08:09.939333 131715127822144 saver.py:93] 20100\r\n",
      "I0412 15:08:09.939503 131715127822144 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 169...\r\n",
      "I0412 15:08:14.903574 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.25497636, Global Step = 170, Loss = 3.4799633, Perplexity = 32.45853 (22.329 sec)\r\n",
      "I0412 15:08:14.904646 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.447848\r\n",
      "I0412 15:08:36.973251 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.25045696, Global Step = 180, Loss = 3.509492, Perplexity = 33.43128 (22.070 sec)\r\n",
      "I0412 15:08:36.974289 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.453111\r\n",
      "I0412 15:08:59.934604 131715127822144 basic_session_run_hooks.py:264] Accuracy = 0.2570735, Global Step = 190, Loss = 3.4428976, Perplexity = 31.277454 (22.961 sec)\r\n",
      "I0412 15:08:59.935724 131715127822144 basic_session_run_hooks.py:717] global_step/sec: 0.435513\r\n",
      "I0412 15:09:11.740738 131715127822144 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 196...\r\n",
      "I0412 15:09:11.741018 131715127822144 basic_session_run_hooks.py:633] Saving checkpoints for 196 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 15:09:11.843609 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-196.meta\r\n",
      "I0412 15:09:11.843868 131715127822144 saver.py:93] 400\r\n",
      "I0412 15:09:11.844045 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-196.data-00000-of-00001\r\n",
      "I0412 15:09:11.844175 131715127822144 saver.py:93] 20100\r\n",
      "I0412 15:09:11.844290 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-196.index\r\n",
      "I0412 15:09:11.844406 131715127822144 saver.py:93] 20100\r\n",
      "I0412 15:09:11.844618 131715127822144 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 196...\r\n",
      "I0412 15:09:21.540820 131715127822144 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 200...\r\n",
      "I0412 15:09:21.541109 131715127822144 basic_session_run_hooks.py:633] Saving checkpoints for 200 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 15:09:21.640292 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-200.index\r\n",
      "I0412 15:09:21.640505 131715127822144 saver.py:93] 0\r\n",
      "I0412 15:09:21.640619 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-200.meta\r\n",
      "I0412 15:09:21.640730 131715127822144 saver.py:93] 400\r\n",
      "I0412 15:09:21.640834 131715127822144 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-200.data-00000-of-00001\r\n",
      "I0412 15:09:21.640955 131715127822144 saver.py:93] 20100\r\n",
      "I0412 15:09:21.641146 131715127822144 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 200...\r\n",
      "I0412 15:09:21.686926 131715127822144 events_rnn_train.py:113] Training complete.\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 15:09:39.100475 133111408150336 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 15:09:39.101032 133111408150336 polyphony_rnn_train.py:94] Train dir: /kaggle/tmp/polyphony_rnn/logdir/run-005/train\r\n",
      "I0412 15:09:39.101518 133111408150336 polyphony_rnn_train.py:99] Eval dir: /kaggle/tmp/polyphony_rnn/logdir/run-005/eval\r\n",
      "W0412 15:09:39.102206 133111408150336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/polyphony_rnn_train\", line 8, in <module>\r\n",
      "    sys.exit(console_entry_point())\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 114, in console_entry_point\r\n",
      "    tf.app.run(main)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 36, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 308, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 254, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 104, in main\r\n",
      "    events_rnn_train.run_eval(build_graph_fn, train_dir, eval_dir, num_batches)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_train.py\", line 138, in run_eval\r\n",
      "    build_graph_fn()\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py\", line 113, in build\r\n",
      "    label_shape=label_shape, shuffle=mode == 'train')\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py\", line 66, in get_padded_batch\r\n",
      "    file_queue = tf.train.string_input_producer(file_list)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 357, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py\", line 253, in string_input_producer\r\n",
      "    raise ValueError(not_null_err)\r\n",
      "ValueError: string_input_producer requires a non-null input tensor\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 15:09:56.790606 133420666869568 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.01, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 15:09:56.791444 133420666869568 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-01/train\r\n",
      "W0412 15:09:56.792515 133420666869568 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 15:09:56.800942 133420666869568 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 15:09:56.802922 133420666869568 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0412 15:09:56.804438 133420666869568 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 15:09:56.805393 133420666869568 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 15:09:56.809430 133420666869568 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0412 15:09:56.817497 133420666869568 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0412 15:09:56.817638 133420666869568 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0412 15:09:56.856530 133420666869568 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0412 15:09:56.859971 133420666869568 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0412 15:09:56.860261 133420666869568 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0412 15:09:56.886325 133420666869568 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0412 15:09:56.908344 133420666869568 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0412 15:09:56.972617 133420666869568 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0412 15:09:57.119957 133420666869568 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0412 15:09:57.124132 133420666869568 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0412 15:09:57.930512 133420666869568 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0412 15:09:57.930809 133420666869568 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0412 15:09:58.039989 133420666869568 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0412 15:09:58.108807 133420666869568 monitored_session.py:243] Graph was finalized.\r\n",
      "I0412 15:09:59.295255 133420666869568 session_manager.py:527] Running local_init_op.\r\n",
      "I0412 15:09:59.302087 133420666869568 session_manager.py:530] Done running local_init_op.\r\n",
      "W0412 15:09:59.348455 133420666869568 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0412 15:10:00.051359 133420666869568 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0412 15:10:00.052964 133420666869568 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 15:10:00.270801 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-0.meta\r\n",
      "I0412 15:10:00.274209 133420666869568 saver.py:93] 400\r\n",
      "I0412 15:10:00.274441 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-0.index\r\n",
      "I0412 15:10:00.274808 133420666869568 saver.py:93] 400\r\n",
      "I0412 15:10:00.274992 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0412 15:10:00.275181 133420666869568 saver.py:93] 20100\r\n",
      "I0412 15:10:00.275497 133420666869568 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0412 15:10:04.377812 133420666869568 basic_session_run_hooks.py:266] Accuracy = 0.0022009464, Global Step = 0, Loss = 5.557768, Perplexity = 259.2435\r\n",
      "I0412 15:10:28.546953 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.26814753, Global Step = 10, Loss = 4.153025, Perplexity = 63.62619 (24.169 sec)\r\n",
      "I0412 15:10:28.547988 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.413752\r\n",
      "I0412 15:10:48.714046 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.28258654, Global Step = 20, Loss = 4.0350533, Perplexity = 56.545933 (20.167 sec)\r\n",
      "I0412 15:10:48.715074 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.495857\r\n",
      "I0412 15:11:01.965445 133420666869568 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 27...\r\n",
      "I0412 15:11:01.965730 133420666869568 basic_session_run_hooks.py:633] Saving checkpoints for 27 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 15:11:02.093935 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-27.meta\r\n",
      "I0412 15:11:02.094172 133420666869568 saver.py:93] 400\r\n",
      "I0412 15:11:02.094280 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-27.data-00000-of-00001\r\n",
      "I0412 15:11:02.094384 133420666869568 saver.py:93] 20100\r\n",
      "I0412 15:11:02.094481 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-27.index\r\n",
      "I0412 15:11:02.094571 133420666869568 saver.py:93] 20100\r\n",
      "I0412 15:11:02.094789 133420666869568 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 27...\r\n",
      "I0412 15:11:11.178107 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.24285783, Global Step = 30, Loss = 4.1843247, Perplexity = 65.649155 (22.464 sec)\r\n",
      "I0412 15:11:11.179197 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.445154\r\n",
      "I0412 15:11:32.544008 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.25021973, Global Step = 40, Loss = 4.0590124, Perplexity = 57.917084 (21.366 sec)\r\n",
      "I0412 15:11:32.545144 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.468035\r\n",
      "I0412 15:11:52.973602 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.27631006, Global Step = 50, Loss = 3.9262533, Perplexity = 50.716602 (20.430 sec)\r\n",
      "I0412 15:11:52.974849 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.489484\r\n",
      "I0412 15:12:02.132000 133420666869568 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 55...\r\n",
      "I0412 15:12:02.132254 133420666869568 basic_session_run_hooks.py:633] Saving checkpoints for 55 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 15:12:02.232379 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-55.index\r\n",
      "I0412 15:12:02.232598 133420666869568 saver.py:93] 0\r\n",
      "I0412 15:12:02.232717 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-55.data-00000-of-00001\r\n",
      "I0412 15:12:02.232821 133420666869568 saver.py:93] 19700\r\n",
      "I0412 15:12:02.232943 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-55.meta\r\n",
      "I0412 15:12:02.233046 133420666869568 saver.py:93] 20100\r\n",
      "I0412 15:12:02.233241 133420666869568 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 55...\r\n",
      "I0412 15:12:14.351893 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.26961982, Global Step = 60, Loss = 3.946137, Perplexity = 51.735126 (21.378 sec)\r\n",
      "I0412 15:12:14.352962 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.467768\r\n",
      "I0412 15:12:34.956296 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.2669015, Global Step = 70, Loss = 3.95256, Perplexity = 52.06849 (20.604 sec)\r\n",
      "I0412 15:12:34.957351 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.485333\r\n",
      "I0412 15:12:57.751476 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.23376743, Global Step = 80, Loss = 4.1094246, Perplexity = 60.91166 (22.795 sec)\r\n",
      "I0412 15:12:57.752666 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.438687\r\n",
      "I0412 15:13:02.159716 133420666869568 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 83...\r\n",
      "I0412 15:13:02.160043 133420666869568 basic_session_run_hooks.py:633] Saving checkpoints for 83 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 15:13:02.259346 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-83.meta\r\n",
      "I0412 15:13:02.259574 133420666869568 saver.py:93] 400\r\n",
      "I0412 15:13:02.259701 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-83.data-00000-of-00001\r\n",
      "I0412 15:13:02.259813 133420666869568 saver.py:93] 20100\r\n",
      "I0412 15:13:02.259929 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-83.index\r\n",
      "I0412 15:13:02.260038 133420666869568 saver.py:93] 20100\r\n",
      "I0412 15:13:02.260231 133420666869568 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 83...\r\n",
      "I0412 15:13:21.446520 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.25741965, Global Step = 90, Loss = 4.059879, Perplexity = 57.967285 (23.695 sec)\r\n",
      "I0412 15:13:21.447611 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.422031\r\n",
      "I0412 15:13:44.569187 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.454149\r\n",
      "I0412 15:13:44.570339 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.2625415, Global Step = 100, Loss = 4.0351667, Perplexity = 56.552345 (23.124 sec)\r\n",
      "I0412 15:13:44.571248 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.432457\r\n",
      "I0412 15:14:02.492096 133420666869568 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 109...\r\n",
      "I0412 15:14:02.492347 133420666869568 basic_session_run_hooks.py:633] Saving checkpoints for 109 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 15:14:02.590848 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-109.meta\r\n",
      "I0412 15:14:02.591094 133420666869568 saver.py:93] 400\r\n",
      "I0412 15:14:02.591221 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-109.data-00000-of-00001\r\n",
      "I0412 15:14:02.591334 133420666869568 saver.py:93] 20100\r\n",
      "I0412 15:14:02.591430 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-109.index\r\n",
      "I0412 15:14:02.591519 133420666869568 saver.py:93] 20100\r\n",
      "I0412 15:14:02.591696 133420666869568 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 109...\r\n",
      "I0412 15:14:07.046011 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.26709828, Global Step = 110, Loss = 4.0752196, Perplexity = 58.863403 (22.476 sec)\r\n",
      "I0412 15:14:07.047043 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.444923\r\n",
      "I0412 15:14:28.647341 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.2632459, Global Step = 120, Loss = 4.04446, Perplexity = 57.080345 (21.601 sec)\r\n",
      "I0412 15:14:28.648396 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.462934\r\n",
      "I0412 15:14:50.640947 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.2448876, Global Step = 130, Loss = 4.1070085, Perplexity = 60.764664 (21.994 sec)\r\n",
      "I0412 15:14:50.642120 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.454675\r\n",
      "I0412 15:15:04.741227 133420666869568 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 138...\r\n",
      "I0412 15:15:04.741489 133420666869568 basic_session_run_hooks.py:633] Saving checkpoints for 138 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 15:15:04.869325 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-138.meta\r\n",
      "I0412 15:15:04.869543 133420666869568 saver.py:93] 400\r\n",
      "I0412 15:15:04.869674 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-138.index\r\n",
      "I0412 15:15:04.869786 133420666869568 saver.py:93] 400\r\n",
      "I0412 15:15:04.869916 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-138.data-00000-of-00001\r\n",
      "I0412 15:15:04.870042 133420666869568 saver.py:93] 20100\r\n",
      "I0412 15:15:04.870223 133420666869568 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 138...\r\n",
      "I0412 15:15:11.139349 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.27587444, Global Step = 140, Loss = 3.938096, Perplexity = 51.320797 (20.498 sec)\r\n",
      "I0412 15:15:11.140497 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.487844\r\n",
      "I0412 15:15:33.214437 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.24396653, Global Step = 150, Loss = 4.0567865, Perplexity = 57.78831 (22.075 sec)\r\n",
      "I0412 15:15:33.215769 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.452995\r\n",
      "I0412 15:15:55.030021 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.2695249, Global Step = 160, Loss = 3.9138658, Perplexity = 50.092224 (21.816 sec)\r\n",
      "I0412 15:15:55.031216 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.458391\r\n",
      "I0412 15:16:07.078516 133420666869568 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 166...\r\n",
      "I0412 15:16:07.078769 133420666869568 basic_session_run_hooks.py:633] Saving checkpoints for 166 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 15:16:07.179470 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-166.meta\r\n",
      "I0412 15:16:07.179704 133420666869568 saver.py:93] 400\r\n",
      "I0412 15:16:07.179826 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-166.data-00000-of-00001\r\n",
      "I0412 15:16:07.179963 133420666869568 saver.py:93] 20100\r\n",
      "I0412 15:16:07.180073 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-166.index\r\n",
      "I0412 15:16:07.180175 133420666869568 saver.py:93] 20100\r\n",
      "I0412 15:16:07.180362 133420666869568 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 166...\r\n",
      "I0412 15:16:17.927520 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.2523832, Global Step = 170, Loss = 4.0475, Perplexity = 57.254154 (22.898 sec)\r\n",
      "I0412 15:16:17.928647 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.43673\r\n",
      "I0412 15:16:39.568773 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.29150808, Global Step = 180, Loss = 3.977798, Perplexity = 53.39932 (21.641 sec)\r\n",
      "I0412 15:16:39.569807 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.462082\r\n",
      "I0412 15:17:04.591059 133420666869568 basic_session_run_hooks.py:264] Accuracy = 0.26023683, Global Step = 190, Loss = 4.076162, Perplexity = 58.918896 (25.022 sec)\r\n",
      "I0412 15:17:04.592093 133420666869568 basic_session_run_hooks.py:717] global_step/sec: 0.399644\r\n",
      "I0412 15:17:09.279543 133420666869568 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 193...\r\n",
      "I0412 15:17:09.279797 133420666869568 basic_session_run_hooks.py:633] Saving checkpoints for 193 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 15:17:09.384400 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-193.data-00000-of-00001\r\n",
      "I0412 15:17:09.384630 133420666869568 saver.py:93] 19700\r\n",
      "I0412 15:17:09.384758 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-193.index\r\n",
      "I0412 15:17:09.384858 133420666869568 saver.py:93] 19700\r\n",
      "I0412 15:17:09.384990 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-193.meta\r\n",
      "I0412 15:17:09.385084 133420666869568 saver.py:93] 20100\r\n",
      "I0412 15:17:09.385277 133420666869568 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 193...\r\n",
      "I0412 15:17:25.547005 133420666869568 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 200...\r\n",
      "I0412 15:17:25.547274 133420666869568 basic_session_run_hooks.py:633] Saving checkpoints for 200 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 15:17:25.667261 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-200.index\r\n",
      "I0412 15:17:25.667478 133420666869568 saver.py:93] 0\r\n",
      "I0412 15:17:25.667604 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-200.meta\r\n",
      "I0412 15:17:25.667738 133420666869568 saver.py:93] 400\r\n",
      "I0412 15:17:25.667829 133420666869568 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-200.data-00000-of-00001\r\n",
      "I0412 15:17:25.667939 133420666869568 saver.py:93] 20100\r\n",
      "I0412 15:17:25.668131 133420666869568 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 200...\r\n",
      "I0412 15:17:25.715116 133420666869568 events_rnn_train.py:113] Training complete.\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 15:17:44.049518 139213749905216 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 15:17:44.050053 139213749905216 polyphony_rnn_train.py:94] Train dir: /kaggle/tmp/polyphony_rnn/logdir/run-01/train\r\n",
      "I0412 15:17:44.050539 139213749905216 polyphony_rnn_train.py:99] Eval dir: /kaggle/tmp/polyphony_rnn/logdir/run-01/eval\r\n",
      "W0412 15:17:44.051259 139213749905216 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/polyphony_rnn_train\", line 8, in <module>\r\n",
      "    sys.exit(console_entry_point())\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 114, in console_entry_point\r\n",
      "    tf.app.run(main)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 36, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 308, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 254, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 104, in main\r\n",
      "    events_rnn_train.run_eval(build_graph_fn, train_dir, eval_dir, num_batches)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_train.py\", line 138, in run_eval\r\n",
      "    build_graph_fn()\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py\", line 113, in build\r\n",
      "    label_shape=label_shape, shuffle=mode == 'train')\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py\", line 66, in get_padded_batch\r\n",
      "    file_queue = tf.train.string_input_producer(file_list)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 357, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py\", line 253, in string_input_producer\r\n",
      "    raise ValueError(not_null_err)\r\n",
      "ValueError: string_input_producer requires a non-null input tensor\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 15:18:01.166149 129424944232256 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.02, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 15:18:01.166588 129424944232256 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-02/train\r\n",
      "W0412 15:18:01.167684 129424944232256 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 15:18:01.175853 129424944232256 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 15:18:01.177831 129424944232256 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0412 15:18:01.179400 129424944232256 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 15:18:01.180375 129424944232256 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 15:18:01.184049 129424944232256 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0412 15:18:01.191540 129424944232256 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0412 15:18:01.191693 129424944232256 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0412 15:18:01.231904 129424944232256 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0412 15:18:01.235966 129424944232256 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0412 15:18:01.236345 129424944232256 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0412 15:18:01.275760 129424944232256 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0412 15:18:01.300376 129424944232256 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0412 15:18:01.365093 129424944232256 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0412 15:18:01.510363 129424944232256 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0412 15:18:01.513438 129424944232256 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0412 15:18:02.331250 129424944232256 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0412 15:18:02.331517 129424944232256 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0412 15:18:02.442423 129424944232256 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0412 15:18:02.510636 129424944232256 monitored_session.py:243] Graph was finalized.\r\n",
      "I0412 15:18:03.779737 129424944232256 session_manager.py:527] Running local_init_op.\r\n",
      "I0412 15:18:03.786603 129424944232256 session_manager.py:530] Done running local_init_op.\r\n",
      "W0412 15:18:03.812087 129424944232256 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0412 15:18:04.545785 129424944232256 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0412 15:18:04.547838 129424944232256 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0412 15:18:04.805505 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-0.meta\r\n",
      "I0412 15:18:04.806015 129424944232256 saver.py:93] 400\r\n",
      "I0412 15:18:04.806414 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-0.index\r\n",
      "I0412 15:18:04.806614 129424944232256 saver.py:93] 400\r\n",
      "I0412 15:18:04.806726 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0412 15:18:04.806994 129424944232256 saver.py:93] 20100\r\n",
      "I0412 15:18:04.807501 129424944232256 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0412 15:18:08.946962 129424944232256 basic_session_run_hooks.py:266] Accuracy = 0.0058151884, Global Step = 0, Loss = 5.5557566, Perplexity = 258.72263\r\n",
      "I0412 15:18:31.881188 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.26495728, Global Step = 10, Loss = 4.1867523, Perplexity = 65.80872 (22.934 sec)\r\n",
      "I0412 15:18:31.882303 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.436029\r\n",
      "I0412 15:18:53.197448 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.28243995, Global Step = 20, Loss = 4.03224, Perplexity = 56.38707 (21.316 sec)\r\n",
      "I0412 15:18:53.198517 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.469126\r\n",
      "I0412 15:19:06.015603 129424944232256 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 27...\r\n",
      "I0412 15:19:06.015864 129424944232256 basic_session_run_hooks.py:633] Saving checkpoints for 27 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0412 15:19:06.151569 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-27.meta\r\n",
      "I0412 15:19:06.151791 129424944232256 saver.py:93] 400\r\n",
      "I0412 15:19:06.151947 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-27.data-00000-of-00001\r\n",
      "I0412 15:19:06.152069 129424944232256 saver.py:93] 20100\r\n",
      "I0412 15:19:06.152182 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-27.index\r\n",
      "I0412 15:19:06.152286 129424944232256 saver.py:93] 20100\r\n",
      "I0412 15:19:06.152476 129424944232256 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 27...\r\n",
      "I0412 15:19:15.786445 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.26582578, Global Step = 30, Loss = 4.101555, Perplexity = 60.43418 (22.589 sec)\r\n",
      "I0412 15:19:15.787539 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.442693\r\n",
      "I0412 15:19:36.538260 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.2633686, Global Step = 40, Loss = 4.032432, Perplexity = 56.397907 (20.752 sec)\r\n",
      "I0412 15:19:36.539339 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.481885\r\n",
      "I0412 15:19:56.293421 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.26455718, Global Step = 50, Loss = 3.9816406, Perplexity = 53.604904 (19.755 sec)\r\n",
      "I0412 15:19:56.294631 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.506194\r\n",
      "I0412 15:20:06.736853 129424944232256 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 56...\r\n",
      "I0412 15:20:06.737114 129424944232256 basic_session_run_hooks.py:633] Saving checkpoints for 56 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0412 15:20:06.863987 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-56.data-00000-of-00001\r\n",
      "I0412 15:20:06.864214 129424944232256 saver.py:93] 19700\r\n",
      "I0412 15:20:06.864332 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-56.index\r\n",
      "I0412 15:20:06.864438 129424944232256 saver.py:93] 19700\r\n",
      "I0412 15:20:06.864535 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-56.meta\r\n",
      "I0412 15:20:06.864624 129424944232256 saver.py:93] 20100\r\n",
      "I0412 15:20:06.864815 129424944232256 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 56...\r\n",
      "I0412 15:20:18.269276 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.25812867, Global Step = 60, Loss = 4.0234013, Perplexity = 55.890884 (21.976 sec)\r\n",
      "I0412 15:20:18.270350 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.455048\r\n",
      "I0412 15:20:39.597661 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.26249936, Global Step = 70, Loss = 4.0608783, Perplexity = 58.025253 (21.328 sec)\r\n",
      "I0412 15:20:39.598717 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.468859\r\n",
      "I0412 15:21:01.606823 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.24491806, Global Step = 80, Loss = 4.060086, Perplexity = 57.979282 (22.009 sec)\r\n",
      "I0412 15:21:01.607959 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.454355\r\n",
      "I0412 15:21:08.621600 129424944232256 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 84...\r\n",
      "I0412 15:21:08.621860 129424944232256 basic_session_run_hooks.py:633] Saving checkpoints for 84 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0412 15:21:08.731990 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-84.index\r\n",
      "I0412 15:21:08.732233 129424944232256 saver.py:93] 0\r\n",
      "I0412 15:21:08.732353 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-84.data-00000-of-00001\r\n",
      "I0412 15:21:08.732460 129424944232256 saver.py:93] 19700\r\n",
      "I0412 15:21:08.732554 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-84.meta\r\n",
      "I0412 15:21:08.732644 129424944232256 saver.py:93] 20100\r\n",
      "I0412 15:21:08.732830 129424944232256 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 84...\r\n",
      "I0412 15:21:26.034753 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.2635695, Global Step = 90, Loss = 4.021892, Perplexity = 55.8066 (24.428 sec)\r\n",
      "I0412 15:21:26.036016 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.409366\r\n",
      "I0412 15:21:48.322841 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.455837\r\n",
      "I0412 15:21:48.323969 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.27251127, Global Step = 100, Loss = 4.033512, Perplexity = 56.458855 (22.289 sec)\r\n",
      "I0412 15:21:48.324978 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.448652\r\n",
      "I0412 15:22:08.874154 129424944232256 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 111...\r\n",
      "I0412 15:22:08.874409 129424944232256 basic_session_run_hooks.py:633] Saving checkpoints for 111 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0412 15:22:09.009851 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-111.index\r\n",
      "I0412 15:22:09.010129 129424944232256 saver.py:93] 0\r\n",
      "I0412 15:22:09.010247 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-111.meta\r\n",
      "I0412 15:22:09.010354 129424944232256 saver.py:93] 400\r\n",
      "I0412 15:22:09.010456 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-111.data-00000-of-00001\r\n",
      "I0412 15:22:09.010548 129424944232256 saver.py:93] 20100\r\n",
      "I0412 15:22:09.010757 129424944232256 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 111...\r\n",
      "I0412 15:22:09.011168 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.26787615, Global Step = 110, Loss = 4.0339284, Perplexity = 56.48236 (20.687 sec)\r\n",
      "I0412 15:22:09.012167 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.483391\r\n",
      "I0412 15:22:31.705806 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.28522316, Global Step = 120, Loss = 3.937686, Perplexity = 51.299755 (22.695 sec)\r\n",
      "I0412 15:22:31.707003 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.440629\r\n",
      "I0412 15:22:54.332924 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.26519266, Global Step = 130, Loss = 4.0442014, Perplexity = 57.065594 (22.627 sec)\r\n",
      "I0412 15:22:54.333983 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.44195\r\n",
      "I0412 15:23:09.419140 129424944232256 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 139...\r\n",
      "I0412 15:23:09.419413 129424944232256 basic_session_run_hooks.py:633] Saving checkpoints for 139 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0412 15:23:09.523592 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-139.meta\r\n",
      "I0412 15:23:09.523813 129424944232256 saver.py:93] 400\r\n",
      "I0412 15:23:09.523965 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-139.data-00000-of-00001\r\n",
      "I0412 15:23:09.524112 129424944232256 saver.py:93] 20100\r\n",
      "I0412 15:23:09.524212 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-139.index\r\n",
      "I0412 15:23:09.524323 129424944232256 saver.py:93] 20100\r\n",
      "I0412 15:23:09.524502 129424944232256 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 139...\r\n",
      "I0412 15:23:14.582571 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.26182786, Global Step = 140, Loss = 4.0728946, Perplexity = 58.726704 (20.250 sec)\r\n",
      "I0412 15:23:14.583576 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.493836\r\n",
      "I0412 15:23:36.181229 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.2454007, Global Step = 150, Loss = 4.054424, Perplexity = 57.65193 (21.599 sec)\r\n",
      "I0412 15:23:36.182225 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.462993\r\n",
      "I0412 15:23:57.944135 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.2598227, Global Step = 160, Loss = 4.0350075, Perplexity = 56.543343 (21.763 sec)\r\n",
      "I0412 15:23:57.945154 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.459497\r\n",
      "I0412 15:24:11.304772 129424944232256 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 167...\r\n",
      "I0412 15:24:11.305086 129424944232256 basic_session_run_hooks.py:633] Saving checkpoints for 167 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0412 15:24:11.439089 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-167.meta\r\n",
      "I0412 15:24:11.439321 129424944232256 saver.py:93] 400\r\n",
      "I0412 15:24:11.439465 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-167.index\r\n",
      "I0412 15:24:11.439614 129424944232256 saver.py:93] 400\r\n",
      "I0412 15:24:11.439715 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-167.data-00000-of-00001\r\n",
      "I0412 15:24:11.439831 129424944232256 saver.py:93] 20100\r\n",
      "I0412 15:24:11.440057 129424944232256 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 167...\r\n",
      "I0412 15:24:19.270170 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.24415466, Global Step = 170, Loss = 4.0222025, Perplexity = 55.82392 (21.326 sec)\r\n",
      "I0412 15:24:19.271459 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.468905\r\n",
      "I0412 15:24:41.083876 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.24607918, Global Step = 180, Loss = 4.094254, Perplexity = 59.994564 (21.814 sec)\r\n",
      "I0412 15:24:41.085036 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.458429\r\n",
      "I0412 15:25:06.354150 129424944232256 basic_session_run_hooks.py:264] Accuracy = 0.24486968, Global Step = 190, Loss = 4.1186333, Perplexity = 61.475163 (25.270 sec)\r\n",
      "I0412 15:25:06.355292 129424944232256 basic_session_run_hooks.py:717] global_step/sec: 0.395722\r\n",
      "I0412 15:25:14.157372 129424944232256 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 194...\r\n",
      "I0412 15:25:14.157619 129424944232256 basic_session_run_hooks.py:633] Saving checkpoints for 194 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0412 15:25:14.267006 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-194.meta\r\n",
      "I0412 15:25:14.267230 129424944232256 saver.py:93] 400\r\n",
      "I0412 15:25:14.267351 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-194.data-00000-of-00001\r\n",
      "I0412 15:25:14.267458 129424944232256 saver.py:93] 20100\r\n",
      "I0412 15:25:14.267566 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-194.index\r\n",
      "I0412 15:25:14.267668 129424944232256 saver.py:93] 20100\r\n",
      "I0412 15:25:14.267862 129424944232256 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 194...\r\n",
      "I0412 15:25:27.651262 129424944232256 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 200...\r\n",
      "I0412 15:25:27.651518 129424944232256 basic_session_run_hooks.py:633] Saving checkpoints for 200 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0412 15:25:27.752811 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-200.index\r\n",
      "I0412 15:25:27.753063 129424944232256 saver.py:93] 0\r\n",
      "I0412 15:25:27.753196 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-200.meta\r\n",
      "I0412 15:25:27.753333 129424944232256 saver.py:93] 400\r\n",
      "I0412 15:25:27.753413 129424944232256 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-200.data-00000-of-00001\r\n",
      "I0412 15:25:27.753518 129424944232256 saver.py:93] 20100\r\n",
      "I0412 15:25:27.753702 129424944232256 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 200...\r\n",
      "I0412 15:25:27.801074 129424944232256 events_rnn_train.py:113] Training complete.\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 15:25:45.671850 123300050954048 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 15:25:45.672325 123300050954048 polyphony_rnn_train.py:94] Train dir: /kaggle/tmp/polyphony_rnn/logdir/run-02/train\r\n",
      "I0412 15:25:45.672902 123300050954048 polyphony_rnn_train.py:99] Eval dir: /kaggle/tmp/polyphony_rnn/logdir/run-02/eval\r\n",
      "W0412 15:25:45.673657 123300050954048 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/polyphony_rnn_train\", line 8, in <module>\r\n",
      "    sys.exit(console_entry_point())\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 114, in console_entry_point\r\n",
      "    tf.app.run(main)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 36, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 308, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 254, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 104, in main\r\n",
      "    events_rnn_train.run_eval(build_graph_fn, train_dir, eval_dir, num_batches)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_train.py\", line 138, in run_eval\r\n",
      "    build_graph_fn()\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py\", line 113, in build\r\n",
      "    label_shape=label_shape, shuffle=mode == 'train')\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py\", line 66, in get_padded_batch\r\n",
      "    file_queue = tf.train.string_input_producer(file_list)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 357, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py\", line 253, in string_input_producer\r\n",
      "    raise ValueError(not_null_err)\r\n",
      "ValueError: string_input_producer requires a non-null input tensor\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 15:26:03.270433 123774169519936 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.05, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 15:26:03.270987 123774169519936 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-05/train\r\n",
      "W0412 15:26:03.272312 123774169519936 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 15:26:03.281554 123774169519936 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 15:26:03.284047 123774169519936 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0412 15:26:03.285977 123774169519936 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 15:26:03.286910 123774169519936 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 15:26:03.291466 123774169519936 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0412 15:26:03.299813 123774169519936 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0412 15:26:03.299962 123774169519936 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0412 15:26:03.338789 123774169519936 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0412 15:26:03.341698 123774169519936 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0412 15:26:03.341971 123774169519936 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0412 15:26:03.367600 123774169519936 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0412 15:26:03.389324 123774169519936 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0412 15:26:03.452641 123774169519936 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0412 15:26:03.595722 123774169519936 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0412 15:26:03.598673 123774169519936 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0412 15:26:04.386617 123774169519936 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0412 15:26:04.386876 123774169519936 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0412 15:26:04.497504 123774169519936 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0412 15:26:04.565870 123774169519936 monitored_session.py:243] Graph was finalized.\r\n",
      "I0412 15:26:05.762313 123774169519936 session_manager.py:527] Running local_init_op.\r\n",
      "I0412 15:26:05.769143 123774169519936 session_manager.py:530] Done running local_init_op.\r\n",
      "W0412 15:26:05.794373 123774169519936 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0412 15:26:06.542968 123774169519936 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0412 15:26:06.544608 123774169519936 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt.\r\n",
      "I0412 15:26:06.732239 123774169519936 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-0.meta\r\n",
      "I0412 15:26:06.732589 123774169519936 saver.py:93] 400\r\n",
      "I0412 15:26:06.732958 123774169519936 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-0.index\r\n",
      "I0412 15:26:06.733112 123774169519936 saver.py:93] 400\r\n",
      "I0412 15:26:06.733243 123774169519936 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0412 15:26:06.733354 123774169519936 saver.py:93] 20100\r\n",
      "I0412 15:26:06.733562 123774169519936 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0412 15:26:10.885258 123774169519936 basic_session_run_hooks.py:266] Accuracy = 0.0027072667, Global Step = 0, Loss = 5.558312, Perplexity = 259.3846\r\n",
      "E0412 15:26:18.088278 123774169519936 basic_session_run_hooks.py:785] Model diverged with loss = NaN.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/polyphony_rnn_train\", line 8, in <module>\r\n",
      "    sys.exit(console_entry_point())\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 114, in console_entry_point\r\n",
      "    tf.app.run(main)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 36, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 308, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 254, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 110, in main\r\n",
      "    checkpoints_to_keep=FLAGS.num_checkpoints)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_train.py\", line 112, in run_training\r\n",
      "    is_chief=task == 0)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tf_slim/training/training.py\", line 551, in train\r\n",
      "    loss = session.run(train_op, run_metadata=run_metadata)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 786, in run\r\n",
      "    run_metadata=run_metadata)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1315, in run\r\n",
      "    run_metadata=run_metadata)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1416, in run\r\n",
      "    raise six.reraise(*original_exc_info)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/six.py\", line 719, in reraise\r\n",
      "    raise value\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1401, in run\r\n",
      "    return self._sess.run(*args, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1482, in run\r\n",
      "    run_metadata=run_metadata))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 786, in after_run\r\n",
      "    raise NanLossDuringTrainingError\r\n",
      "tensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 15:26:36.173509 131566067377984 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 15:26:36.174011 131566067377984 polyphony_rnn_train.py:94] Train dir: /kaggle/tmp/polyphony_rnn/logdir/run-05/train\r\n",
      "I0412 15:26:36.174546 131566067377984 polyphony_rnn_train.py:99] Eval dir: /kaggle/tmp/polyphony_rnn/logdir/run-05/eval\r\n",
      "W0412 15:26:36.175260 131566067377984 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/polyphony_rnn_train\", line 8, in <module>\r\n",
      "    sys.exit(console_entry_point())\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 114, in console_entry_point\r\n",
      "    tf.app.run(main)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 36, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 308, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 254, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 104, in main\r\n",
      "    events_rnn_train.run_eval(build_graph_fn, train_dir, eval_dir, num_batches)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_train.py\", line 138, in run_eval\r\n",
      "    build_graph_fn()\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py\", line 113, in build\r\n",
      "    label_shape=label_shape, shuffle=mode == 'train')\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py\", line 66, in get_padded_batch\r\n",
      "    file_queue = tf.train.string_input_producer(file_list)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 357, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py\", line 253, in string_input_producer\r\n",
      "    raise ValueError(not_null_err)\r\n",
      "ValueError: string_input_producer requires a non-null input tensor\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 15:26:53.429325 124271328356160 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.1, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 15:26:53.429828 124271328356160 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-1/train\r\n",
      "W0412 15:26:53.430860 124271328356160 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 15:26:53.441022 124271328356160 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 15:26:53.443375 124271328356160 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0412 15:26:53.445419 124271328356160 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 15:26:53.446642 124271328356160 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 15:26:53.451645 124271328356160 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0412 15:26:53.461930 124271328356160 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0412 15:26:53.462108 124271328356160 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0412 15:26:53.504689 124271328356160 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0412 15:26:53.508601 124271328356160 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0412 15:26:53.508960 124271328356160 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0412 15:26:53.537474 124271328356160 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0412 15:26:53.560919 124271328356160 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0412 15:26:53.625079 124271328356160 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0412 15:26:53.770332 124271328356160 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0412 15:26:53.773348 124271328356160 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0412 15:26:54.574346 124271328356160 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0412 15:26:54.574626 124271328356160 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0412 15:26:54.684219 124271328356160 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0412 15:26:54.753083 124271328356160 monitored_session.py:243] Graph was finalized.\r\n",
      "I0412 15:26:55.936072 124271328356160 session_manager.py:527] Running local_init_op.\r\n",
      "I0412 15:26:55.944381 124271328356160 session_manager.py:530] Done running local_init_op.\r\n",
      "W0412 15:26:55.969237 124271328356160 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0412 15:26:56.663237 124271328356160 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0412 15:26:56.665108 124271328356160 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt.\r\n",
      "I0412 15:26:56.859801 124271328356160 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-0.meta\r\n",
      "I0412 15:26:56.863019 124271328356160 saver.py:93] 400\r\n",
      "I0412 15:26:56.863487 124271328356160 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-0.index\r\n",
      "I0412 15:26:56.863750 124271328356160 saver.py:93] 400\r\n",
      "I0412 15:26:56.863940 124271328356160 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0412 15:26:56.864149 124271328356160 saver.py:93] 20100\r\n",
      "I0412 15:26:56.864438 124271328356160 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0412 15:27:00.996248 124271328356160 basic_session_run_hooks.py:266] Accuracy = 0.0027019228, Global Step = 0, Loss = 5.557715, Perplexity = 259.2298\r\n",
      "E0412 15:27:08.347073 124271328356160 basic_session_run_hooks.py:785] Model diverged with loss = NaN.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/polyphony_rnn_train\", line 8, in <module>\r\n",
      "    sys.exit(console_entry_point())\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 114, in console_entry_point\r\n",
      "    tf.app.run(main)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 36, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 308, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 254, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 110, in main\r\n",
      "    checkpoints_to_keep=FLAGS.num_checkpoints)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_train.py\", line 112, in run_training\r\n",
      "    is_chief=task == 0)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tf_slim/training/training.py\", line 551, in train\r\n",
      "    loss = session.run(train_op, run_metadata=run_metadata)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 786, in run\r\n",
      "    run_metadata=run_metadata)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1315, in run\r\n",
      "    run_metadata=run_metadata)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1416, in run\r\n",
      "    raise six.reraise(*original_exc_info)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/six.py\", line 719, in reraise\r\n",
      "    raise value\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1401, in run\r\n",
      "    return self._sess.run(*args, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1482, in run\r\n",
      "    run_metadata=run_metadata))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 786, in after_run\r\n",
      "    raise NanLossDuringTrainingError\r\n",
      "tensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/flags/_flag.py\", line 180, in _parse\r\n",
      "    return self.parser.parse(argument)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/flags/_argument_parser.py\", line 166, in parse\r\n",
      "    val = self.convert(argument)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/flags/_argument_parser.py\", line 279, in convert\r\n",
      "    return int(argument, base)\r\n",
      "ValueError: invalid literal for int() with base 10: '='\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/polyphony_rnn_train\", line 8, in <module>\r\n",
      "    sys.exit(console_entry_point())\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 114, in console_entry_point\r\n",
      "    tf.app.run(main)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 36, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 302, in run\r\n",
      "    flags_parser,\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 371, in _run_init\r\n",
      "    flags_parser=flags_parser,\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 216, in _register_and_parse_flags_with_usage\r\n",
      "    args_to_main = flags_parser(original_argv)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 27, in _parse_flags_tolerate_undef\r\n",
      "    return flags.FLAGS(_sys.argv if argv is None else argv, known_only=True)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/flags.py\", line 109, in __call__\r\n",
      "    return self.__dict__['__wrapped'].__call__(*args, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/flags/_flagvalues.py\", line 643, in __call__\r\n",
      "    unknown_flags, unparsed_args = self._parse_args(args, known_only)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/flags/_flagvalues.py\", line 792, in _parse_args\r\n",
      "    flag.parse(value)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/flags/_flag.py\", line 165, in parse\r\n",
      "    self.value = self._parse(argument)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/flags/_flag.py\", line 183, in _parse\r\n",
      "    'flag --%s=%s: %s' % (self.name, argument, e))\r\n",
      "absl.flags._exceptions.IllegalFlagValueError: flag --num_eval_examples==: invalid literal for int() with base 10: '='\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Different Learning Rates: 0.001, 0.005, 0.01, 0.02, 0.05, 0.1\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-001 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.001\" \\\n",
    "--num_checkpoints=100 \n",
    "# --summary_frequency=10\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-001 \\\n",
    "--sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "--num_eval_examples=1000 \\\n",
    "--eval\n",
    "\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-005 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.005\" \\\n",
    "--num_checkpoints=100 \n",
    "# --summary_frequency=10\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-005 \\\n",
    "--sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "--num_eval_examples=1000 \\\n",
    "--eval\n",
    "\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-01 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.01\" \\\n",
    "--num_checkpoints=20\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-01 \\\n",
    "--sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "--num_eval_examples=1000 \\\n",
    "--eval\n",
    "\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-02 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.02\" \\\n",
    "--num_checkpoints=20\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-02 \\\n",
    "--sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "--num_eval_examples=1000 \\\n",
    "--eval\n",
    "\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-05 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.05\" \\\n",
    "--num_checkpoints=20 \n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-05 \\\n",
    "--sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "--num_eval_examples=1000 \\\n",
    "--eval\n",
    "\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-1 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.1\" \\\n",
    "--num_checkpoints=20\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-1 \\\n",
    "--sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "--num_eval_examples = 1000 \\\n",
    "--eval\n",
    "\n",
    "\n",
    "!cp -R /kaggle/tmp/polyphony_rnn/logdir /kaggle/working/logdir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2678.131707,
   "end_time": "2023-04-12T15:27:31.204784",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-12T14:42:53.073077",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
