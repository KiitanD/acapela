{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4af8bc5a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-12T23:32:45.305358Z",
     "iopub.status.busy": "2023-04-12T23:32:45.304206Z",
     "iopub.status.idle": "2023-04-12T23:34:57.838978Z",
     "shell.execute_reply": "2023-04-12T23:34:57.837614Z"
    },
    "papermill": {
     "duration": 132.541672,
     "end_time": "2023-04-12T23:34:57.841911",
     "exception": false,
     "start_time": "2023-04-12T23:32:45.300239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "build-essential is already the newest version (12.8ubuntu1.1).\r\n",
      "The following additional packages will be installed:\r\n",
      "  libjack0 libportaudio2 libportaudiocpp0 uuid-dev\r\n",
      "Suggested packages:\r\n",
      "  libasound2-doc jackd1 portaudio19-doc\r\n",
      "The following packages will be REMOVED:\r\n",
      "  libjack-jackd2-0\r\n",
      "The following NEW packages will be installed:\r\n",
      "  libasound2-dev libjack-dev libjack0 libportaudio2 libportaudiocpp0\r\n",
      "  portaudio19-dev uuid-dev\r\n",
      "0 upgraded, 7 newly installed, 1 to remove and 76 not upgraded.\r\n",
      "Need to get 625 kB of archives.\r\n",
      "After this operation, 3340 kB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 libjack0 amd64 1:0.125.0-3build2 [93.3 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libasound2-dev amd64 1.2.2-2.1ubuntu2.5 [104 kB]\r\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 uuid-dev amd64 2.34-0.1ubuntu9.3 [33.6 kB]\r\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 libjack-dev amd64 1:0.125.0-3build2 [206 kB]\r\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 libportaudio2 amd64 19.6.0-1build1 [65.4 kB]\r\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/universe amd64 libportaudiocpp0 amd64 19.6.0-1build1 [16.1 kB]\r\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/universe amd64 portaudio19-dev amd64 19.6.0-1build1 [106 kB]\r\n",
      "Fetched 625 kB in 1s (813 kB/s)\r\n",
      "dpkg: libjack-jackd2-0:amd64: dependency problems, but removing anyway as you requested:\r\n",
      " libavdevice58:amd64 depends on libjack-jackd2-0 (>= 1.9.10+20150825) | libjack-0.125; however:\r\n",
      "  Package libjack-jackd2-0:amd64 is to be removed.\r\n",
      "  Package libjack-0.125 is not installed.\r\n",
      "  Package libjack-jackd2-0:amd64 which provides libjack-0.125 is to be removed.\r\n",
      " libavdevice58:amd64 depends on libjack-jackd2-0 (>= 1.9.10+20150825) | libjack-0.125; however:\r\n",
      "  Package libjack-jackd2-0:amd64 is to be removed.\r\n",
      "  Package libjack-0.125 is not installed.\r\n",
      "  Package libjack-jackd2-0:amd64 which provides libjack-0.125 is to be removed.\r\n",
      "\r\n",
      "(Reading database ... 111522 files and directories currently installed.)\r\n",
      "Removing libjack-jackd2-0:amd64 (1.9.12~dfsg-2ubuntu2) ...\r\n",
      "Selecting previously unselected package libjack0:amd64.\r\n",
      "(Reading database ... 111510 files and directories currently installed.)\r\n",
      "Preparing to unpack .../0-libjack0_1%3a0.125.0-3build2_amd64.deb ...\r\n",
      "Unpacking libjack0:amd64 (1:0.125.0-3build2) ...\r\n",
      "Selecting previously unselected package libasound2-dev:amd64.\r\n",
      "Preparing to unpack .../1-libasound2-dev_1.2.2-2.1ubuntu2.5_amd64.deb ...\r\n",
      "Unpacking libasound2-dev:amd64 (1.2.2-2.1ubuntu2.5) ...\r\n",
      "Selecting previously unselected package uuid-dev:amd64.\r\n",
      "Preparing to unpack .../2-uuid-dev_2.34-0.1ubuntu9.3_amd64.deb ...\r\n",
      "Unpacking uuid-dev:amd64 (2.34-0.1ubuntu9.3) ...\r\n",
      "Selecting previously unselected package libjack-dev.\r\n",
      "Preparing to unpack .../3-libjack-dev_1%3a0.125.0-3build2_amd64.deb ...\r\n",
      "Unpacking libjack-dev (1:0.125.0-3build2) ...\r\n",
      "Selecting previously unselected package libportaudio2:amd64.\r\n",
      "Preparing to unpack .../4-libportaudio2_19.6.0-1build1_amd64.deb ...\r\n",
      "Unpacking libportaudio2:amd64 (19.6.0-1build1) ...\r\n",
      "Selecting previously unselected package libportaudiocpp0:amd64.\r\n",
      "Preparing to unpack .../5-libportaudiocpp0_19.6.0-1build1_amd64.deb ...\r\n",
      "Unpacking libportaudiocpp0:amd64 (19.6.0-1build1) ...\r\n",
      "Selecting previously unselected package portaudio19-dev:amd64.\r\n",
      "Preparing to unpack .../6-portaudio19-dev_19.6.0-1build1_amd64.deb ...\r\n",
      "Unpacking portaudio19-dev:amd64 (19.6.0-1build1) ...\r\n",
      "Setting up libjack0:amd64 (1:0.125.0-3build2) ...\r\n",
      "Setting up uuid-dev:amd64 (2.34-0.1ubuntu9.3) ...\r\n",
      "Setting up libjack-dev (1:0.125.0-3build2) ...\r\n",
      "Setting up libasound2-dev:amd64 (1.2.2-2.1ubuntu2.5) ...\r\n",
      "Setting up libportaudio2:amd64 (19.6.0-1build1) ...\r\n",
      "Setting up libportaudiocpp0:amd64 (19.6.0-1build1) ...\r\n",
      "Setting up portaudio19-dev:amd64 (19.6.0-1build1) ...\r\n",
      "Processing triggers for man-db (2.9.1-1) ...\r\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\r\n",
      "Collecting magenta\r\n",
      "  Downloading magenta-2.1.4-py3-none-any.whl (1.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting llvmlite\r\n",
      "  Downloading llvmlite-0.39.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pretty-midi==0.2.9\r\n",
      "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting mir-eval==0.7\r\n",
      "  Downloading mir_eval-0.7.tar.gz (90 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting scipy==1.7.3\r\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting mido==1.2.6\r\n",
      "  Downloading mido-1.2.6-py2.py3-none-any.whl (69 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.8/69.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting numba==0.49.1\r\n",
      "  Downloading numba-0.49.1-cp37-cp37m-manylinux2014_x86_64.whl (3.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow==2.9.1\r\n",
      "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pygtrie==2.5.0\r\n",
      "  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\r\n",
      "Collecting sox==1.4.1\r\n",
      "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\r\n",
      "Collecting Pillow==9.2.0\r\n",
      "  Downloading Pillow-9.2.0-cp37-cp37m-manylinux_2_28_x86_64.whl (3.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting python-rtmidi==1.1.2\r\n",
      "  Downloading python-rtmidi-1.1.2.tar.gz (204 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.5/204.5 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting six==1.16.0\r\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\r\n",
      "Collecting note-seq==0.0.3\r\n",
      "  Downloading note_seq-0.0.3-py3-none-any.whl (210 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.1/210.1 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting sk-video==1.1.10\r\n",
      "  Downloading sk_video-1.1.10-py2.py3-none-any.whl (2.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tf-slim==1.1.0\r\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting absl-py==1.2.0\r\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow-datasets==4.6.0\r\n",
      "  Downloading tensorflow_datasets-4.6.0-py3-none-any.whl (4.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting librosa==0.7.2\r\n",
      "  Downloading librosa-0.7.2.tar.gz (1.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting scikit-image==0.19.3\r\n",
      "  Downloading scikit_image-0.19.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (13.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow-probability==0.17.0\r\n",
      "  Downloading tensorflow_probability-0.17.0-py2.py3-none-any.whl (6.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting imageio==2.20.0\r\n",
      "  Downloading imageio-2.20.0-py3-none-any.whl (3.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting wheel==0.37.1\r\n",
      "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\r\n",
      "Collecting matplotlib==3.5.2\r\n",
      "  Downloading matplotlib-3.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting dm-sonnet==2.0.0\r\n",
      "  Downloading dm_sonnet-2.0.0-py3-none-any.whl (254 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.5/254.5 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting numpy==1.21.6\r\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting dm-tree>=0.1.1\r\n",
      "  Downloading dm_tree-0.1.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.8/153.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting wrapt>=1.11.1\r\n",
      "  Downloading wrapt-1.15.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tabulate>=0.7.5\r\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\r\n",
      "Collecting audioread>=2.0.0\r\n",
      "  Downloading audioread-3.0.0.tar.gz (377 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting scikit-learn!=0.19.0,>=0.14.0\r\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting joblib>=0.12\r\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting decorator>=3.0.0\r\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\r\n",
      "Collecting resampy>=0.2.2\r\n",
      "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting soundfile>=0.9.0\r\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting packaging>=20.0\r\n",
      "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting cycler>=0.10\r\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\r\n",
      "Collecting kiwisolver>=1.0.1\r\n",
      "  Downloading kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pyparsing>=2.2.1\r\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\r\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting python-dateutil>=2.7\r\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting future\r\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting protobuf>=3.6.1\r\n",
      "  Downloading protobuf-4.22.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pydub\r\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\r\n",
      "Collecting attrs\r\n",
      "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pandas>=0.18.1\r\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting bokeh>=0.12.0\r\n",
      "  Downloading bokeh-2.4.3-py3-none-any.whl (18.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting IPython\r\n",
      "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m793.8/793.8 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting intervaltree>=2.1.0\r\n",
      "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting setuptools\r\n",
      "  Downloading setuptools-67.6.1-py3-none-any.whl (1.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting llvmlite\r\n",
      "  Downloading llvmlite-0.32.1-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tifffile>=2019.7.26\r\n",
      "  Downloading tifffile-2021.11.2-py3-none-any.whl (178 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting PyWavelets>=1.1.1\r\n",
      "  Downloading PyWavelets-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting networkx>=2.2\r\n",
      "  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting keras<2.10.0,>=2.9.0rc0\r\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting h5py>=2.9.0\r\n",
      "  Downloading h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.1\r\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\r\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting libclang>=13.0.0\r\n",
      "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\r\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting typing-extensions>=3.6.6\r\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\r\n",
      "Collecting termcolor>=1.1.0\r\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\r\n",
      "Collecting grpcio<2.0,>=1.24.3\r\n",
      "  Downloading grpcio-1.53.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorboard<2.10,>=2.9\r\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting protobuf>=3.6.1\r\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting flatbuffers<2,>=1.12\r\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\r\n",
      "Collecting gast<=0.4.0,>=0.2.1\r\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\r\n",
      "Collecting google-pasta>=0.1.1\r\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\r\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\r\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting toml\r\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\r\n",
      "Collecting promise\r\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting etils[epath]\r\n",
      "  Downloading etils-0.9.0-py3-none-any.whl (140 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting dill\r\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tqdm\r\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting importlib-resources\r\n",
      "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\r\n",
      "Collecting tensorflow-metadata\r\n",
      "  Downloading tensorflow_metadata-1.12.0-py3-none-any.whl (52 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting requests>=2.19.0\r\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting cloudpickle>=1.3\r\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\r\n",
      "Collecting PyYAML>=3.10\r\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting Jinja2>=2.9\r\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tornado>=5.1\r\n",
      "  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.0/424.0 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting sortedcontainers<3.0,>=2.0\r\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\r\n",
      "Collecting pytz>=2017.3\r\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\r\n",
      "  Downloading charset_normalizer-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (171 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.0/171.0 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting idna<4,>=2.5\r\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\r\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\r\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting resampy>=0.2.2\r\n",
      "  Downloading resampy-0.4.1-py3-none-any.whl (3.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading resampy-0.4.0-py3-none-any.whl (3.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading resampy-0.3.1-py3-none-any.whl (3.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\r\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\r\n",
      "Collecting cffi>=1.0\r\n",
      "  Downloading cffi-1.15.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.9/427.9 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\r\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting markdown>=2.6.8\r\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\r\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\r\n",
      "Collecting google-auth<3,>=1.6.3\r\n",
      "  Downloading google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.2/178.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\r\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\r\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting zipp\r\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\r\n",
      "Collecting traitlets>=4.2\r\n",
      "  Downloading traitlets-5.9.0-py3-none-any.whl (117 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting jedi>=0.16\r\n",
      "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting matplotlib-inline\r\n",
      "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\r\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\r\n",
      "  Downloading prompt_toolkit-3.0.38-py3-none-any.whl (385 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pickleshare\r\n",
      "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\r\n",
      "Collecting backcall\r\n",
      "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\r\n",
      "Collecting pexpect>4.3\r\n",
      "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pygments\r\n",
      "  Downloading Pygments-2.15.0-py3-none-any.whl (1.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting googleapis-common-protos<2,>=1.52.0\r\n",
      "  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pycparser\r\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\r\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\r\n",
      "Collecting cachetools<6.0,>=2.0.0\r\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\r\n",
      "Collecting pyasn1-modules>=0.2.1\r\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\r\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\r\n",
      "Collecting parso<0.9.0,>=0.8.0\r\n",
      "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\r\n",
      "  Downloading MarkupSafe-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\r\n",
      "Collecting importlib-metadata>=4.4\r\n",
      "  Downloading importlib_metadata-6.3.0-py3-none-any.whl (22 kB)\r\n",
      "Collecting ptyprocess>=0.5\r\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\r\n",
      "Collecting wcwidth\r\n",
      "  Downloading wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\r\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\r\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\r\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: librosa, mir-eval, pretty-midi, python-rtmidi, audioread, intervaltree, future, promise\r\n",
      "  Building wheel for librosa (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for librosa: filename=librosa-0.7.2-py3-none-any.whl size=1612903 sha256=46b0d763f3f66ded4914337570871164ccbbcb37090d02a8b67a8b92022c1547\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/b2/93/fb/ae0f9f40f5c4aaf1c8d73f2194581a8f249c7169670acfdf6d\r\n",
      "  Building wheel for mir-eval (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for mir-eval: filename=mir_eval-0.7-py3-none-any.whl size=100720 sha256=aa648b573be8f730e87432c99000eae607b7ea2186ecb5cab1d22754bca54b26\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/a5/db/bc282c9ac9ec68db9b0a1c1bb3184d91f83f8897d51b6db511\r\n",
      "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591954 sha256=a077a5f88416f12535274ffeb7cb688a4658b953da50c6a1627bcbb685713aad\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/57/18/93/2909d84ca856ef11cfb220d650eecb99a4723ce139bb553464\r\n",
      "  Building wheel for python-rtmidi (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for python-rtmidi: filename=python_rtmidi-1.1.2-cp37-cp37m-linux_x86_64.whl size=592287 sha256=aef2b747e70c4803af8e63da63e83c3355397afba6b6e71eee00ece1da11628a\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/ac/7c/bf918313c78466861766f5dc370c1ae591fca433531f8c5e09\r\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23706 sha256=d23ebafe2c7567a0abf1451fc443569117b80e17b3e369632999fd26aad7352c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/dc/38/51/1be02cf6dbd3ef3e2e50a562071c9d574170c4f5096c09d8e1\r\n",
      "  Building wheel for intervaltree (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26118 sha256=a2d3e47daa6b6263f9433c9c5bd6082902317a0655979154479075ecda8a3f5a\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/94/38/6fe5b0e582dca953d1ef8f2e8714fa1def8e5bf514c25c11fb\r\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492036 sha256=49fc40f66a1d522cb2228ab98d27192f99755fab6bf9807f72c9405c7f615627\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/52/2a/fc/520209cfa6448febd490720a0b09036cb367628f7c4e9cc172\r\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=e28118cd7db0fef341a08f044273919d556111a2858c11e82c22bcc0d15cf43a\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/9d/ad/15/e6d5c43a0f01b88ee5883bd249a18e09d72821e43b1c3e8187\r\n",
      "Successfully built librosa mir-eval pretty-midi python-rtmidi audioread intervaltree future promise\r\n",
      "Installing collected packages: wcwidth, tensorboard-plugin-wit, sortedcontainers, pytz, python-rtmidi, pygtrie, pydub, pyasn1, ptyprocess, pickleshare, mido, llvmlite, libclang, keras, flatbuffers, dm-tree, backcall, zipp, wrapt, wheel, urllib3, typing-extensions, traitlets, tqdm, tornado, toml, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, tabulate, six, setuptools, rsa, PyYAML, pyparsing, pygments, pycparser, pyasn1-modules, protobuf, prompt-toolkit, Pillow, pexpect, parso, packaging, oauthlib, numpy, networkx, MarkupSafe, joblib, intervaltree, idna, grpcio, gast, future, fonttools, etils, dill, decorator, cycler, cloudpickle, charset-normalizer, certifi, cachetools, audioread, attrs, absl-py, werkzeug, tifffile, tf-slim, tensorflow-probability, sox, scipy, requests, PyWavelets, python-dateutil, promise, pretty-midi, opt-einsum, numba, matplotlib-inline, kiwisolver, keras-preprocessing, Jinja2, jedi, importlib-resources, importlib-metadata, imageio, h5py, googleapis-common-protos, google-pasta, google-auth, dm-sonnet, cffi, astunparse, tensorflow-metadata, soundfile, sk-video, scikit-learn, scikit-image, resampy, requests-oauthlib, pandas, mir-eval, matplotlib, markdown, IPython, bokeh, tensorflow-datasets, librosa, google-auth-oauthlib, tensorboard, note-seq, tensorflow, magenta\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "dask-cudf 21.12.2 requires cupy-cuda115, which is not installed.\r\n",
      "cudf 21.12.2 requires cupy-cuda115, which is not installed.\r\n",
      "wfdb 4.1.0 requires SoundFile<0.12.0,>=0.10.0, but you have soundfile 0.12.1 which is incompatible.\r\n",
      "wasabi 1.1.1 requires typing-extensions<4.5.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\r\n",
      "thinc 8.1.9 requires typing-extensions<4.5.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\r\n",
      "tfx-bsl 1.12.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.83.0 which is incompatible.\r\n",
      "tfx-bsl 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\r\n",
      "tfx-bsl 1.12.0 requires tensorflow<3,>=2.11, but you have tensorflow 2.9.1 which is incompatible.\r\n",
      "tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\r\n",
      "tensorflow-transform 1.12.0 requires tensorflow<2.12,>=2.11.0, but you have tensorflow 2.9.1 which is incompatible.\r\n",
      "tensorflow-text 2.11.0 requires tensorflow<2.12,>=2.11.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.9.1 which is incompatible.\r\n",
      "tensorflow-serving-api 2.11.0 requires tensorflow<3,>=2.11.0, but you have tensorflow 2.9.1 which is incompatible.\r\n",
      "tensorflow-io 0.29.0 requires tensorflow-io-gcs-filesystem==0.29.0, but you have tensorflow-io-gcs-filesystem 0.32.0 which is incompatible.\r\n",
      "tensorflow-decision-forests 1.2.0 requires tensorflow~=2.11.0, but you have tensorflow 2.9.1 which is incompatible.\r\n",
      "stumpy 1.11.1 requires numba>=0.54, but you have numba 0.49.1 which is incompatible.\r\n",
      "spacy 3.5.1 requires typing-extensions<4.5.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\r\n",
      "pynndescent 0.5.8 requires numba>=0.51.2, but you have numba 0.49.1 which is incompatible.\r\n",
      "pydocstyle 6.3.0 requires importlib-metadata<5.0.0,>=2.0.0; python_version < \"3.8\", but you have importlib-metadata 6.3.0 which is incompatible.\r\n",
      "pandas-profiling 3.6.2 requires tqdm<4.65,>=4.48.2, but you have tqdm 4.65.0 which is incompatible.\r\n",
      "onnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "ibis-framework 2.1.1 requires importlib-metadata<5,>=4; python_version < \"3.8\", but you have importlib-metadata 6.3.0 which is incompatible.\r\n",
      "google-cloud-core 1.7.3 requires google-auth<2.0dev,>=1.24.0, but you have google-auth 2.17.3 which is incompatible.\r\n",
      "flake8 5.0.4 requires importlib-metadata<4.3,>=1.1.0; python_version < \"3.8\", but you have importlib-metadata 6.3.0 which is incompatible.\r\n",
      "distributed 2021.11.2 requires dask==2021.11.2, but you have dask 2022.2.0 which is incompatible.\r\n",
      "datashader 0.14.4 requires numba>=0.51, but you have numba 0.49.1 which is incompatible.\r\n",
      "dask-cudf 21.12.2 requires dask<=2021.11.2,>=2021.11.1, but you have dask 2022.2.0 which is incompatible.\r\n",
      "cudf 21.12.2 requires numba>=0.53.1, but you have numba 0.49.1 which is incompatible.\r\n",
      "confection 0.0.4 requires typing-extensions<4.5.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\r\n",
      "cmudict 1.0.13 requires importlib-metadata<6.0.0,>=5.1.0, but you have importlib-metadata 6.3.0 which is incompatible.\r\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 2.83.0 which is incompatible.\r\n",
      "boto3 1.26.100 requires botocore<1.30.0,>=1.29.100, but you have botocore 1.27.59 which is incompatible.\r\n",
      "apache-beam 2.44.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\r\n",
      "aiohttp 3.8.3 requires charset-normalizer<3.0,>=2.0, but you have charset-normalizer 3.1.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed IPython-7.34.0 Jinja2-3.1.2 MarkupSafe-2.1.2 Pillow-9.4.0 PyWavelets-1.3.0 PyYAML-6.0 absl-py-1.4.0 astunparse-1.6.3 attrs-22.2.0 audioread-3.0.0 backcall-0.2.0 bokeh-2.4.3 cachetools-5.3.0 certifi-2022.12.7 cffi-1.15.1 charset-normalizer-3.1.0 cloudpickle-2.2.1 cycler-0.11.0 decorator-5.1.1 dill-0.3.6 dm-sonnet-2.0.0 dm-tree-0.1.8 etils-0.9.0 flatbuffers-23.1.21 fonttools-4.38.0 future-0.18.3 gast-0.4.0 google-auth-2.17.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 googleapis-common-protos-1.59.0 grpcio-1.53.0 h5py-3.8.0 idna-3.4 imageio-2.25.0 importlib-metadata-6.3.0 importlib-resources-5.12.0 intervaltree-3.1.0 jedi-0.18.2 joblib-1.2.0 keras-2.11.0 keras-preprocessing-1.1.2 kiwisolver-1.4.4 libclang-16.0.0 librosa-0.10.0.post2 llvmlite-0.39.1 magenta-2.1.4 markdown-3.4.3 matplotlib-3.5.3 matplotlib-inline-0.1.6 mido-1.2.6 mir-eval-0.7 networkx-2.6.3 note-seq-0.0.3 numba-0.56.4 numpy-1.21.6 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-23.1 pandas-1.3.5 parso-0.8.3 pexpect-4.8.0 pickleshare-0.7.5 pretty-midi-0.2.9 promise-2.3 prompt-toolkit-3.0.38 protobuf-3.20.3 ptyprocess-0.7.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.21 pydub-0.25.1 pygments-2.15.0 pygtrie-2.5.0 pyparsing-3.0.9 python-dateutil-2.8.2 python-rtmidi-1.1.2 pytz-2023.3 requests-2.28.2 requests-oauthlib-1.3.1 resampy-0.3.1 rsa-4.9 scikit-image-0.19.3 scikit-learn-1.0.2 scipy-1.7.3 setuptools-67.6.1 six-1.16.0 sk-video-1.1.10 sortedcontainers-2.4.0 soundfile-0.12.1 sox-1.4.1 tabulate-0.9.0 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-datasets-4.8.2 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.32.0 tensorflow-metadata-1.12.0 tensorflow-probability-0.19.0 termcolor-2.2.0 tf-slim-1.1.0 threadpoolctl-3.1.0 tifffile-2021.11.2 toml-0.10.2 tornado-6.2 tqdm-4.65.0 traitlets-5.9.0 typing-extensions-4.5.0 urllib3-1.26.15 wcwidth-0.2.6 werkzeug-2.2.3 wheel-0.38.4 wrapt-1.15.0 zipp-3.15.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install -y build-essential libasound2-dev libjack-dev portaudio19-dev\n",
    "!pip install magenta --ignore-installed llvmlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32d1cf74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T23:34:57.908809Z",
     "iopub.status.busy": "2023-04-12T23:34:57.908426Z",
     "iopub.status.idle": "2023-04-12T23:37:14.289216Z",
     "shell.execute_reply": "2023-04-12T23:37:14.287905Z"
    },
    "papermill": {
     "duration": 136.417457,
     "end_time": "2023-04-12T23:37:14.291957",
     "exception": false,
     "start_time": "2023-04-12T23:34:57.874500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "9961\n"
     ]
    }
   ],
   "source": [
    "import magenta\n",
    "from magenta.scripts.convert_dir_to_note_sequences import convert_midi\n",
    "from magenta.music.sequences_lib import split_note_sequence\n",
    "import tensorflow.compat.v1 as tf\n",
    "import os\n",
    "\n",
    "prepath = \"/kaggle/input/rockmidi0-1499/Rock0-1499\"\n",
    "all_ns = []\n",
    "count = 0\n",
    "for file in os.listdir(prepath):\n",
    "    try:\n",
    "        ns = convert_midi(prepath, \"midi\", os.path.join(prepath, file))\n",
    "\n",
    "    except: \n",
    "        print(\"Error on:\", file)\n",
    "        count += 1\n",
    "        continue\n",
    "    \n",
    "    ns_split = split_note_sequence(ns,\n",
    "                            20,\n",
    "                            False)\n",
    "#     print(len(ns_split))\n",
    "    count += 1\n",
    "    all_ns += ns_split\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "\n",
    "print(len(all_ns))\n",
    "!mkdir /kaggle/tmp/\n",
    "if (os.path.exists(\"/kaggle/tmp/rock.tfrecord\") == False):\n",
    "    f = open(\"/kaggle/tmp/rock.tfrecord\", \"w\")\n",
    "with tf.io.TFRecordWriter(\"/kaggle/tmp/rock.tfrecord\") as writer:\n",
    "    for sequence in all_ns:\n",
    "        writer.write(sequence.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c17b17c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T23:37:14.355041Z",
     "iopub.status.busy": "2023-04-12T23:37:14.354088Z",
     "iopub.status.idle": "2023-04-12T23:37:14.408713Z",
     "shell.execute_reply": "2023-04-12T23:37:14.407759Z"
    },
    "papermill": {
     "duration": 0.088638,
     "end_time": "2023-04-12T23:37:14.411205",
     "exception": false,
     "start_time": "2023-04-12T23:37:14.322567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from note_seq.protobuf import music_pb2\n",
    "from magenta.pipelines import pipeline, note_sequence_pipelines, statistics\n",
    "from magenta.models import polyphony_rnn\n",
    "from magenta.models.polyphony_rnn import polyphony_lib, polyphony_model, polyphony_rnn_pipeline\n",
    "from magenta.pipelines import dag_pipeline\n",
    "from magenta.pipelines import event_sequence_pipeline\n",
    "from magenta.pipelines import pipelines_common\n",
    "\n",
    "\n",
    "class PolyphonicSequenceExtractor(pipeline.Pipeline):\n",
    "  \"\"\"Extracts polyphonic tracks from a quantized NoteSequence.\"\"\"\n",
    "\n",
    "  def __init__(self, min_steps, max_steps, name=None):\n",
    "    super(PolyphonicSequenceExtractor, self).__init__(\n",
    "        input_type=music_pb2.NoteSequence,\n",
    "        output_type=polyphony_lib.PolyphonicSequence,\n",
    "        name=name)\n",
    "    self._min_steps = min_steps\n",
    "    self._max_steps = max_steps\n",
    "\n",
    "  def transform(self, input_object):\n",
    "    quantized_sequence = input_object\n",
    "    poly_seqs, stats = polyphony_lib.extract_polyphonic_sequences(\n",
    "        quantized_sequence,\n",
    "        min_steps_discard=self._min_steps,\n",
    "        max_steps_discard=self._max_steps)\n",
    "    self._set_stats(stats)\n",
    "    return poly_seqs\n",
    "\n",
    "# changed from the library version: remove transposition from the pipeline\n",
    "def get_pipeline_mine(config, min_steps, max_steps, eval_ratio):\n",
    "  \"\"\"Returns the Pipeline instance which creates the RNN dataset.\n",
    "  Args:\n",
    "    config: An EventSequenceRnnConfig.\n",
    "    min_steps: Minimum number of steps for an extracted sequence.\n",
    "    max_steps: Maximum number of steps for an extracted sequence.\n",
    "    eval_ratio: Fraction of input to set aside for evaluation set.\n",
    "  Returns:\n",
    "    A pipeline.Pipeline instance.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  partitioner = pipelines_common.RandomPartition(\n",
    "      music_pb2.NoteSequence,\n",
    "      ['eval_poly_tracks', 'training_poly_tracks'],\n",
    "      [eval_ratio])\n",
    "  dag = {partitioner: dag_pipeline.DagInput(music_pb2.NoteSequence)}\n",
    "\n",
    "  for mode in ['eval', 'training']:\n",
    "    time_change_splitter = note_sequence_pipelines.TimeChangeSplitter(\n",
    "        name='TimeChangeSplitter_' + mode)\n",
    "    quantizer = note_sequence_pipelines.Quantizer(\n",
    "        steps_per_quarter=config.steps_per_quarter, name='Quantizer_' + mode)\n",
    "    poly_extractor = PolyphonicSequenceExtractor(\n",
    "        min_steps=min_steps, max_steps=max_steps, name='PolyExtractor_' + mode)\n",
    "    encoder_pipeline = event_sequence_pipeline.EncoderPipeline(\n",
    "        polyphony_lib.PolyphonicSequence, config.encoder_decoder,\n",
    "        name='EncoderPipeline_' + mode)\n",
    "\n",
    "    dag[time_change_splitter] = partitioner[mode + '_poly_tracks']\n",
    "    dag[quantizer] = time_change_splitter\n",
    "    dag[poly_extractor] = quantizer\n",
    "    dag[encoder_pipeline] = poly_extractor\n",
    "    dag[dag_pipeline.DagOutput(mode + '_poly_tracks')] = encoder_pipeline\n",
    "\n",
    "  return dag_pipeline.DAGPipeline(dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1a82f90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T23:37:14.472841Z",
     "iopub.status.busy": "2023-04-12T23:37:14.472548Z",
     "iopub.status.idle": "2023-04-12T23:43:36.706736Z",
     "shell.execute_reply": "2023-04-12T23:43:36.705522Z"
    },
    "papermill": {
     "duration": 382.268425,
     "end_time": "2023-04-12T23:43:36.709429",
     "exception": false,
     "start_time": "2023-04-12T23:37:14.441004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_instance = get_pipeline_mine(\n",
    "      min_steps=80,  # 5 measures\n",
    "      max_steps=None,\n",
    "      eval_ratio=0.2,\n",
    "      config=polyphony_model.default_configs['polyphony'])\n",
    "\n",
    "input_dir = '/kaggle/tmp/rock.tfrecord'\n",
    "output_dir = '/kaggle/tmp/split'\n",
    "pipeline.run_pipeline_serial(\n",
    "  pipeline_instance,\n",
    "  pipeline.tf_record_iterator(input_dir, pipeline_instance.input_type),\n",
    "  output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de3f5e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T23:43:36.773123Z",
     "iopub.status.busy": "2023-04-12T23:43:36.772197Z",
     "iopub.status.idle": "2023-04-12T23:43:39.394014Z",
     "shell.execute_reply": "2023-04-12T23:43:39.392448Z"
    },
    "papermill": {
     "duration": 2.655748,
     "end_time": "2023-04-12T23:43:39.396456",
     "exception": false,
     "start_time": "2023-04-12T23:43:36.740708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1869 7598\n"
     ]
    }
   ],
   "source": [
    "print(sum(1 for _ in tf.python_io.tf_record_iterator('/kaggle/tmp/split/eval_poly_tracks.tfrecord')),\n",
    "sum(1 for _ in tf.python_io.tf_record_iterator('/kaggle/tmp/split/training_poly_tracks.tfrecord')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c34c89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T23:43:39.461611Z",
     "iopub.status.busy": "2023-04-12T23:43:39.460022Z",
     "iopub.status.idle": "2023-04-13T00:16:09.502737Z",
     "shell.execute_reply": "2023-04-13T00:16:09.501328Z"
    },
    "papermill": {
     "duration": 1950.07766,
     "end_time": "2023-04-13T00:16:09.505630",
     "exception": false,
     "start_time": "2023-04-12T23:43:39.427970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 23:43:58.835704 135283875542848 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 23:43:58.836251 135283875542848 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-001/train\r\n",
      "W0412 23:43:58.862044 135283875542848 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 23:43:58.885188 135283875542848 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 23:43:58.889744 135283875542848 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0412 23:43:58.891290 135283875542848 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 23:43:58.892217 135283875542848 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 23:43:58.895819 135283875542848 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0412 23:43:58.913949 135283875542848 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0412 23:43:58.914109 135283875542848 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0412 23:43:58.965107 135283875542848 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0412 23:43:58.968301 135283875542848 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0412 23:43:58.968640 135283875542848 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0412 23:43:58.994103 135283875542848 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0412 23:43:59.058778 135283875542848 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0412 23:43:59.131342 135283875542848 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0412 23:43:59.282025 135283875542848 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0412 23:43:59.285048 135283875542848 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0412 23:44:00.101431 135283875542848 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0412 23:44:00.101702 135283875542848 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0412 23:44:00.216868 135283875542848 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0412 23:44:00.302735 135283875542848 monitored_session.py:243] Graph was finalized.\r\n",
      "I0412 23:44:03.268019 135283875542848 session_manager.py:527] Running local_init_op.\r\n",
      "I0412 23:44:03.276723 135283875542848 session_manager.py:530] Done running local_init_op.\r\n",
      "W0412 23:44:03.306936 135283875542848 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0412 23:44:04.034421 135283875542848 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0412 23:44:04.034949 135283875542848 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 23:44:04.288464 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-0.meta\r\n",
      "I0412 23:44:04.288722 135283875542848 saver.py:93] 400\r\n",
      "I0412 23:44:04.288896 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0412 23:44:04.289068 135283875542848 saver.py:93] 20100\r\n",
      "I0412 23:44:04.289222 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-0.index\r\n",
      "I0412 23:44:04.289386 135283875542848 saver.py:93] 20100\r\n",
      "I0412 23:44:04.289897 135283875542848 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0412 23:44:10.306037 135283875542848 basic_session_run_hooks.py:266] Accuracy = 0.002643906, Global Step = 0, Loss = 5.5597734, Perplexity = 259.76398\r\n",
      "I0412 23:44:27.739670 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.31121352, Global Step = 10, Loss = 4.0689874, Perplexity = 58.497696 (17.434 sec)\r\n",
      "I0412 23:44:27.740867 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.573597\r\n",
      "I0412 23:44:44.592851 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.32825655, Global Step = 20, Loss = 3.7358606, Perplexity = 41.924088 (16.853 sec)\r\n",
      "I0412 23:44:44.593911 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.593364\r\n",
      "I0412 23:45:02.427775 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.28911713, Global Step = 30, Loss = 3.8631072, Perplexity = 47.613068 (17.835 sec)\r\n",
      "I0412 23:45:02.429007 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.560693\r\n",
      "I0412 23:45:05.761985 135283875542848 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 33...\r\n",
      "I0412 23:45:05.762241 135283875542848 basic_session_run_hooks.py:633] Saving checkpoints for 33 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 23:45:05.878869 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-33.meta\r\n",
      "I0412 23:45:05.879088 135283875542848 saver.py:93] 400\r\n",
      "I0412 23:45:05.879208 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-33.index\r\n",
      "I0412 23:45:05.879325 135283875542848 saver.py:93] 400\r\n",
      "I0412 23:45:05.879443 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-33.data-00000-of-00001\r\n",
      "I0412 23:45:05.879539 135283875542848 saver.py:93] 20100\r\n",
      "I0412 23:45:05.879729 135283875542848 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 33...\r\n",
      "I0412 23:45:19.130855 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.3032476, Global Step = 40, Loss = 3.8567305, Perplexity = 47.310413 (16.703 sec)\r\n",
      "I0412 23:45:19.131912 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.598697\r\n",
      "I0412 23:45:36.620275 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.3000973, Global Step = 50, Loss = 3.8115947, Perplexity = 45.222496 (17.489 sec)\r\n",
      "I0412 23:45:36.621698 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.571763\r\n",
      "I0412 23:45:54.402137 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.3089367, Global Step = 60, Loss = 3.7399197, Perplexity = 42.09461 (17.782 sec)\r\n",
      "I0412 23:45:54.403249 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.562383\r\n",
      "I0412 23:46:06.339457 135283875542848 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 67...\r\n",
      "I0412 23:46:06.339734 135283875542848 basic_session_run_hooks.py:633] Saving checkpoints for 67 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 23:46:06.443994 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-67.data-00000-of-00001\r\n",
      "I0412 23:46:06.444225 135283875542848 saver.py:93] 19700\r\n",
      "I0412 23:46:06.444382 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-67.index\r\n",
      "I0412 23:46:06.444500 135283875542848 saver.py:93] 19700\r\n",
      "I0412 23:46:06.444594 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-67.meta\r\n",
      "I0412 23:46:06.444690 135283875542848 saver.py:93] 20100\r\n",
      "I0412 23:46:06.444870 135283875542848 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 67...\r\n",
      "I0412 23:46:13.363527 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.33551916, Global Step = 70, Loss = 3.6732755, Perplexity = 39.380688 (18.961 sec)\r\n",
      "I0412 23:46:13.364540 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.527388\r\n",
      "I0412 23:46:31.532010 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.29930457, Global Step = 80, Loss = 3.7866194, Perplexity = 44.10704 (18.168 sec)\r\n",
      "I0412 23:46:31.533092 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.550403\r\n",
      "I0412 23:46:48.598154 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.2893715, Global Step = 90, Loss = 3.8040845, Perplexity = 44.88414 (17.066 sec)\r\n",
      "I0412 23:46:48.599283 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.585952\r\n",
      "I0412 23:47:06.962727 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.566068\r\n",
      "I0412 23:47:06.964226 135283875542848 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 101...\r\n",
      "I0412 23:47:06.964412 135283875542848 basic_session_run_hooks.py:633] Saving checkpoints for 101 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 23:47:07.094428 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-101.data-00000-of-00001\r\n",
      "I0412 23:47:07.094656 135283875542848 saver.py:93] 19700\r\n",
      "I0412 23:47:07.094769 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-101.meta\r\n",
      "I0412 23:47:07.094867 135283875542848 saver.py:93] 20100\r\n",
      "I0412 23:47:07.094969 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-101.index\r\n",
      "I0412 23:47:07.095059 135283875542848 saver.py:93] 20100\r\n",
      "I0412 23:47:07.095242 135283875542848 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 101...\r\n",
      "I0412 23:47:07.095645 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.30200508, Global Step = 100, Loss = 3.792827, Perplexity = 44.381683 (18.498 sec)\r\n",
      "I0412 23:47:07.096581 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.540619\r\n",
      "I0412 23:47:23.883183 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.31059286, Global Step = 110, Loss = 3.683102, Perplexity = 39.76956 (16.788 sec)\r\n",
      "I0412 23:47:23.884251 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.595675\r\n",
      "I0412 23:47:42.064903 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.31114987, Global Step = 120, Loss = 3.7622058, Perplexity = 43.04327 (18.182 sec)\r\n",
      "I0412 23:47:42.066054 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.550001\r\n",
      "I0412 23:47:58.732895 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.29533207, Global Step = 130, Loss = 3.7619374, Perplexity = 43.031715 (16.668 sec)\r\n",
      "I0412 23:47:58.733915 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.599958\r\n",
      "I0412 23:48:07.595536 135283875542848 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 136...\r\n",
      "I0412 23:48:07.595792 135283875542848 basic_session_run_hooks.py:633] Saving checkpoints for 136 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 23:48:07.702701 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-136.index\r\n",
      "I0412 23:48:07.702909 135283875542848 saver.py:93] 0\r\n",
      "I0412 23:48:07.703024 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-136.meta\r\n",
      "I0412 23:48:07.703116 135283875542848 saver.py:93] 400\r\n",
      "I0412 23:48:07.703213 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-136.data-00000-of-00001\r\n",
      "I0412 23:48:07.703303 135283875542848 saver.py:93] 20100\r\n",
      "I0412 23:48:07.703514 135283875542848 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 136...\r\n",
      "I0412 23:48:15.543141 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.29992238, Global Step = 140, Loss = 3.7379897, Perplexity = 42.013443 (16.810 sec)\r\n",
      "I0412 23:48:15.544167 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.594875\r\n",
      "I0412 23:48:32.573141 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.30071163, Global Step = 150, Loss = 3.7330167, Perplexity = 41.80503 (17.030 sec)\r\n",
      "I0412 23:48:32.574253 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.587196\r\n",
      "I0412 23:48:48.893851 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.30457836, Global Step = 160, Loss = 3.7392442, Perplexity = 42.066185 (16.321 sec)\r\n",
      "I0412 23:48:48.894995 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.612718\r\n",
      "I0412 23:49:06.508853 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.31287768, Global Step = 170, Loss = 3.7894232, Perplexity = 44.23088 (17.615 sec)\r\n",
      "I0412 23:49:06.509890 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.567701\r\n",
      "I0412 23:49:08.056109 135283875542848 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 172...\r\n",
      "I0412 23:49:08.056371 135283875542848 basic_session_run_hooks.py:633] Saving checkpoints for 172 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 23:49:08.164926 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-172.data-00000-of-00001\r\n",
      "I0412 23:49:08.165145 135283875542848 saver.py:93] 19700\r\n",
      "I0412 23:49:08.165254 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-172.meta\r\n",
      "I0412 23:49:08.165364 135283875542848 saver.py:93] 20100\r\n",
      "I0412 23:49:08.165477 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-172.index\r\n",
      "I0412 23:49:08.165574 135283875542848 saver.py:93] 20100\r\n",
      "I0412 23:49:08.165756 135283875542848 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 172...\r\n",
      "I0412 23:49:23.823807 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.28671446, Global Step = 180, Loss = 3.7553024, Perplexity = 42.747143 (17.315 sec)\r\n",
      "I0412 23:49:23.824982 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.57753\r\n",
      "I0412 23:49:42.406820 135283875542848 basic_session_run_hooks.py:264] Accuracy = 0.29200503, Global Step = 190, Loss = 3.7401118, Perplexity = 42.1027 (18.583 sec)\r\n",
      "I0412 23:49:42.407914 135283875542848 basic_session_run_hooks.py:717] global_step/sec: 0.538128\r\n",
      "I0412 23:49:57.309859 135283875542848 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 200...\r\n",
      "I0412 23:49:57.310100 135283875542848 basic_session_run_hooks.py:633] Saving checkpoints for 200 into /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt.\r\n",
      "I0412 23:49:57.441031 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-200.index\r\n",
      "I0412 23:49:57.441244 135283875542848 saver.py:93] 0\r\n",
      "I0412 23:49:57.441388 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-200.meta\r\n",
      "I0412 23:49:57.441667 135283875542848 saver.py:93] 400\r\n",
      "I0412 23:49:57.441787 135283875542848 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-001/train/model.ckpt-200.data-00000-of-00001\r\n",
      "I0412 23:49:57.441887 135283875542848 saver.py:93] 20100\r\n",
      "I0412 23:49:57.442072 135283875542848 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 200...\r\n",
      "I0412 23:49:57.491018 135283875542848 events_rnn_train.py:113] Training complete.\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 23:50:16.560903 133318565951296 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 23:50:16.561536 133318565951296 polyphony_rnn_train.py:94] Train dir: /kaggle/tmp/polyphony_rnn/logdir/run-001/train\r\n",
      "I0412 23:50:16.562016 133318565951296 polyphony_rnn_train.py:99] Eval dir: /kaggle/tmp/polyphony_rnn/logdir/run-001/eval\r\n",
      "W0412 23:50:16.562707 133318565951296 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/polyphony_rnn_train\", line 8, in <module>\r\n",
      "    sys.exit(console_entry_point())\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 114, in console_entry_point\r\n",
      "    tf.app.run(main)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 36, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 308, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 254, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 104, in main\r\n",
      "    events_rnn_train.run_eval(build_graph_fn, train_dir, eval_dir, num_batches)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_train.py\", line 138, in run_eval\r\n",
      "    build_graph_fn()\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py\", line 113, in build\r\n",
      "    label_shape=label_shape, shuffle=mode == 'train')\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py\", line 66, in get_padded_batch\r\n",
      "    file_queue = tf.train.string_input_producer(file_list)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 357, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py\", line 253, in string_input_producer\r\n",
      "    raise ValueError(not_null_err)\r\n",
      "ValueError: string_input_producer requires a non-null input tensor\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 23:50:33.990936 127805528086336 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.005, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 23:50:33.991632 127805528086336 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-005/train\r\n",
      "W0412 23:50:33.992833 127805528086336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 23:50:34.002731 127805528086336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 23:50:34.004772 127805528086336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0412 23:50:34.006324 127805528086336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 23:50:34.007260 127805528086336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 23:50:34.010893 127805528086336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0412 23:50:34.018188 127805528086336 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0412 23:50:34.018343 127805528086336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0412 23:50:34.059951 127805528086336 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0412 23:50:34.064045 127805528086336 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0412 23:50:34.064398 127805528086336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0412 23:50:34.115380 127805528086336 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0412 23:50:34.153127 127805528086336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0412 23:50:34.218917 127805528086336 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0412 23:50:34.366583 127805528086336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0412 23:50:34.369602 127805528086336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0412 23:50:35.170302 127805528086336 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0412 23:50:35.170600 127805528086336 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0412 23:50:35.280616 127805528086336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0412 23:50:35.351132 127805528086336 monitored_session.py:243] Graph was finalized.\r\n",
      "I0412 23:50:36.632938 127805528086336 session_manager.py:527] Running local_init_op.\r\n",
      "I0412 23:50:36.640077 127805528086336 session_manager.py:530] Done running local_init_op.\r\n",
      "W0412 23:50:36.667069 127805528086336 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0412 23:50:37.345079 127805528086336 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0412 23:50:37.346373 127805528086336 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 23:50:37.545176 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-0.meta\r\n",
      "I0412 23:50:37.547516 127805528086336 saver.py:93] 400\r\n",
      "I0412 23:50:37.548375 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0412 23:50:37.548647 127805528086336 saver.py:93] 20100\r\n",
      "I0412 23:50:37.548867 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-0.index\r\n",
      "I0412 23:50:37.549058 127805528086336 saver.py:93] 20100\r\n",
      "I0412 23:50:37.549437 127805528086336 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0412 23:50:41.831275 127805528086336 basic_session_run_hooks.py:266] Accuracy = 0.0033347611, Global Step = 0, Loss = 5.556982, Perplexity = 259.0399\r\n",
      "I0412 23:50:59.234086 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.31836644, Global Step = 10, Loss = 3.801075, Perplexity = 44.74926 (17.403 sec)\r\n",
      "I0412 23:50:59.235188 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.574617\r\n",
      "I0412 23:51:15.530555 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.3063, Global Step = 20, Loss = 3.806019, Perplexity = 44.971058 (16.296 sec)\r\n",
      "I0412 23:51:15.531771 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.613626\r\n",
      "I0412 23:51:33.403632 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.3024386, Global Step = 30, Loss = 3.7033012, Perplexity = 40.58105 (17.873 sec)\r\n",
      "I0412 23:51:33.404709 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.559505\r\n",
      "I0412 23:51:37.947347 127805528086336 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 34...\r\n",
      "I0412 23:51:37.947608 127805528086336 basic_session_run_hooks.py:633] Saving checkpoints for 34 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 23:51:38.055934 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-34.meta\r\n",
      "I0412 23:51:38.056156 127805528086336 saver.py:93] 400\r\n",
      "I0412 23:51:38.056267 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-34.index\r\n",
      "I0412 23:51:38.056393 127805528086336 saver.py:93] 400\r\n",
      "I0412 23:51:38.056498 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-34.data-00000-of-00001\r\n",
      "I0412 23:51:38.056603 127805528086336 saver.py:93] 20100\r\n",
      "I0412 23:51:38.056790 127805528086336 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 34...\r\n",
      "I0412 23:51:49.633005 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.3120302, Global Step = 40, Loss = 3.7363389, Perplexity = 41.94414 (16.229 sec)\r\n",
      "I0412 23:51:49.634236 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.616161\r\n",
      "I0412 23:52:07.574808 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.32790586, Global Step = 50, Loss = 3.7044902, Perplexity = 40.629326 (17.942 sec)\r\n",
      "I0412 23:52:07.575982 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.557359\r\n",
      "I0412 23:52:25.751065 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.30374587, Global Step = 60, Loss = 3.6764908, Perplexity = 39.50751 (18.176 sec)\r\n",
      "I0412 23:52:25.752265 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.550167\r\n",
      "I0412 23:52:39.651026 127805528086336 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 69...\r\n",
      "I0412 23:52:39.651297 127805528086336 basic_session_run_hooks.py:633] Saving checkpoints for 69 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 23:52:39.761271 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-69.data-00000-of-00001\r\n",
      "I0412 23:52:39.761525 127805528086336 saver.py:93] 19700\r\n",
      "I0412 23:52:39.761655 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-69.meta\r\n",
      "I0412 23:52:39.761769 127805528086336 saver.py:93] 20100\r\n",
      "I0412 23:52:39.761865 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-69.index\r\n",
      "I0412 23:52:39.761955 127805528086336 saver.py:93] 20100\r\n",
      "I0412 23:52:39.762145 127805528086336 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 69...\r\n",
      "I0412 23:52:43.171506 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.32718578, Global Step = 70, Loss = 3.5986416, Perplexity = 36.548553 (17.420 sec)\r\n",
      "I0412 23:52:43.172583 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.574042\r\n",
      "I0412 23:53:01.174393 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.30619898, Global Step = 80, Loss = 3.734022, Perplexity = 41.847073 (18.003 sec)\r\n",
      "I0412 23:53:01.175594 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.555463\r\n",
      "I0412 23:53:19.293657 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.29166067, Global Step = 90, Loss = 3.7608464, Perplexity = 42.98479 (18.119 sec)\r\n",
      "I0412 23:53:19.294805 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.551901\r\n",
      "I0412 23:53:36.725916 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.571771\r\n",
      "I0412 23:53:36.727095 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.31636024, Global Step = 100, Loss = 3.6877213, Perplexity = 39.953697 (17.433 sec)\r\n",
      "I0412 23:53:36.727919 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.573619\r\n",
      "I0412 23:53:40.351030 127805528086336 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 103...\r\n",
      "I0412 23:53:40.351281 127805528086336 basic_session_run_hooks.py:633] Saving checkpoints for 103 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 23:53:40.454387 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-103.meta\r\n",
      "I0412 23:53:40.454607 127805528086336 saver.py:93] 400\r\n",
      "I0412 23:53:40.454726 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-103.index\r\n",
      "I0412 23:53:40.454835 127805528086336 saver.py:93] 400\r\n",
      "I0412 23:53:40.454941 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-103.data-00000-of-00001\r\n",
      "I0412 23:53:40.455032 127805528086336 saver.py:93] 20100\r\n",
      "I0412 23:53:40.455225 127805528086336 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 103...\r\n",
      "I0412 23:53:54.566019 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.30055356, Global Step = 110, Loss = 3.6884167, Perplexity = 39.981495 (17.839 sec)\r\n",
      "I0412 23:53:54.567203 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.560562\r\n",
      "I0412 23:54:12.334269 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.3133682, Global Step = 120, Loss = 3.559922, Perplexity = 35.160454 (17.768 sec)\r\n",
      "I0412 23:54:12.335392 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.562804\r\n",
      "I0412 23:54:29.749459 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.29597405, Global Step = 130, Loss = 3.6215906, Perplexity = 37.397003 (17.415 sec)\r\n",
      "I0412 23:54:29.750516 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.574213\r\n",
      "I0412 23:54:40.992699 127805528086336 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 138...\r\n",
      "I0412 23:54:40.992966 127805528086336 basic_session_run_hooks.py:633] Saving checkpoints for 138 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 23:54:41.093098 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-138.meta\r\n",
      "I0412 23:54:41.093347 127805528086336 saver.py:93] 400\r\n",
      "I0412 23:54:41.093483 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-138.index\r\n",
      "I0412 23:54:41.093599 127805528086336 saver.py:93] 400\r\n",
      "I0412 23:54:41.093697 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-138.data-00000-of-00001\r\n",
      "I0412 23:54:41.093788 127805528086336 saver.py:93] 20100\r\n",
      "I0412 23:54:41.093981 127805528086336 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 138...\r\n",
      "I0412 23:54:46.087929 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.28997275, Global Step = 140, Loss = 3.5348687, Perplexity = 34.290512 (16.338 sec)\r\n",
      "I0412 23:54:46.089258 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.612043\r\n",
      "I0412 23:55:03.912222 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.299332, Global Step = 150, Loss = 3.4759824, Perplexity = 32.329575 (17.824 sec)\r\n",
      "I0412 23:55:03.913339 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.561038\r\n",
      "I0412 23:55:20.438218 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.297314, Global Step = 160, Loss = 3.3910282, Perplexity = 29.696468 (16.526 sec)\r\n",
      "I0412 23:55:20.439411 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.605105\r\n",
      "I0412 23:55:38.244911 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.30517676, Global Step = 170, Loss = 3.4146614, Perplexity = 30.406652 (17.807 sec)\r\n",
      "I0412 23:55:38.246011 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.561589\r\n",
      "I0412 23:55:41.930205 127805528086336 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 173...\r\n",
      "I0412 23:55:41.930466 127805528086336 basic_session_run_hooks.py:633] Saving checkpoints for 173 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 23:55:42.039271 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-173.index\r\n",
      "I0412 23:55:42.039524 127805528086336 saver.py:93] 0\r\n",
      "I0412 23:55:42.039645 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-173.meta\r\n",
      "I0412 23:55:42.039754 127805528086336 saver.py:93] 400\r\n",
      "I0412 23:55:42.039853 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-173.data-00000-of-00001\r\n",
      "I0412 23:55:42.039946 127805528086336 saver.py:93] 20100\r\n",
      "I0412 23:55:42.040138 127805528086336 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 173...\r\n",
      "I0412 23:55:56.868821 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.2955888, Global Step = 180, Loss = 3.226966, Perplexity = 25.203074 (18.624 sec)\r\n",
      "I0412 23:55:56.870006 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.536942\r\n",
      "I0412 23:56:14.357816 127805528086336 basic_session_run_hooks.py:264] Accuracy = 0.32305503, Global Step = 190, Loss = 3.1806505, Perplexity = 24.0624 (17.489 sec)\r\n",
      "I0412 23:56:14.358989 127805528086336 basic_session_run_hooks.py:717] global_step/sec: 0.571788\r\n",
      "I0412 23:56:29.503563 127805528086336 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 200...\r\n",
      "I0412 23:56:29.503850 127805528086336 basic_session_run_hooks.py:633] Saving checkpoints for 200 into /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt.\r\n",
      "I0412 23:56:29.627468 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-200.index\r\n",
      "I0412 23:56:29.627699 127805528086336 saver.py:93] 0\r\n",
      "I0412 23:56:29.627819 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-200.meta\r\n",
      "I0412 23:56:29.627919 127805528086336 saver.py:93] 400\r\n",
      "I0412 23:56:29.628019 127805528086336 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-005/train/model.ckpt-200.data-00000-of-00001\r\n",
      "I0412 23:56:29.628113 127805528086336 saver.py:93] 20100\r\n",
      "I0412 23:56:29.628305 127805528086336 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 200...\r\n",
      "I0412 23:56:29.674263 127805528086336 events_rnn_train.py:113] Training complete.\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 23:56:47.523525 124486826628928 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 23:56:47.524016 124486826628928 polyphony_rnn_train.py:94] Train dir: /kaggle/tmp/polyphony_rnn/logdir/run-005/train\r\n",
      "I0412 23:56:47.524607 124486826628928 polyphony_rnn_train.py:99] Eval dir: /kaggle/tmp/polyphony_rnn/logdir/run-005/eval\r\n",
      "W0412 23:56:47.525325 124486826628928 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/polyphony_rnn_train\", line 8, in <module>\r\n",
      "    sys.exit(console_entry_point())\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 114, in console_entry_point\r\n",
      "    tf.app.run(main)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 36, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 308, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 254, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 104, in main\r\n",
      "    events_rnn_train.run_eval(build_graph_fn, train_dir, eval_dir, num_batches)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_train.py\", line 138, in run_eval\r\n",
      "    build_graph_fn()\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py\", line 113, in build\r\n",
      "    label_shape=label_shape, shuffle=mode == 'train')\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py\", line 66, in get_padded_batch\r\n",
      "    file_queue = tf.train.string_input_producer(file_list)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 357, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py\", line 253, in string_input_producer\r\n",
      "    raise ValueError(not_null_err)\r\n",
      "ValueError: string_input_producer requires a non-null input tensor\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0412 23:57:05.382098 138858993907520 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.01, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0412 23:57:05.382608 138858993907520 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-01/train\r\n",
      "W0412 23:57:05.383614 138858993907520 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 23:57:05.391639 138858993907520 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0412 23:57:05.393738 138858993907520 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0412 23:57:05.395281 138858993907520 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 23:57:05.396235 138858993907520 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0412 23:57:05.399880 138858993907520 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0412 23:57:05.407224 138858993907520 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0412 23:57:05.407379 138858993907520 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0412 23:57:05.452446 138858993907520 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0412 23:57:05.456705 138858993907520 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0412 23:57:05.457074 138858993907520 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0412 23:57:05.493755 138858993907520 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0412 23:57:05.516174 138858993907520 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0412 23:57:05.581698 138858993907520 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0412 23:57:05.729241 138858993907520 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0412 23:57:05.732334 138858993907520 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0412 23:57:06.534007 138858993907520 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0412 23:57:06.534283 138858993907520 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0412 23:57:06.648297 138858993907520 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0412 23:57:06.718626 138858993907520 monitored_session.py:243] Graph was finalized.\r\n",
      "I0412 23:57:07.921877 138858993907520 session_manager.py:527] Running local_init_op.\r\n",
      "I0412 23:57:07.928773 138858993907520 session_manager.py:530] Done running local_init_op.\r\n",
      "W0412 23:57:07.954046 138858993907520 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0412 23:57:08.653398 138858993907520 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0412 23:57:08.656105 138858993907520 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 23:57:08.848230 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-0.meta\r\n",
      "I0412 23:57:08.852087 138858993907520 saver.py:93] 400\r\n",
      "I0412 23:57:08.854405 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0412 23:57:08.854638 138858993907520 saver.py:93] 20100\r\n",
      "I0412 23:57:08.854831 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-0.index\r\n",
      "I0412 23:57:08.854974 138858993907520 saver.py:93] 20100\r\n",
      "I0412 23:57:08.855189 138858993907520 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0412 23:57:12.765117 138858993907520 basic_session_run_hooks.py:266] Accuracy = 0.0031677075, Global Step = 0, Loss = 5.557027, Perplexity = 259.05148\r\n",
      "I0412 23:57:29.144424 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.33130336, Global Step = 10, Loss = 3.7416146, Perplexity = 42.166016 (16.379 sec)\r\n",
      "I0412 23:57:29.145521 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.610522\r\n",
      "I0412 23:57:45.139559 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.3317266, Global Step = 20, Loss = 3.766045, Perplexity = 43.20884 (15.995 sec)\r\n",
      "I0412 23:57:45.140757 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.625187\r\n",
      "I0412 23:58:02.501157 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.3000887, Global Step = 30, Loss = 3.763217, Perplexity = 43.086815 (17.362 sec)\r\n",
      "I0412 23:58:02.502180 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.575989\r\n",
      "I0412 23:58:09.365437 138858993907520 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 35...\r\n",
      "I0412 23:58:09.365710 138858993907520 basic_session_run_hooks.py:633] Saving checkpoints for 35 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 23:58:09.492851 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-35.data-00000-of-00001\r\n",
      "I0412 23:58:09.493096 138858993907520 saver.py:93] 19700\r\n",
      "I0412 23:58:09.493233 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-35.index\r\n",
      "I0412 23:58:09.493385 138858993907520 saver.py:93] 19700\r\n",
      "I0412 23:58:09.493498 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-35.meta\r\n",
      "I0412 23:58:09.493635 138858993907520 saver.py:93] 20100\r\n",
      "I0412 23:58:09.493833 138858993907520 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 35...\r\n",
      "I0412 23:58:19.285717 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.31709927, Global Step = 40, Loss = 3.7733114, Perplexity = 43.52395 (16.785 sec)\r\n",
      "I0412 23:58:19.286808 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.595784\r\n",
      "I0412 23:58:36.305060 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.3072702, Global Step = 50, Loss = 3.7958229, Perplexity = 44.51485 (17.019 sec)\r\n",
      "I0412 23:58:36.306339 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.587561\r\n",
      "I0412 23:58:54.911975 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.30510452, Global Step = 60, Loss = 3.789024, Perplexity = 44.21323 (18.607 sec)\r\n",
      "I0412 23:58:54.913078 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.53744\r\n",
      "I0412 23:59:10.871327 138858993907520 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 70...\r\n",
      "I0412 23:59:10.871586 138858993907520 basic_session_run_hooks.py:633] Saving checkpoints for 70 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0412 23:59:10.975281 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-70.meta\r\n",
      "I0412 23:59:10.975533 138858993907520 saver.py:93] 400\r\n",
      "I0412 23:59:10.975653 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-70.data-00000-of-00001\r\n",
      "I0412 23:59:10.975762 138858993907520 saver.py:93] 20100\r\n",
      "I0412 23:59:10.975861 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-70.index\r\n",
      "I0412 23:59:10.975959 138858993907520 saver.py:93] 20100\r\n",
      "I0412 23:59:10.976150 138858993907520 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 70...\r\n",
      "I0412 23:59:12.949783 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.31244406, Global Step = 70, Loss = 3.630615, Perplexity = 37.73602 (18.038 sec)\r\n",
      "I0412 23:59:12.950884 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.55439\r\n",
      "I0412 23:59:29.172580 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.3112519, Global Step = 80, Loss = 3.6863654, Perplexity = 39.899563 (16.223 sec)\r\n",
      "I0412 23:59:29.173758 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.616414\r\n",
      "I0412 23:59:47.060173 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.3097572, Global Step = 90, Loss = 3.6870174, Perplexity = 39.92559 (17.888 sec)\r\n",
      "I0412 23:59:47.061385 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.559046\r\n",
      "I0413 00:00:04.643627 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.581804\r\n",
      "I0413 00:00:04.644763 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.3279781, Global Step = 100, Loss = 3.6095786, Perplexity = 36.950478 (17.585 sec)\r\n",
      "I0413 00:00:04.645700 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.568687\r\n",
      "I0413 00:00:11.888050 138858993907520 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 105...\r\n",
      "I0413 00:00:11.888326 138858993907520 basic_session_run_hooks.py:633] Saving checkpoints for 105 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0413 00:00:11.993233 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-105.data-00000-of-00001\r\n",
      "I0413 00:00:11.993486 138858993907520 saver.py:93] 19700\r\n",
      "I0413 00:00:11.993602 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-105.meta\r\n",
      "I0413 00:00:11.993710 138858993907520 saver.py:93] 20100\r\n",
      "I0413 00:00:11.993815 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-105.index\r\n",
      "I0413 00:00:11.993907 138858993907520 saver.py:93] 20100\r\n",
      "I0413 00:00:11.994097 138858993907520 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 105...\r\n",
      "I0413 00:00:21.802211 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.3318747, Global Step = 110, Loss = 3.6202347, Perplexity = 37.346333 (17.157 sec)\r\n",
      "I0413 00:00:21.803278 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.582833\r\n",
      "I0413 00:00:40.312386 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.31295595, Global Step = 120, Loss = 3.6958313, Perplexity = 40.27904 (18.510 sec)\r\n",
      "I0413 00:00:40.313485 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.540242\r\n",
      "I0413 00:00:56.895485 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.31264475, Global Step = 130, Loss = 3.7242355, Perplexity = 41.439545 (16.583 sec)\r\n",
      "I0413 00:00:56.896681 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.60302\r\n",
      "I0413 00:01:13.231333 138858993907520 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 141...\r\n",
      "I0413 00:01:13.231655 138858993907520 basic_session_run_hooks.py:633] Saving checkpoints for 141 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0413 00:01:13.442828 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-141.index\r\n",
      "I0413 00:01:13.443154 138858993907520 saver.py:93] 0\r\n",
      "I0413 00:01:13.443455 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-141.meta\r\n",
      "I0413 00:01:13.443670 138858993907520 saver.py:93] 400\r\n",
      "I0413 00:01:13.443844 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-141.data-00000-of-00001\r\n",
      "I0413 00:01:13.443965 138858993907520 saver.py:93] 20100\r\n",
      "I0413 00:01:13.444303 138858993907520 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 141...\r\n",
      "I0413 00:01:13.444800 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.32245553, Global Step = 140, Loss = 3.6318264, Perplexity = 37.78176 (16.549 sec)\r\n",
      "I0413 00:01:13.450714 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.604084\r\n",
      "I0413 00:01:29.514934 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.29684025, Global Step = 150, Loss = 3.7336211, Perplexity = 41.830307 (16.070 sec)\r\n",
      "I0413 00:01:29.515947 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.622461\r\n",
      "I0413 00:01:46.861615 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.31226215, Global Step = 160, Loss = 3.7329977, Perplexity = 41.804237 (17.347 sec)\r\n",
      "I0413 00:01:46.862799 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.576474\r\n",
      "I0413 00:02:03.705114 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.3253151, Global Step = 170, Loss = 3.7562735, Perplexity = 42.788677 (16.844 sec)\r\n",
      "I0413 00:02:03.706344 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.5937\r\n",
      "I0413 00:02:14.827936 138858993907520 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 177...\r\n",
      "I0413 00:02:14.828203 138858993907520 basic_session_run_hooks.py:633] Saving checkpoints for 177 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0413 00:02:14.929575 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-177.meta\r\n",
      "I0413 00:02:14.929796 138858993907520 saver.py:93] 400\r\n",
      "I0413 00:02:14.929911 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-177.data-00000-of-00001\r\n",
      "I0413 00:02:14.930009 138858993907520 saver.py:93] 20100\r\n",
      "I0413 00:02:14.930117 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-177.index\r\n",
      "I0413 00:02:14.930209 138858993907520 saver.py:93] 20100\r\n",
      "I0413 00:02:14.930406 138858993907520 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 177...\r\n",
      "I0413 00:02:22.446894 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.29147854, Global Step = 180, Loss = 3.7540092, Perplexity = 42.691902 (18.742 sec)\r\n",
      "I0413 00:02:22.448010 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.53357\r\n",
      "I0413 00:02:39.381051 138858993907520 basic_session_run_hooks.py:264] Accuracy = 0.33206132, Global Step = 190, Loss = 3.623487, Perplexity = 37.46799 (16.934 sec)\r\n",
      "I0413 00:02:39.382083 138858993907520 basic_session_run_hooks.py:717] global_step/sec: 0.590526\r\n",
      "I0413 00:02:55.341283 138858993907520 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 200...\r\n",
      "I0413 00:02:55.341543 138858993907520 basic_session_run_hooks.py:633] Saving checkpoints for 200 into /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt.\r\n",
      "I0413 00:02:55.474473 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-200.index\r\n",
      "I0413 00:02:55.474705 138858993907520 saver.py:93] 0\r\n",
      "I0413 00:02:55.474823 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-200.meta\r\n",
      "I0413 00:02:55.474927 138858993907520 saver.py:93] 400\r\n",
      "I0413 00:02:55.475034 138858993907520 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-01/train/model.ckpt-200.data-00000-of-00001\r\n",
      "I0413 00:02:55.475130 138858993907520 saver.py:93] 20100\r\n",
      "I0413 00:02:55.475334 138858993907520 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 200...\r\n",
      "I0413 00:02:55.520952 138858993907520 events_rnn_train.py:113] Training complete.\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0413 00:03:13.369433 137802575157056 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0413 00:03:13.369919 137802575157056 polyphony_rnn_train.py:94] Train dir: /kaggle/tmp/polyphony_rnn/logdir/run-01/train\r\n",
      "I0413 00:03:13.370454 137802575157056 polyphony_rnn_train.py:99] Eval dir: /kaggle/tmp/polyphony_rnn/logdir/run-01/eval\r\n",
      "W0413 00:03:13.371166 137802575157056 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/polyphony_rnn_train\", line 8, in <module>\r\n",
      "    sys.exit(console_entry_point())\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 114, in console_entry_point\r\n",
      "    tf.app.run(main)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 36, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 308, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 254, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 104, in main\r\n",
      "    events_rnn_train.run_eval(build_graph_fn, train_dir, eval_dir, num_batches)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_train.py\", line 138, in run_eval\r\n",
      "    build_graph_fn()\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py\", line 113, in build\r\n",
      "    label_shape=label_shape, shuffle=mode == 'train')\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py\", line 66, in get_padded_batch\r\n",
      "    file_queue = tf.train.string_input_producer(file_list)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 357, in new_func\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py\", line 253, in string_input_producer\r\n",
      "    raise ValueError(not_null_err)\r\n",
      "ValueError: string_input_producer requires a non-null input tensor\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0413 00:03:31.122039 126121407809344 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.02, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0413 00:03:31.122563 126121407809344 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-02/train\r\n",
      "W0413 00:03:31.123747 126121407809344 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0413 00:03:31.131924 126121407809344 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0413 00:03:31.133922 126121407809344 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0413 00:03:31.135474 126121407809344 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0413 00:03:31.136454 126121407809344 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0413 00:03:31.140119 126121407809344 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0413 00:03:31.147488 126121407809344 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0413 00:03:31.147638 126121407809344 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0413 00:03:31.190623 126121407809344 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0413 00:03:31.194884 126121407809344 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0413 00:03:31.195233 126121407809344 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0413 00:03:31.234266 126121407809344 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0413 00:03:31.257734 126121407809344 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0413 00:03:31.324196 126121407809344 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0413 00:03:31.476489 126121407809344 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0413 00:03:31.479591 126121407809344 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0413 00:03:32.295569 126121407809344 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0413 00:03:32.295835 126121407809344 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0413 00:03:32.406489 126121407809344 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0413 00:03:32.480445 126121407809344 monitored_session.py:243] Graph was finalized.\r\n",
      "I0413 00:03:33.692852 126121407809344 session_manager.py:527] Running local_init_op.\r\n",
      "I0413 00:03:33.700088 126121407809344 session_manager.py:530] Done running local_init_op.\r\n",
      "W0413 00:03:33.727863 126121407809344 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0413 00:03:34.588116 126121407809344 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0413 00:03:34.589844 126121407809344 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0413 00:03:34.781864 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-0.meta\r\n",
      "I0413 00:03:34.782222 126121407809344 saver.py:93] 400\r\n",
      "I0413 00:03:34.782566 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0413 00:03:34.782952 126121407809344 saver.py:93] 20100\r\n",
      "I0413 00:03:34.783253 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-0.index\r\n",
      "I0413 00:03:34.783466 126121407809344 saver.py:93] 20100\r\n",
      "I0413 00:03:34.783762 126121407809344 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0413 00:03:38.590299 126121407809344 basic_session_run_hooks.py:266] Accuracy = 0.002137941, Global Step = 0, Loss = 5.5586553, Perplexity = 259.47366\r\n",
      "I0413 00:03:55.379372 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.31929418, Global Step = 10, Loss = 4.0887055, Perplexity = 59.662613 (16.789 sec)\r\n",
      "I0413 00:03:55.380414 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.595634\r\n",
      "I0413 00:04:11.643610 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.3043646, Global Step = 20, Loss = 3.9782605, Perplexity = 53.424023 (16.264 sec)\r\n",
      "I0413 00:04:11.644694 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.614843\r\n",
      "I0413 00:04:28.692861 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.30314633, Global Step = 30, Loss = 3.759785, Perplexity = 42.93919 (17.049 sec)\r\n",
      "I0413 00:04:28.694064 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.586533\r\n",
      "I0413 00:04:35.604464 126121407809344 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 35...\r\n",
      "I0413 00:04:35.604732 126121407809344 basic_session_run_hooks.py:633] Saving checkpoints for 35 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0413 00:04:35.711125 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-35.data-00000-of-00001\r\n",
      "I0413 00:04:35.711365 126121407809344 saver.py:93] 19700\r\n",
      "I0413 00:04:35.711489 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-35.index\r\n",
      "I0413 00:04:35.711605 126121407809344 saver.py:93] 19700\r\n",
      "I0413 00:04:35.711709 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-35.meta\r\n",
      "I0413 00:04:35.711803 126121407809344 saver.py:93] 20100\r\n",
      "I0413 00:04:35.711991 126121407809344 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 35...\r\n",
      "I0413 00:04:45.448506 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.29034355, Global Step = 40, Loss = 3.9246333, Perplexity = 50.634506 (16.756 sec)\r\n",
      "I0413 00:04:45.449694 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.596815\r\n",
      "I0413 00:05:02.689803 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.2998646, Global Step = 50, Loss = 3.878328, Perplexity = 48.343323 (17.241 sec)\r\n",
      "I0413 00:05:02.690951 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.580004\r\n",
      "I0413 00:05:20.610412 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.29301026, Global Step = 60, Loss = 3.6967332, Perplexity = 40.315388 (17.921 sec)\r\n",
      "I0413 00:05:20.611612 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.558015\r\n",
      "I0413 00:05:35.795153 126121407809344 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 69...\r\n",
      "I0413 00:05:35.795442 126121407809344 basic_session_run_hooks.py:633] Saving checkpoints for 69 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0413 00:05:35.903803 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-69.data-00000-of-00001\r\n",
      "I0413 00:05:35.904074 126121407809344 saver.py:93] 19700\r\n",
      "I0413 00:05:35.904198 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-69.meta\r\n",
      "I0413 00:05:35.904332 126121407809344 saver.py:93] 20100\r\n",
      "I0413 00:05:35.904436 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-69.index\r\n",
      "I0413 00:05:35.904554 126121407809344 saver.py:93] 20100\r\n",
      "I0413 00:05:35.904738 126121407809344 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 69...\r\n",
      "I0413 00:05:39.416862 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.31940648, Global Step = 70, Loss = 3.679327, Perplexity = 39.619724 (18.806 sec)\r\n",
      "I0413 00:05:39.418392 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.531724\r\n",
      "I0413 00:05:56.001610 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.32557452, Global Step = 80, Loss = 3.5818307, Perplexity = 35.939278 (16.585 sec)\r\n",
      "I0413 00:05:56.002841 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.602975\r\n",
      "I0413 00:06:13.656974 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.29298666, Global Step = 90, Loss = 3.8011181, Perplexity = 44.75119 (17.655 sec)\r\n",
      "I0413 00:06:13.658088 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.566403\r\n",
      "I0413 00:06:32.330407 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.575571\r\n",
      "I0413 00:06:32.331555 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.28556427, Global Step = 100, Loss = 3.8291924, Perplexity = 46.025356 (18.675 sec)\r\n",
      "I0413 00:06:32.332575 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.535489\r\n",
      "I0413 00:06:37.271567 126121407809344 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 104...\r\n",
      "I0413 00:06:37.271850 126121407809344 basic_session_run_hooks.py:633] Saving checkpoints for 104 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0413 00:06:37.372200 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-104.meta\r\n",
      "I0413 00:06:37.372481 126121407809344 saver.py:93] 400\r\n",
      "I0413 00:06:37.373437 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-104.data-00000-of-00001\r\n",
      "I0413 00:06:37.373600 126121407809344 saver.py:93] 20100\r\n",
      "I0413 00:06:37.373716 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-104.index\r\n",
      "I0413 00:06:37.373811 126121407809344 saver.py:93] 20100\r\n",
      "I0413 00:06:37.374007 126121407809344 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 104...\r\n",
      "I0413 00:06:48.892745 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.29291716, Global Step = 110, Loss = 3.732104, Perplexity = 41.766895 (16.561 sec)\r\n",
      "I0413 00:06:48.893869 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.603819\r\n",
      "I0413 00:07:06.715879 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.30172235, Global Step = 120, Loss = 3.7848322, Perplexity = 44.028282 (17.823 sec)\r\n",
      "I0413 00:07:06.717005 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.561069\r\n",
      "I0413 00:07:23.154426 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.31652582, Global Step = 130, Loss = 3.618892, Perplexity = 37.29622 (16.439 sec)\r\n",
      "I0413 00:07:23.155552 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.608325\r\n",
      "I0413 00:07:38.639523 126121407809344 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 140...\r\n",
      "I0413 00:07:38.639808 126121407809344 basic_session_run_hooks.py:633] Saving checkpoints for 140 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0413 00:07:38.769623 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-140.data-00000-of-00001\r\n",
      "I0413 00:07:38.769864 126121407809344 saver.py:93] 19700\r\n",
      "I0413 00:07:38.769988 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-140.meta\r\n",
      "I0413 00:07:38.770100 126121407809344 saver.py:93] 20100\r\n",
      "I0413 00:07:38.770204 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-140.index\r\n",
      "I0413 00:07:38.770298 126121407809344 saver.py:93] 20100\r\n",
      "I0413 00:07:38.770551 126121407809344 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 140...\r\n",
      "I0413 00:07:40.297999 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.31404004, Global Step = 140, Loss = 3.7555718, Perplexity = 42.758663 (17.144 sec)\r\n",
      "I0413 00:07:40.299028 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.583312\r\n",
      "I0413 00:07:57.804788 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.3049587, Global Step = 150, Loss = 3.7919087, Perplexity = 44.340954 (17.507 sec)\r\n",
      "I0413 00:07:57.805935 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.571204\r\n",
      "I0413 00:08:14.510610 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.30434123, Global Step = 160, Loss = 3.7908409, Perplexity = 44.293633 (16.706 sec)\r\n",
      "I0413 00:08:14.511832 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.598591\r\n",
      "I0413 00:08:32.200926 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.3052294, Global Step = 170, Loss = 3.778823, Perplexity = 43.764496 (17.690 sec)\r\n",
      "I0413 00:08:32.202056 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.565284\r\n",
      "I0413 00:08:39.680473 126121407809344 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 175...\r\n",
      "I0413 00:08:39.680748 126121407809344 basic_session_run_hooks.py:633] Saving checkpoints for 175 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0413 00:08:39.782631 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-175.data-00000-of-00001\r\n",
      "I0413 00:08:39.782871 126121407809344 saver.py:93] 19700\r\n",
      "I0413 00:08:39.782999 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-175.meta\r\n",
      "I0413 00:08:39.783119 126121407809344 saver.py:93] 20100\r\n",
      "I0413 00:08:39.783225 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-175.index\r\n",
      "I0413 00:08:39.783337 126121407809344 saver.py:93] 20100\r\n",
      "I0413 00:08:39.783551 126121407809344 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 175...\r\n",
      "I0413 00:08:50.212279 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.2990234, Global Step = 180, Loss = 3.7567356, Perplexity = 42.808453 (18.011 sec)\r\n",
      "I0413 00:08:50.213502 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.555203\r\n",
      "I0413 00:09:08.884645 126121407809344 basic_session_run_hooks.py:264] Accuracy = 0.31831947, Global Step = 190, Loss = 3.677813, Perplexity = 39.559784 (18.672 sec)\r\n",
      "I0413 00:09:08.885718 126121407809344 basic_session_run_hooks.py:717] global_step/sec: 0.535555\r\n",
      "I0413 00:09:23.544133 126121407809344 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 200...\r\n",
      "I0413 00:09:23.544414 126121407809344 basic_session_run_hooks.py:633] Saving checkpoints for 200 into /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt.\r\n",
      "I0413 00:09:23.670825 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-200.index\r\n",
      "I0413 00:09:23.671052 126121407809344 saver.py:93] 0\r\n",
      "I0413 00:09:23.671177 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-200.meta\r\n",
      "I0413 00:09:23.671277 126121407809344 saver.py:93] 400\r\n",
      "I0413 00:09:23.671411 126121407809344 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-02/train/model.ckpt-200.data-00000-of-00001\r\n",
      "I0413 00:09:23.671513 126121407809344 saver.py:93] 20100\r\n",
      "I0413 00:09:23.671713 126121407809344 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 200...\r\n",
      "I0413 00:09:23.722891 126121407809344 events_rnn_train.py:113] Training complete.\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0413 00:09:42.805073 127681119635264 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.05, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0413 00:09:42.805609 127681119635264 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-05/train\r\n",
      "W0413 00:09:42.806609 127681119635264 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0413 00:09:42.814343 127681119635264 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0413 00:09:42.816270 127681119635264 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0413 00:09:42.817780 127681119635264 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0413 00:09:42.818720 127681119635264 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0413 00:09:42.822203 127681119635264 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0413 00:09:42.829286 127681119635264 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0413 00:09:42.829437 127681119635264 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0413 00:09:42.868635 127681119635264 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0413 00:09:42.871653 127681119635264 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0413 00:09:42.871920 127681119635264 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0413 00:09:42.897427 127681119635264 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0413 00:09:42.919184 127681119635264 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0413 00:09:42.982231 127681119635264 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0413 00:09:43.123990 127681119635264 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0413 00:09:43.126923 127681119635264 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0413 00:09:43.962844 127681119635264 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0413 00:09:43.963115 127681119635264 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0413 00:09:44.074075 127681119635264 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0413 00:09:44.154776 127681119635264 monitored_session.py:243] Graph was finalized.\r\n",
      "I0413 00:09:45.351660 127681119635264 session_manager.py:527] Running local_init_op.\r\n",
      "I0413 00:09:45.358693 127681119635264 session_manager.py:530] Done running local_init_op.\r\n",
      "W0413 00:09:45.384143 127681119635264 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0413 00:09:46.073457 127681119635264 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0413 00:09:46.075846 127681119635264 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt.\r\n",
      "I0413 00:09:46.272950 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-0.meta\r\n",
      "I0413 00:09:46.273291 127681119635264 saver.py:93] 400\r\n",
      "I0413 00:09:46.273622 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0413 00:09:46.273825 127681119635264 saver.py:93] 20100\r\n",
      "I0413 00:09:46.274012 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-0.index\r\n",
      "I0413 00:09:46.276972 127681119635264 saver.py:93] 20100\r\n",
      "I0413 00:09:46.277428 127681119635264 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0413 00:09:50.140924 127681119635264 basic_session_run_hooks.py:266] Accuracy = 0.0024113888, Global Step = 0, Loss = 5.558811, Perplexity = 259.51413\r\n",
      "I0413 00:10:06.325350 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.2895366, Global Step = 10, Loss = 4.014381, Perplexity = 55.388996 (16.184 sec)\r\n",
      "I0413 00:10:06.326464 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.617875\r\n",
      "I0413 00:10:22.586189 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.3109876, Global Step = 20, Loss = 3.8044837, Perplexity = 44.90206 (16.261 sec)\r\n",
      "I0413 00:10:22.587337 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.614972\r\n",
      "I0413 00:10:38.870775 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.30254284, Global Step = 30, Loss = 3.7944891, Perplexity = 44.455517 (16.285 sec)\r\n",
      "I0413 00:10:38.871859 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.614079\r\n",
      "I0413 00:10:47.567469 127681119635264 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 36...\r\n",
      "I0413 00:10:47.567738 127681119635264 basic_session_run_hooks.py:633] Saving checkpoints for 36 into /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt.\r\n",
      "I0413 00:10:47.675288 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-36.index\r\n",
      "I0413 00:10:47.675552 127681119635264 saver.py:93] 0\r\n",
      "I0413 00:10:47.675678 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-36.meta\r\n",
      "I0413 00:10:47.675790 127681119635264 saver.py:93] 400\r\n",
      "I0413 00:10:47.675886 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-36.data-00000-of-00001\r\n",
      "I0413 00:10:47.675981 127681119635264 saver.py:93] 20100\r\n",
      "I0413 00:10:47.676169 127681119635264 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 36...\r\n",
      "I0413 00:10:55.419239 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.31149122, Global Step = 40, Loss = 3.7876034, Perplexity = 44.150463 (16.548 sec)\r\n",
      "I0413 00:10:55.420515 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.604279\r\n",
      "I0413 00:11:11.717892 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.31651834, Global Step = 50, Loss = 3.8295307, Perplexity = 46.040928 (16.299 sec)\r\n",
      "I0413 00:11:11.718907 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.613557\r\n",
      "I0413 00:11:30.272511 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.28564972, Global Step = 60, Loss = 3.8240898, Perplexity = 45.791103 (18.555 sec)\r\n",
      "I0413 00:11:30.273609 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.538947\r\n",
      "I0413 00:11:48.627108 127681119635264 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 71...\r\n",
      "I0413 00:11:48.627370 127681119635264 basic_session_run_hooks.py:633] Saving checkpoints for 71 into /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt.\r\n",
      "I0413 00:11:48.758834 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-71.index\r\n",
      "I0413 00:11:48.759056 127681119635264 saver.py:93] 0\r\n",
      "I0413 00:11:48.759176 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-71.data-00000-of-00001\r\n",
      "I0413 00:11:48.759274 127681119635264 saver.py:93] 19700\r\n",
      "I0413 00:11:48.759410 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-71.meta\r\n",
      "I0413 00:11:48.759507 127681119635264 saver.py:93] 20100\r\n",
      "I0413 00:11:48.759700 127681119635264 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 71...\r\n",
      "I0413 00:11:48.760067 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.3187331, Global Step = 70, Loss = 3.6632624, Perplexity = 38.98833 (18.488 sec)\r\n",
      "I0413 00:11:48.760985 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.54091\r\n",
      "I0413 00:12:05.870059 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.31061482, Global Step = 80, Loss = 3.645149, Perplexity = 38.288475 (17.110 sec)\r\n",
      "I0413 00:12:05.871179 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.584448\r\n",
      "I0413 00:12:23.981261 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.28086182, Global Step = 90, Loss = 3.8757446, Perplexity = 48.218586 (18.111 sec)\r\n",
      "I0413 00:12:23.982602 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.552138\r\n",
      "I0413 00:12:41.538100 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.583438\r\n",
      "I0413 00:12:41.539214 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.30701646, Global Step = 100, Loss = 3.7791257, Perplexity = 43.77775 (17.558 sec)\r\n",
      "I0413 00:12:41.540182 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.569553\r\n",
      "I0413 00:12:50.316456 127681119635264 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 106...\r\n",
      "I0413 00:12:50.316727 127681119635264 basic_session_run_hooks.py:633] Saving checkpoints for 106 into /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt.\r\n",
      "I0413 00:12:50.423262 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-106.data-00000-of-00001\r\n",
      "I0413 00:12:50.423518 127681119635264 saver.py:93] 19700\r\n",
      "I0413 00:12:50.423629 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-106.meta\r\n",
      "I0413 00:12:50.423734 127681119635264 saver.py:93] 20100\r\n",
      "I0413 00:12:50.423835 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-106.index\r\n",
      "I0413 00:12:50.423927 127681119635264 saver.py:93] 20100\r\n",
      "I0413 00:12:50.424118 127681119635264 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 106...\r\n",
      "I0413 00:12:58.612535 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.3084654, Global Step = 110, Loss = 3.6656432, Perplexity = 39.081264 (17.073 sec)\r\n",
      "I0413 00:12:58.613606 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.585706\r\n",
      "I0413 00:13:16.578099 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.3287403, Global Step = 120, Loss = 3.6147313, Perplexity = 37.141365 (17.966 sec)\r\n",
      "I0413 00:13:16.579239 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.556618\r\n",
      "I0413 00:13:33.403481 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.31394592, Global Step = 130, Loss = 3.6822636, Perplexity = 39.73624 (16.825 sec)\r\n",
      "I0413 00:13:33.404417 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.594348\r\n",
      "I0413 00:13:49.775232 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.30162653, Global Step = 140, Loss = 3.7410355, Perplexity = 42.141605 (16.372 sec)\r\n",
      "I0413 00:13:49.776419 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.610799\r\n",
      "I0413 00:13:51.537770 127681119635264 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 142...\r\n",
      "I0413 00:13:51.538032 127681119635264 basic_session_run_hooks.py:633] Saving checkpoints for 142 into /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt.\r\n",
      "I0413 00:13:51.638132 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-142.data-00000-of-00001\r\n",
      "I0413 00:13:51.638370 127681119635264 saver.py:93] 19700\r\n",
      "I0413 00:13:51.638485 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-142.index\r\n",
      "I0413 00:13:51.638608 127681119635264 saver.py:93] 19700\r\n",
      "I0413 00:13:51.638721 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-142.meta\r\n",
      "I0413 00:13:51.638830 127681119635264 saver.py:93] 20100\r\n",
      "I0413 00:13:51.639030 127681119635264 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 142...\r\n",
      "I0413 00:14:06.668601 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.29839382, Global Step = 150, Loss = 3.77146, Perplexity = 43.443447 (16.893 sec)\r\n",
      "I0413 00:14:06.669806 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.591947\r\n",
      "I0413 00:14:23.166512 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.30830783, Global Step = 160, Loss = 3.7060077, Perplexity = 40.691032 (16.498 sec)\r\n",
      "I0413 00:14:23.167584 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.606143\r\n",
      "I0413 00:14:40.621928 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.34251654, Global Step = 170, Loss = 3.777574, Perplexity = 43.709873 (17.455 sec)\r\n",
      "I0413 00:14:40.623134 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.572883\r\n",
      "I0413 00:14:52.683940 127681119635264 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 178...\r\n",
      "I0413 00:14:52.684200 127681119635264 basic_session_run_hooks.py:633] Saving checkpoints for 178 into /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt.\r\n",
      "I0413 00:14:52.787093 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-178.data-00000-of-00001\r\n",
      "I0413 00:14:52.787329 127681119635264 saver.py:93] 19700\r\n",
      "I0413 00:14:52.787446 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-178.index\r\n",
      "I0413 00:14:52.787545 127681119635264 saver.py:93] 19700\r\n",
      "I0413 00:14:52.787661 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-178.meta\r\n",
      "I0413 00:14:52.787751 127681119635264 saver.py:93] 20100\r\n",
      "I0413 00:14:52.787934 127681119635264 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 178...\r\n",
      "I0413 00:14:59.307916 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.30874375, Global Step = 180, Loss = 3.7171693, Perplexity = 41.14775 (18.686 sec)\r\n",
      "I0413 00:14:59.309026 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.535164\r\n",
      "I0413 00:15:16.918098 127681119635264 basic_session_run_hooks.py:264] Accuracy = 0.31129205, Global Step = 190, Loss = 3.6354716, Perplexity = 37.91973 (17.610 sec)\r\n",
      "I0413 00:15:16.919147 127681119635264 basic_session_run_hooks.py:717] global_step/sec: 0.567855\r\n",
      "I0413 00:15:31.942825 127681119635264 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 200...\r\n",
      "I0413 00:15:31.943074 127681119635264 basic_session_run_hooks.py:633] Saving checkpoints for 200 into /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt.\r\n",
      "I0413 00:15:32.046650 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-200.index\r\n",
      "I0413 00:15:32.046874 127681119635264 saver.py:93] 0\r\n",
      "I0413 00:15:32.046989 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-200.meta\r\n",
      "I0413 00:15:32.047091 127681119635264 saver.py:93] 400\r\n",
      "I0413 00:15:32.047199 127681119635264 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-05/train/model.ckpt-200.data-00000-of-00001\r\n",
      "I0413 00:15:32.047292 127681119635264 saver.py:93] 20100\r\n",
      "I0413 00:15:32.047518 127681119635264 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 200...\r\n",
      "I0413 00:15:32.094393 127681119635264 events_rnn_train.py:113] Training complete.\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\r\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\r\n",
      "  from numba.decorators import jit as optional_jit\r\n",
      "I0413 00:15:49.936818 135473632048960 events_rnn_graph.py:96] hparams = {'batch_size': 64, 'rnn_layer_sizes': [256, 256, 256], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.1, 'residual_connections': False, 'use_cudnn': False}\r\n",
      "I0413 00:15:49.937291 135473632048960 polyphony_rnn_train.py:94] Train dir: /kaggle/working/polyphony_rnn/logdir/run-1/train\r\n",
      "W0413 00:15:49.938305 135473632048960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0413 00:15:49.946172 135473632048960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:272: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W0413 00:15:49.948278 135473632048960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W0413 00:15:49.949833 135473632048960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0413 00:15:49.950816 135473632048960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/input.py:193: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W0413 00:15:49.954431 135473632048960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "I0413 00:15:49.961665 135473632048960 sequence_example_lib.py:121] Counting records in /kaggle/tmp/split/training_poly_tracks.tfrecord.\r\n",
      "W0413 00:15:49.961809 135473632048960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use eager execution and: \r\n",
      "`tf.data.TFRecordDataset(path)`\r\n",
      "I0413 00:15:50.002548 135473632048960 sequence_example_lib.py:125] Number of records is at least 100.\r\n",
      "I0413 00:15:50.006601 135473632048960 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\r\n",
      "W0413 00:15:50.006948 135473632048960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\n",
      "/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:50: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "  cell = base_cell(rnn_layer_sizes[i])\r\n",
      "W0413 00:15:50.045066 135473632048960 legacy_cells.py:1109] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\r\n",
      "W0413 00:15:50.074742 135473632048960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n",
      "W0413 00:15:50.143007 135473632048960 deprecation.py:560] From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\r\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n",
      "  warnings.warn('`layer.apply` is deprecated and '\r\n",
      "W0413 00:15:50.289519 135473632048960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "W0413 00:15:50.292613 135473632048960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\r\n",
      "    options available in V2.\r\n",
      "    - tf.py_function takes a python function which manipulates tf eager\r\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n",
      "    being differentiable using a gradient tape.\r\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n",
      "    stateful argument making all functions stateful.\r\n",
      "    \r\n",
      "I0413 00:15:51.096017 135473632048960 events_rnn_train.py:103] Starting training loop...\r\n",
      "I0413 00:15:51.096388 135473632048960 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\r\n",
      "W0413 00:15:51.305941 135473632048960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n",
      "I0413 00:15:51.376982 135473632048960 monitored_session.py:243] Graph was finalized.\r\n",
      "I0413 00:15:52.566460 135473632048960 session_manager.py:527] Running local_init_op.\r\n",
      "I0413 00:15:52.573387 135473632048960 session_manager.py:530] Done running local_init_op.\r\n",
      "W0413 00:15:52.600720 135473632048960 deprecation.py:356] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:914: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "I0413 00:15:53.264994 135473632048960 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\r\n",
      "I0413 00:15:53.266241 135473632048960 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt.\r\n",
      "I0413 00:15:53.452969 135473632048960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-0.meta\r\n",
      "I0413 00:15:53.453492 135473632048960 saver.py:93] 400\r\n",
      "I0413 00:15:53.453866 135473632048960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-0.data-00000-of-00001\r\n",
      "I0413 00:15:53.454349 135473632048960 saver.py:93] 20100\r\n",
      "I0413 00:15:53.454618 135473632048960 saver.py:90] /kaggle/working/polyphony_rnn/logdir/run-1/train/model.ckpt-0.index\r\n",
      "I0413 00:15:53.454832 135473632048960 saver.py:93] 20100\r\n",
      "I0413 00:15:53.455226 135473632048960 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\r\n",
      "I0413 00:15:57.257037 135473632048960 basic_session_run_hooks.py:266] Accuracy = 0.0067177895, Global Step = 0, Loss = 5.553405, Perplexity = 258.1149\r\n",
      "E0413 00:16:07.145941 135473632048960 basic_session_run_hooks.py:785] Model diverged with loss = NaN.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/polyphony_rnn_train\", line 8, in <module>\r\n",
      "    sys.exit(console_entry_point())\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 114, in console_entry_point\r\n",
      "    tf.app.run(main)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 36, in run\r\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 308, in run\r\n",
      "    _run_main(main, args)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 254, in _run_main\r\n",
      "    sys.exit(main(argv))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 110, in main\r\n",
      "    checkpoints_to_keep=FLAGS.num_checkpoints)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/magenta/models/shared/events_rnn_train.py\", line 112, in run_training\r\n",
      "    is_chief=task == 0)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tf_slim/training/training.py\", line 551, in train\r\n",
      "    loss = session.run(train_op, run_metadata=run_metadata)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 786, in run\r\n",
      "    run_metadata=run_metadata)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1315, in run\r\n",
      "    run_metadata=run_metadata)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1416, in run\r\n",
      "    raise six.reraise(*original_exc_info)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/six.py\", line 719, in reraise\r\n",
      "    raise value\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1401, in run\r\n",
      "    return self._sess.run(*args, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1482, in run\r\n",
      "    run_metadata=run_metadata))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 786, in after_run\r\n",
      "    raise NanLossDuringTrainingError\r\n",
      "tensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Different Learning Rates: 0.001, 0.005, 0.01, 0.02, 0.05, 0.1\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-001 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.001\" \\\n",
    "--num_checkpoints=100 \n",
    "# --summary_frequency=10\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-001 \\\n",
    "--sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "--num_eval_examples=1000 \\\n",
    "--eval\n",
    "\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-005 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.005\" \\\n",
    "--num_checkpoints=100 \n",
    "# --summary_frequency=10\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-005 \\\n",
    "--sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "--num_eval_examples=1000 \\\n",
    "--eval\n",
    "\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-01 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.01\" \\\n",
    "--num_checkpoints=20\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-01 \\\n",
    "--sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "--num_eval_examples=1000 \\\n",
    "--eval\n",
    "\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-02 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.02\" \\\n",
    "--num_checkpoints=20\n",
    "\n",
    "# !polyphony_rnn_train \\\n",
    "# --run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-02 \\\n",
    "# --sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "# --num_eval_examples=1000 \\\n",
    "# --eval\n",
    "\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-05 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.05\" \\\n",
    "--num_checkpoints=20 \n",
    "\n",
    "# !polyphony_rnn_train \\\n",
    "# --run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-05 \\\n",
    "# --sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "# --num_eval_examples=1000 \\\n",
    "# --eval\n",
    "\n",
    "\n",
    "!polyphony_rnn_train \\\n",
    "--run_dir=/kaggle/working/polyphony_rnn/logdir/run-1 \\\n",
    "--sequence_example_file=/kaggle/tmp/split/training_poly_tracks.tfrecord \\\n",
    "--num_training_steps=200 \\\n",
    "--hparams=\"learning_rate=0.1\" \\\n",
    "--num_checkpoints=20\n",
    "\n",
    "# !polyphony_rnn_train \\\n",
    "# --run_dir=/kaggle/tmp/polyphony_rnn/logdir/run-1 \\\n",
    "# --sequence_example_file=/tmp/polyphony_rnn/split/eval_poly_tracks.tfrecord \\\n",
    "# --num_eval_examples = 1000 \\\n",
    "# --eval\n",
    "\n",
    "\n",
    "!cp -R /kaggle/tmp/polyphony_rnn/logdir /kaggle/working/logdir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2615.767269,
   "end_time": "2023-04-13T00:16:12.178286",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-12T23:32:36.411017",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
